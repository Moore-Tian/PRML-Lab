2023-12-31 02:01:14,274	44k	INFO	{'train': {'log_interval': 200, 'eval_interval': 800, 'seed': 1234, 'epochs': 10000, 'learning_rate': 0.0001, 'betas': [0.8, 0.99], 'eps': 1e-09, 'batch_size': 6, 'fp16_run': False, 'half_type': 'fp16', 'lr_decay': 0.999875, 'segment_size': 10240, 'init_lr_ratio': 1, 'warmup_epochs': 0, 'c_mel': 45, 'c_kl': 1.0, 'use_sr': True, 'max_speclen': 512, 'port': '8001', 'keep_ckpts': 3, 'all_in_mem': False, 'vol_aug': False}, 'data': {'training_files': 'filelists/train.txt', 'validation_files': 'filelists/val.txt', 'max_wav_value': 32768.0, 'sampling_rate': 44100, 'filter_length': 2048, 'hop_length': 512, 'win_length': 2048, 'n_mel_channels': 80, 'mel_fmin': 0.0, 'mel_fmax': 22050, 'unit_interpolate_mode': 'nearest'}, 'model': {'inter_channels': 192, 'hidden_channels': 192, 'filter_channels': 768, 'n_heads': 2, 'n_layers': 6, 'kernel_size': 3, 'p_dropout': 0.1, 'resblock': '1', 'resblock_kernel_sizes': [3, 7, 11], 'resblock_dilation_sizes': [[1, 3, 5], [1, 3, 5], [1, 3, 5]], 'upsample_rates': [8, 8, 2, 2, 2], 'upsample_initial_channel': 512, 'upsample_kernel_sizes': [16, 16, 4, 4, 4], 'n_layers_q': 3, 'n_layers_trans_flow': 3, 'n_flow_layer': 4, 'use_spectral_norm': False, 'gin_channels': 768, 'ssl_dim': 768, 'n_speakers': 1, 'vocoder_name': 'nsf-hifigan', 'speech_encoder': 'vec768l12', 'speaker_embedding': False, 'vol_embedding': False, 'use_depthwise_conv': False, 'flow_share_parameter': False, 'use_automatic_f0_prediction': True, 'use_transformer_flow': False}, 'spk': {'Ganyu': 0}, 'model_dir': './logs/44k'}
2023-12-31 02:01:35,055	44k	INFO	Train Epoch: 1 [0%]
2023-12-31 02:01:35,055	44k	INFO	Losses: [6.071122646331787, 5.321061134338379, 1.2117773294448853, 149.56130981445312, 303.1567077636719], step: 0, lr: 0.0001, reference_loss: 465.32196044921875
2023-12-31 02:01:47,002	44k	INFO	Saving model and optimizer state at iteration 1 to ./logs/44k/G_0.pth
2023-12-31 02:01:49,940	44k	INFO	Saving model and optimizer state at iteration 1 to ./logs/44k/D_0.pth
2023-12-31 02:02:45,684	44k	INFO	====> Epoch: 1, cost 91.41 s
2023-12-31 02:03:29,207	44k	INFO	====> Epoch: 2, cost 43.52 s
2023-12-31 02:03:52,606	44k	INFO	Train Epoch: 3 [44%]
2023-12-31 02:03:52,607	44k	INFO	Losses: [1.3621411323547363, 2.8556137084960938, 7.77647066116333, 41.86549377441406, 4.016615867614746], step: 200, lr: 9.99750015625e-05, reference_loss: 57.87633514404297
2023-12-31 02:04:13,910	44k	INFO	====> Epoch: 3, cost 44.70 s
2023-12-31 02:04:57,490	44k	INFO	====> Epoch: 4, cost 43.58 s
2023-12-31 02:05:35,611	44k	INFO	Train Epoch: 5 [88%]
2023-12-31 02:05:35,612	44k	INFO	Losses: [1.983984112739563, 2.68752384185791, 4.898625373840332, 40.960994720458984, 2.532919406890869], step: 400, lr: 9.995000937421877e-05, reference_loss: 53.064048767089844
2023-12-31 02:05:41,623	44k	INFO	====> Epoch: 5, cost 44.13 s
2023-12-31 02:06:26,268	44k	INFO	====> Epoch: 6, cost 44.64 s
2023-12-31 02:07:10,890	44k	INFO	====> Epoch: 7, cost 44.62 s
2023-12-31 02:07:29,951	44k	INFO	Train Epoch: 8 [32%]
2023-12-31 02:07:29,952	44k	INFO	Losses: [1.4765549898147583, 2.864208221435547, 7.135033130645752, 35.448570251464844, 2.321347236633301], step: 600, lr: 9.991253280566489e-05, reference_loss: 49.24571228027344
2023-12-31 02:07:55,787	44k	INFO	====> Epoch: 8, cost 44.90 s
2023-12-31 02:08:39,590	44k	INFO	====> Epoch: 9, cost 43.80 s
2023-12-31 02:09:13,426	44k	INFO	Train Epoch: 10 [76%]
2023-12-31 02:09:13,426	44k	INFO	Losses: [1.97531259059906, 2.660757303237915, 5.3650994300842285, 36.437015533447266, 1.832939863204956], step: 800, lr: 9.98875562335968e-05, reference_loss: 48.27112579345703
2023-12-31 02:09:21,062	44k	INFO	Saving model and optimizer state at iteration 10 to ./logs/44k/G_800.pth
2023-12-31 02:09:22,254	44k	INFO	Saving model and optimizer state at iteration 10 to ./logs/44k/D_800.pth
2023-12-31 02:09:32,711	44k	INFO	====> Epoch: 10, cost 53.12 s
2023-12-31 02:10:17,526	44k	INFO	====> Epoch: 11, cost 44.82 s
2023-12-31 02:11:01,277	44k	INFO	====> Epoch: 12, cost 43.75 s
2023-12-31 02:11:15,480	44k	INFO	Train Epoch: 13 [20%]
2023-12-31 02:11:15,481	44k	INFO	Losses: [0.8889723420143127, 3.6606316566467285, 9.98529052734375, 33.946075439453125, 1.683588981628418], step: 1000, lr: 9.98501030820433e-05, reference_loss: 50.1645622253418
2023-12-31 02:11:45,598	44k	INFO	====> Epoch: 13, cost 44.32 s
2023-12-31 02:12:29,679	44k	INFO	====> Epoch: 14, cost 44.08 s
2023-12-31 02:13:00,018	44k	INFO	Train Epoch: 15 [63%]
2023-12-31 02:13:00,019	44k	INFO	Losses: [1.7065857648849487, 2.90606951713562, 8.411450386047363, 33.85582733154297, 1.2835725545883179], step: 1200, lr: 9.982514211643064e-05, reference_loss: 48.16350555419922
2023-12-31 02:13:14,447	44k	INFO	====> Epoch: 15, cost 44.77 s
2023-12-31 02:13:58,986	44k	INFO	====> Epoch: 16, cost 44.54 s
2023-12-31 02:14:42,872	44k	INFO	====> Epoch: 17, cost 43.89 s
2023-12-31 02:14:53,851	44k	INFO	Train Epoch: 18 [7%]
2023-12-31 02:14:53,851	44k	INFO	Losses: [2.171067476272583, 2.814876079559326, 5.612771034240723, 40.235740661621094, 1.375137448310852], step: 1400, lr: 9.978771236724554e-05, reference_loss: 52.2095947265625
2023-12-31 02:15:28,180	44k	INFO	====> Epoch: 18, cost 45.31 s
2023-12-31 02:16:12,681	44k	INFO	====> Epoch: 19, cost 44.50 s
2023-12-31 02:16:38,794	44k	INFO	Train Epoch: 20 [51%]
2023-12-31 02:16:38,795	44k	INFO	Losses: [2.1744182109832764, 2.81388783454895, 5.10505485534668, 32.78493118286133, 0.693832516670227], step: 1600, lr: 9.976276699833672e-05, reference_loss: 43.57212448120117
2023-12-31 02:16:45,930	44k	INFO	Saving model and optimizer state at iteration 20 to ./logs/44k/G_1600.pth
2023-12-31 02:16:47,097	44k	INFO	Saving model and optimizer state at iteration 20 to ./logs/44k/D_1600.pth
2023-12-31 02:18:35,848	44k	INFO	{'train': {'log_interval': 200, 'eval_interval': 800, 'seed': 1234, 'epochs': 10000, 'learning_rate': 0.0001, 'betas': [0.8, 0.99], 'eps': 1e-09, 'batch_size': 10, 'fp16_run': False, 'half_type': 'fp16', 'lr_decay': 0.999875, 'segment_size': 10240, 'init_lr_ratio': 1, 'warmup_epochs': 0, 'c_mel': 45, 'c_kl': 1.0, 'use_sr': True, 'max_speclen': 512, 'port': '8001', 'keep_ckpts': 3, 'all_in_mem': False, 'vol_aug': False}, 'data': {'training_files': 'filelists/train.txt', 'validation_files': 'filelists/val.txt', 'max_wav_value': 32768.0, 'sampling_rate': 44100, 'filter_length': 2048, 'hop_length': 512, 'win_length': 2048, 'n_mel_channels': 80, 'mel_fmin': 0.0, 'mel_fmax': 22050, 'unit_interpolate_mode': 'nearest'}, 'model': {'inter_channels': 192, 'hidden_channels': 192, 'filter_channels': 768, 'n_heads': 2, 'n_layers': 6, 'kernel_size': 3, 'p_dropout': 0.1, 'resblock': '1', 'resblock_kernel_sizes': [3, 7, 11], 'resblock_dilation_sizes': [[1, 3, 5], [1, 3, 5], [1, 3, 5]], 'upsample_rates': [8, 8, 2, 2, 2], 'upsample_initial_channel': 512, 'upsample_kernel_sizes': [16, 16, 4, 4, 4], 'n_layers_q': 3, 'n_layers_trans_flow': 3, 'n_flow_layer': 4, 'use_spectral_norm': False, 'gin_channels': 768, 'ssl_dim': 768, 'n_speakers': 1, 'vocoder_name': 'nsf-hifigan', 'speech_encoder': 'vec768l12', 'speaker_embedding': False, 'vol_embedding': False, 'use_depthwise_conv': False, 'flow_share_parameter': False, 'use_automatic_f0_prediction': True, 'use_transformer_flow': False}, 'spk': {'Ganyu': 0}, 'model_dir': './logs/44k'}
2023-12-31 02:18:42,099	44k	INFO	Loaded checkpoint './logs/44k/G_1600.pth' (iteration 20)
2023-12-31 02:18:57,660	44k	INFO	Train Epoch: 1 [0%]
2023-12-31 02:18:57,660	44k	INFO	Losses: [6.07169246673584, 5.303457260131836, 0.9013373255729675, 34.50192642211914, 1.2386126518249512], step: 0, lr: 9.976276699833672e-05, reference_loss: 48.017024993896484
2023-12-31 02:19:09,306	44k	INFO	Saving model and optimizer state at iteration 1 to ./logs/44k/G_0.pth
2023-12-31 02:19:12,222	44k	INFO	Saving model and optimizer state at iteration 1 to ./logs/44k/D_0.pth
2023-12-31 02:19:57,510	44k	INFO	====> Epoch: 1, cost 81.66 s
2023-12-31 02:20:39,892	44k	INFO	{'train': {'log_interval': 200, 'eval_interval': 800, 'seed': 1234, 'epochs': 10000, 'learning_rate': 0.0001, 'betas': [0.8, 0.99], 'eps': 1e-09, 'batch_size': 14, 'fp16_run': False, 'half_type': 'fp16', 'lr_decay': 0.999875, 'segment_size': 10240, 'init_lr_ratio': 1, 'warmup_epochs': 0, 'c_mel': 45, 'c_kl': 1.0, 'use_sr': True, 'max_speclen': 512, 'port': '8001', 'keep_ckpts': 3, 'all_in_mem': False, 'vol_aug': False}, 'data': {'training_files': 'filelists/train.txt', 'validation_files': 'filelists/val.txt', 'max_wav_value': 32768.0, 'sampling_rate': 44100, 'filter_length': 2048, 'hop_length': 512, 'win_length': 2048, 'n_mel_channels': 80, 'mel_fmin': 0.0, 'mel_fmax': 22050, 'unit_interpolate_mode': 'nearest'}, 'model': {'inter_channels': 192, 'hidden_channels': 192, 'filter_channels': 768, 'n_heads': 2, 'n_layers': 6, 'kernel_size': 3, 'p_dropout': 0.1, 'resblock': '1', 'resblock_kernel_sizes': [3, 7, 11], 'resblock_dilation_sizes': [[1, 3, 5], [1, 3, 5], [1, 3, 5]], 'upsample_rates': [8, 8, 2, 2, 2], 'upsample_initial_channel': 512, 'upsample_kernel_sizes': [16, 16, 4, 4, 4], 'n_layers_q': 3, 'n_layers_trans_flow': 3, 'n_flow_layer': 4, 'use_spectral_norm': False, 'gin_channels': 768, 'ssl_dim': 768, 'n_speakers': 1, 'vocoder_name': 'nsf-hifigan', 'speech_encoder': 'vec768l12', 'speaker_embedding': False, 'vol_embedding': False, 'use_depthwise_conv': False, 'flow_share_parameter': False, 'use_automatic_f0_prediction': True, 'use_transformer_flow': False}, 'spk': {'Ganyu': 0}, 'model_dir': './logs/44k'}
2023-12-31 02:20:46,158	44k	INFO	Loaded checkpoint './logs/44k/G_1600.pth' (iteration 20)
2023-12-31 02:21:04,937	44k	INFO	Train Epoch: 1 [0%]
2023-12-31 02:21:04,937	44k	INFO	Losses: [6.071520805358887, 5.3100762367248535, 0.5920113325119019, 32.21269607543945, 1.196508526802063], step: 0, lr: 9.976276699833672e-05, reference_loss: 45.382816314697266
2023-12-31 02:21:16,512	44k	INFO	Saving model and optimizer state at iteration 1 to ./logs/44k/G_0.pth
2023-12-31 02:21:19,808	44k	INFO	Saving model and optimizer state at iteration 1 to ./logs/44k/D_0.pth
2023-12-31 02:22:01,650	44k	INFO	====> Epoch: 1, cost 81.76 s
2023-12-31 02:22:35,301	44k	INFO	====> Epoch: 2, cost 33.65 s
2023-12-31 02:23:08,747	44k	INFO	====> Epoch: 3, cost 33.45 s
2023-12-31 02:23:42,743	44k	INFO	====> Epoch: 4, cost 34.00 s
2023-12-31 02:24:17,406	44k	INFO	====> Epoch: 5, cost 34.66 s
2023-12-31 02:24:44,048	44k	INFO	Train Epoch: 6 [71%]
2023-12-31 02:24:44,049	44k	INFO	Losses: [2.3055882453918457, 2.278282642364502, 4.256372928619385, 30.275632858276367, 1.0014374256134033], step: 200, lr: 9.970043085494672e-05, reference_loss: 40.117313385009766
2023-12-31 02:24:52,784	44k	INFO	====> Epoch: 6, cost 35.38 s
2023-12-31 02:25:27,007	44k	INFO	====> Epoch: 7, cost 34.22 s
2023-12-31 02:26:01,299	44k	INFO	====> Epoch: 8, cost 34.29 s
2023-12-31 02:26:35,792	44k	INFO	====> Epoch: 9, cost 34.49 s
2023-12-31 02:27:42,638	44k	INFO	{'train': {'log_interval': 200, 'eval_interval': 800, 'seed': 1234, 'epochs': 10000, 'learning_rate': 0.0001, 'betas': [0.8, 0.99], 'eps': 1e-09, 'batch_size': 14, 'fp16_run': False, 'half_type': 'fp16', 'lr_decay': 0.999875, 'segment_size': 10240, 'init_lr_ratio': 1, 'warmup_epochs': 0, 'c_mel': 45, 'c_kl': 1.0, 'use_sr': True, 'max_speclen': 512, 'port': '8001', 'keep_ckpts': 3, 'all_in_mem': True, 'vol_aug': False}, 'data': {'training_files': 'filelists/train.txt', 'validation_files': 'filelists/val.txt', 'max_wav_value': 32768.0, 'sampling_rate': 44100, 'filter_length': 2048, 'hop_length': 512, 'win_length': 2048, 'n_mel_channels': 80, 'mel_fmin': 0.0, 'mel_fmax': 22050, 'unit_interpolate_mode': 'nearest'}, 'model': {'inter_channels': 192, 'hidden_channels': 192, 'filter_channels': 768, 'n_heads': 2, 'n_layers': 6, 'kernel_size': 3, 'p_dropout': 0.1, 'resblock': '1', 'resblock_kernel_sizes': [3, 7, 11], 'resblock_dilation_sizes': [[1, 3, 5], [1, 3, 5], [1, 3, 5]], 'upsample_rates': [8, 8, 2, 2, 2], 'upsample_initial_channel': 512, 'upsample_kernel_sizes': [16, 16, 4, 4, 4], 'n_layers_q': 3, 'n_layers_trans_flow': 3, 'n_flow_layer': 4, 'use_spectral_norm': False, 'gin_channels': 768, 'ssl_dim': 768, 'n_speakers': 1, 'vocoder_name': 'nsf-hifigan', 'speech_encoder': 'vec768l12', 'speaker_embedding': False, 'vol_embedding': False, 'use_depthwise_conv': False, 'flow_share_parameter': False, 'use_automatic_f0_prediction': True, 'use_transformer_flow': False}, 'spk': {'Ganyu': 0}, 'model_dir': './logs/44k'}
2023-12-31 02:27:50,721	44k	INFO	Loaded checkpoint './logs/44k/G_1600.pth' (iteration 20)
2023-12-31 02:28:01,313	44k	INFO	Train Epoch: 1 [0%]
2023-12-31 02:28:01,313	44k	INFO	Losses: [6.07155704498291, 5.310981273651123, 0.5626047849655151, 31.62262725830078, 1.18911874294281], step: 0, lr: 9.976276699833672e-05, reference_loss: 44.75688934326172
2023-12-31 02:28:12,835	44k	INFO	Saving model and optimizer state at iteration 1 to ./logs/44k/G_0.pth
2023-12-31 02:28:16,415	44k	INFO	Saving model and optimizer state at iteration 1 to ./logs/44k/D_0.pth
2023-12-31 02:28:57,479	44k	INFO	====> Epoch: 1, cost 74.84 s
2023-12-31 02:29:24,181	44k	INFO	====> Epoch: 2, cost 26.70 s
2023-12-31 02:29:50,724	44k	INFO	====> Epoch: 3, cost 26.54 s
2023-12-31 02:30:17,440	44k	INFO	====> Epoch: 4, cost 26.72 s
2023-12-31 02:30:44,142	44k	INFO	====> Epoch: 5, cost 26.70 s
2023-12-31 02:31:04,061	44k	INFO	Train Epoch: 6 [71%]
2023-12-31 02:31:04,062	44k	INFO	Losses: [2.2436447143554688, 2.5020015239715576, 4.501670837402344, 31.30000877380371, 1.0146403312683105], step: 200, lr: 9.970043085494672e-05, reference_loss: 41.56196594238281
2023-12-31 02:31:11,338	44k	INFO	====> Epoch: 6, cost 27.20 s
2023-12-31 02:31:38,225	44k	INFO	====> Epoch: 7, cost 26.89 s
2023-12-31 02:32:04,691	44k	INFO	====> Epoch: 8, cost 26.47 s
2023-12-31 02:32:31,330	44k	INFO	====> Epoch: 9, cost 26.64 s
2023-12-31 02:33:14,765	44k	INFO	{'train': {'log_interval': 200, 'eval_interval': 800, 'seed': 1234, 'epochs': 2000, 'learning_rate': 0.0001, 'betas': [0.8, 0.99], 'eps': 1e-09, 'batch_size': 14, 'fp16_run': False, 'half_type': 'fp16', 'lr_decay': 0.999875, 'segment_size': 10240, 'init_lr_ratio': 1, 'warmup_epochs': 0, 'c_mel': 45, 'c_kl': 1.0, 'use_sr': True, 'max_speclen': 512, 'port': '8001', 'keep_ckpts': 3, 'all_in_mem': True, 'vol_aug': False}, 'data': {'training_files': 'filelists/train.txt', 'validation_files': 'filelists/val.txt', 'max_wav_value': 32768.0, 'sampling_rate': 44100, 'filter_length': 2048, 'hop_length': 512, 'win_length': 2048, 'n_mel_channels': 80, 'mel_fmin': 0.0, 'mel_fmax': 22050, 'unit_interpolate_mode': 'nearest'}, 'model': {'inter_channels': 192, 'hidden_channels': 192, 'filter_channels': 768, 'n_heads': 2, 'n_layers': 6, 'kernel_size': 3, 'p_dropout': 0.1, 'resblock': '1', 'resblock_kernel_sizes': [3, 7, 11], 'resblock_dilation_sizes': [[1, 3, 5], [1, 3, 5], [1, 3, 5]], 'upsample_rates': [8, 8, 2, 2, 2], 'upsample_initial_channel': 512, 'upsample_kernel_sizes': [16, 16, 4, 4, 4], 'n_layers_q': 3, 'n_layers_trans_flow': 3, 'n_flow_layer': 4, 'use_spectral_norm': False, 'gin_channels': 768, 'ssl_dim': 768, 'n_speakers': 1, 'vocoder_name': 'nsf-hifigan', 'speech_encoder': 'vec768l12', 'speaker_embedding': False, 'vol_embedding': False, 'use_depthwise_conv': False, 'flow_share_parameter': False, 'use_automatic_f0_prediction': True, 'use_transformer_flow': False}, 'spk': {'Ganyu': 0}, 'model_dir': './logs/44k'}
2023-12-31 02:33:22,897	44k	INFO	Loaded checkpoint './logs/44k/G_1600.pth' (iteration 20)
2023-12-31 02:33:33,544	44k	INFO	Train Epoch: 1 [0%]
2023-12-31 02:33:33,545	44k	INFO	Losses: [6.07155704498291, 5.310981273651123, 0.5626048445701599, 31.62262725830078, 1.18911874294281], step: 0, lr: 9.976276699833672e-05, reference_loss: 44.75688934326172
2023-12-31 02:33:45,175	44k	INFO	Saving model and optimizer state at iteration 1 to ./logs/44k/G_0.pth
2023-12-31 02:33:48,674	44k	INFO	Saving model and optimizer state at iteration 1 to ./logs/44k/D_0.pth
2023-12-31 02:34:29,774	44k	INFO	====> Epoch: 1, cost 75.01 s
2023-12-31 02:34:56,492	44k	INFO	====> Epoch: 2, cost 26.72 s
2023-12-31 02:35:22,998	44k	INFO	====> Epoch: 3, cost 26.51 s
2023-12-31 02:35:49,408	44k	INFO	====> Epoch: 4, cost 26.41 s
2023-12-31 02:36:15,971	44k	INFO	====> Epoch: 5, cost 26.56 s
2023-12-31 02:36:35,664	44k	INFO	Train Epoch: 6 [71%]
2023-12-31 02:36:35,665	44k	INFO	Losses: [2.358379364013672, 2.626269817352295, 4.334921836853027, 31.563461303710938, 0.9844481945037842], step: 200, lr: 9.970043085494672e-05, reference_loss: 41.86748123168945
2023-12-31 02:36:43,027	44k	INFO	====> Epoch: 6, cost 27.06 s
2023-12-31 02:37:09,732	44k	INFO	====> Epoch: 7, cost 26.70 s
2023-12-31 02:37:36,236	44k	INFO	====> Epoch: 8, cost 26.50 s
2023-12-31 02:38:02,899	44k	INFO	====> Epoch: 9, cost 26.66 s
2023-12-31 02:38:29,327	44k	INFO	====> Epoch: 10, cost 26.43 s
2023-12-31 02:38:55,912	44k	INFO	====> Epoch: 11, cost 26.59 s
2023-12-31 02:39:08,148	44k	INFO	Train Epoch: 12 [43%]
2023-12-31 02:39:08,149	44k	INFO	Losses: [2.738971471786499, 2.358032464981079, 2.5263736248016357, 28.57539939880371, 1.0540879964828491], step: 400, lr: 9.962567889519979e-05, reference_loss: 37.252864837646484
2023-12-31 02:39:23,193	44k	INFO	====> Epoch: 12, cost 27.28 s
2023-12-31 02:39:49,856	44k	INFO	====> Epoch: 13, cost 26.66 s
2023-12-31 02:40:16,878	44k	INFO	====> Epoch: 14, cost 27.02 s
2023-12-31 02:40:43,361	44k	INFO	====> Epoch: 15, cost 26.48 s
2023-12-31 02:41:09,829	44k	INFO	====> Epoch: 16, cost 26.47 s
2023-12-31 02:41:36,348	44k	INFO	====> Epoch: 17, cost 26.52 s
2023-12-31 02:41:40,916	44k	INFO	Train Epoch: 18 [14%]
2023-12-31 02:41:40,917	44k	INFO	Losses: [2.683873176574707, 2.4807288646698, 3.7931861877441406, 30.654401779174805, 1.1471295356750488], step: 600, lr: 9.95509829819056e-05, reference_loss: 40.75931930541992
2023-12-31 02:42:03,381	44k	INFO	====> Epoch: 18, cost 27.03 s
2023-12-31 02:42:29,776	44k	INFO	====> Epoch: 19, cost 26.40 s
2023-12-31 02:42:56,391	44k	INFO	====> Epoch: 20, cost 26.61 s
2023-12-31 02:43:22,989	44k	INFO	====> Epoch: 21, cost 26.60 s
2023-12-31 02:43:49,903	44k	INFO	====> Epoch: 22, cost 26.91 s
2023-12-31 02:44:13,197	44k	INFO	Train Epoch: 23 [86%]
2023-12-31 02:44:13,197	44k	INFO	Losses: [2.5140607357025146, 2.667785406112671, 3.585768699645996, 28.55167579650879, 0.8698645234107971], step: 800, lr: 9.948877917043875e-05, reference_loss: 38.18915557861328
2023-12-31 02:44:20,300	44k	INFO	Saving model and optimizer state at iteration 23 to ./logs/44k/G_800.pth
2023-12-31 02:44:23,303	44k	INFO	Saving model and optimizer state at iteration 23 to ./logs/44k/D_800.pth
2023-12-31 02:44:29,463	44k	INFO	====> Epoch: 23, cost 39.56 s
2023-12-31 02:44:55,949	44k	INFO	====> Epoch: 24, cost 26.49 s
2023-12-31 02:45:22,478	44k	INFO	====> Epoch: 25, cost 26.53 s
2023-12-31 02:45:48,975	44k	INFO	====> Epoch: 26, cost 26.50 s
2023-12-31 02:46:15,729	44k	INFO	====> Epoch: 27, cost 26.75 s
2023-12-31 02:46:42,201	44k	INFO	====> Epoch: 28, cost 26.47 s
2023-12-31 02:46:58,158	44k	INFO	Train Epoch: 29 [57%]
2023-12-31 02:46:58,159	44k	INFO	Losses: [2.7040762901306152, 2.3833608627319336, 2.782041072845459, 27.778112411499023, 0.9932965636253357], step: 1000, lr: 9.941418589985758e-05, reference_loss: 36.64088821411133
2023-12-31 02:47:09,231	44k	INFO	====> Epoch: 29, cost 27.03 s
2023-12-31 02:47:35,712	44k	INFO	====> Epoch: 30, cost 26.48 s
2023-12-31 02:48:02,299	44k	INFO	====> Epoch: 31, cost 26.59 s
2023-12-31 02:48:28,675	44k	INFO	====> Epoch: 32, cost 26.38 s
2023-12-31 02:48:55,315	44k	INFO	====> Epoch: 33, cost 26.64 s
2023-12-31 02:49:22,017	44k	INFO	====> Epoch: 34, cost 26.70 s
2023-12-31 02:49:30,511	44k	INFO	Train Epoch: 35 [29%]
2023-12-31 02:49:30,511	44k	INFO	Losses: [2.62790846824646, 2.208394765853882, 2.928049087524414, 28.898448944091797, 0.6843893527984619], step: 1200, lr: 9.933964855674948e-05, reference_loss: 37.347190856933594
2023-12-31 02:49:49,558	44k	INFO	====> Epoch: 35, cost 27.54 s
2023-12-31 02:50:16,152	44k	INFO	====> Epoch: 36, cost 26.59 s
2023-12-31 02:50:42,541	44k	INFO	====> Epoch: 37, cost 26.39 s
2023-12-31 02:51:09,030	44k	INFO	====> Epoch: 38, cost 26.49 s
2023-12-31 02:51:35,545	44k	INFO	====> Epoch: 39, cost 26.51 s
2023-12-31 02:52:02,141	44k	INFO	====> Epoch: 40, cost 26.60 s
2023-12-31 02:52:02,916	44k	INFO	Train Epoch: 41 [0%]
2023-12-31 02:52:02,916	44k	INFO	Losses: [2.2773032188415527, 2.4444010257720947, 3.8377037048339844, 27.02399253845215, 0.7349629402160645], step: 1400, lr: 9.926516709918191e-05, reference_loss: 36.318363189697266
2023-12-31 02:52:29,703	44k	INFO	====> Epoch: 41, cost 27.56 s
2023-12-31 02:52:56,171	44k	INFO	====> Epoch: 42, cost 26.47 s
2023-12-31 02:53:22,705	44k	INFO	====> Epoch: 43, cost 26.53 s
2023-12-31 02:53:49,062	44k	INFO	====> Epoch: 44, cost 26.36 s
2023-12-31 02:54:15,803	44k	INFO	====> Epoch: 45, cost 26.74 s
2023-12-31 02:54:35,503	44k	INFO	Train Epoch: 46 [71%]
2023-12-31 02:54:35,504	44k	INFO	Losses: [2.443932056427002, 2.112372636795044, 3.251061201095581, 23.17296600341797, 1.2719072103500366], step: 1600, lr: 9.92031418779886e-05, reference_loss: 32.25223922729492
2023-12-31 02:54:42,754	44k	INFO	Saving model and optimizer state at iteration 46 to ./logs/44k/G_1600.pth
2023-12-31 02:54:45,707	44k	INFO	Saving model and optimizer state at iteration 46 to ./logs/44k/D_1600.pth
2023-12-31 02:54:55,722	44k	INFO	====> Epoch: 46, cost 39.92 s
2023-12-31 02:55:22,660	44k	INFO	====> Epoch: 47, cost 26.94 s
2023-12-31 02:55:49,178	44k	INFO	====> Epoch: 48, cost 26.52 s
2023-12-31 02:56:15,465	44k	INFO	====> Epoch: 49, cost 26.29 s
2023-12-31 02:56:42,050	44k	INFO	====> Epoch: 50, cost 26.58 s
2023-12-31 02:57:08,398	44k	INFO	====> Epoch: 51, cost 26.35 s
2023-12-31 02:57:20,648	44k	INFO	Train Epoch: 52 [43%]
2023-12-31 02:57:20,649	44k	INFO	Losses: [2.4507927894592285, 2.358691453933716, 3.801288366317749, 25.64254379272461, 1.1766892671585083], step: 1800, lr: 9.912876276844171e-05, reference_loss: 35.43000793457031
2023-12-31 02:57:35,546	44k	INFO	====> Epoch: 52, cost 27.15 s
2023-12-31 02:58:02,169	44k	INFO	====> Epoch: 53, cost 26.62 s
2023-12-31 02:58:28,817	44k	INFO	====> Epoch: 54, cost 26.65 s
2023-12-31 02:58:55,224	44k	INFO	====> Epoch: 55, cost 26.41 s
2023-12-31 02:59:21,891	44k	INFO	====> Epoch: 56, cost 26.67 s
2023-12-31 02:59:48,494	44k	INFO	====> Epoch: 57, cost 26.60 s
2023-12-31 02:59:53,061	44k	INFO	Train Epoch: 58 [14%]
2023-12-31 02:59:53,061	44k	INFO	Losses: [2.37243390083313, 2.4199705123901367, 3.988760232925415, 26.305870056152344, 1.0008862018585205], step: 2000, lr: 9.905443942579728e-05, reference_loss: 36.087921142578125
2023-12-31 03:00:15,673	44k	INFO	====> Epoch: 58, cost 27.18 s
2023-12-31 03:00:42,193	44k	INFO	====> Epoch: 59, cost 26.52 s
2023-12-31 03:01:08,625	44k	INFO	====> Epoch: 60, cost 26.43 s
2023-12-31 03:01:35,370	44k	INFO	====> Epoch: 61, cost 26.75 s
2023-12-31 03:02:01,858	44k	INFO	====> Epoch: 62, cost 26.49 s
2023-12-31 03:02:25,538	44k	INFO	Train Epoch: 63 [86%]
2023-12-31 03:02:25,539	44k	INFO	Losses: [2.3254261016845703, 2.4956281185150146, 5.448492527008057, 25.17543601989746, 1.078399896621704], step: 2200, lr: 9.899254587647776e-05, reference_loss: 36.52338409423828
2023-12-31 03:02:29,012	44k	INFO	====> Epoch: 63, cost 27.15 s
2023-12-31 03:02:55,570	44k	INFO	====> Epoch: 64, cost 26.56 s
2023-12-31 03:03:22,100	44k	INFO	====> Epoch: 65, cost 26.53 s
2023-12-31 03:03:48,525	44k	INFO	====> Epoch: 66, cost 26.43 s
2023-12-31 03:04:15,190	44k	INFO	====> Epoch: 67, cost 26.67 s
2023-12-31 03:04:41,638	44k	INFO	====> Epoch: 68, cost 26.45 s
2023-12-31 03:04:57,509	44k	INFO	Train Epoch: 69 [57%]
2023-12-31 03:04:57,509	44k	INFO	Losses: [2.373394250869751, 2.3982861042022705, 5.137417316436768, 28.95928955078125, 0.9027799963951111], step: 2400, lr: 9.891832466458178e-05, reference_loss: 39.77116394042969
2023-12-31 03:05:05,021	44k	INFO	Saving model and optimizer state at iteration 69 to ./logs/44k/G_2400.pth
2023-12-31 03:05:06,187	44k	INFO	Saving model and optimizer state at iteration 69 to ./logs/44k/D_2400.pth
2023-12-31 03:05:17,452	44k	INFO	====> Epoch: 69, cost 35.81 s
2023-12-31 03:05:44,075	44k	INFO	====> Epoch: 70, cost 26.62 s
2023-12-31 03:06:10,759	44k	INFO	====> Epoch: 71, cost 26.68 s
2023-12-31 03:06:37,113	44k	INFO	====> Epoch: 72, cost 26.35 s
2023-12-31 03:07:03,611	44k	INFO	====> Epoch: 73, cost 26.50 s
2023-12-31 03:07:29,763	44k	INFO	====> Epoch: 74, cost 26.15 s
2023-12-31 03:07:38,179	44k	INFO	Train Epoch: 75 [29%]
2023-12-31 03:07:38,179	44k	INFO	Losses: [2.3413262367248535, 2.0680809020996094, 4.048648834228516, 25.155229568481445, 0.9129629135131836], step: 2600, lr: 9.884415910120204e-05, reference_loss: 34.526248931884766
2023-12-31 03:07:57,122	44k	INFO	====> Epoch: 75, cost 27.36 s
2023-12-31 03:08:23,597	44k	INFO	====> Epoch: 76, cost 26.47 s
2023-12-31 03:08:50,273	44k	INFO	====> Epoch: 77, cost 26.68 s
2023-12-31 03:09:16,577	44k	INFO	====> Epoch: 78, cost 26.30 s
2023-12-31 03:09:42,952	44k	INFO	====> Epoch: 79, cost 26.37 s
2023-12-31 03:10:09,458	44k	INFO	====> Epoch: 80, cost 26.51 s
2023-12-31 03:10:10,229	44k	INFO	Train Epoch: 81 [0%]
2023-12-31 03:10:10,230	44k	INFO	Losses: [2.4643993377685547, 2.164592742919922, 4.088022708892822, 25.080799102783203, 1.0913677215576172], step: 2800, lr: 9.877004914461517e-05, reference_loss: 34.889183044433594
2023-12-31 03:10:36,738	44k	INFO	====> Epoch: 81, cost 27.28 s
2023-12-31 03:11:03,645	44k	INFO	====> Epoch: 82, cost 26.91 s
2023-12-31 03:11:30,120	44k	INFO	====> Epoch: 83, cost 26.48 s
2023-12-31 03:11:56,560	44k	INFO	====> Epoch: 84, cost 26.44 s
2023-12-31 03:12:22,880	44k	INFO	====> Epoch: 85, cost 26.32 s
2023-12-31 03:12:42,571	44k	INFO	Train Epoch: 86 [71%]
2023-12-31 03:12:42,572	44k	INFO	Losses: [2.2743752002716064, 2.3694067001342773, 5.418451309204102, 26.5684757232666, 1.1355810165405273], step: 3000, lr: 9.870833329479095e-05, reference_loss: 37.76628875732422
2023-12-31 03:12:49,855	44k	INFO	====> Epoch: 86, cost 26.98 s
2023-12-31 03:13:16,348	44k	INFO	====> Epoch: 87, cost 26.49 s
2023-12-31 03:13:42,965	44k	INFO	====> Epoch: 88, cost 26.62 s
2023-12-31 03:14:09,707	44k	INFO	====> Epoch: 89, cost 26.74 s
2023-12-31 03:14:36,029	44k	INFO	====> Epoch: 90, cost 26.32 s
2023-12-31 03:15:02,484	44k	INFO	====> Epoch: 91, cost 26.45 s
2023-12-31 03:15:14,660	44k	INFO	Train Epoch: 92 [43%]
2023-12-31 03:15:14,660	44k	INFO	Losses: [2.635450601577759, 1.866696834564209, 3.6110353469848633, 24.41246795654297, 0.8095946311950684], step: 3200, lr: 9.863432517573002e-05, reference_loss: 33.335243225097656
2023-12-31 03:15:21,981	44k	INFO	Saving model and optimizer state at iteration 92 to ./logs/44k/G_3200.pth
2023-12-31 03:15:23,178	44k	INFO	Saving model and optimizer state at iteration 92 to ./logs/44k/D_3200.pth
2023-12-31 03:15:23,950	44k	INFO	.. Free up space by deleting ckpt ./logs/44k/G_800.pth
2023-12-31 03:15:24,007	44k	INFO	.. Free up space by deleting ckpt ./logs/44k/D_800.pth
2023-12-31 03:15:38,459	44k	INFO	====> Epoch: 92, cost 35.98 s
2023-12-31 03:16:04,966	44k	INFO	====> Epoch: 93, cost 26.51 s
2023-12-31 03:16:31,801	44k	INFO	====> Epoch: 94, cost 26.83 s
2023-12-31 03:16:58,202	44k	INFO	====> Epoch: 95, cost 26.40 s
2023-12-31 03:17:24,842	44k	INFO	====> Epoch: 96, cost 26.64 s
2023-12-31 03:17:51,249	44k	INFO	====> Epoch: 97, cost 26.41 s
2023-12-31 03:17:55,817	44k	INFO	Train Epoch: 98 [14%]
2023-12-31 03:17:55,818	44k	INFO	Losses: [2.671356439590454, 1.9330915212631226, 3.40095853805542, 27.27381706237793, 1.0391457080841064], step: 3400, lr: 9.85603725454156e-05, reference_loss: 36.3183708190918
2023-12-31 03:18:18,389	44k	INFO	====> Epoch: 98, cost 27.14 s
2023-12-31 03:18:44,997	44k	INFO	====> Epoch: 99, cost 26.61 s
2023-12-31 03:19:11,734	44k	INFO	====> Epoch: 100, cost 26.74 s
2023-12-31 03:19:38,058	44k	INFO	====> Epoch: 101, cost 26.32 s
2023-12-31 03:20:04,631	44k	INFO	====> Epoch: 102, cost 26.57 s
2023-12-31 03:20:28,416	44k	INFO	Train Epoch: 103 [86%]
2023-12-31 03:20:28,416	44k	INFO	Losses: [2.265285015106201, 2.52199125289917, 5.95382833480835, 26.602706909179688, 1.259446382522583], step: 3600, lr: 9.8498787710708e-05, reference_loss: 38.60325622558594
2023-12-31 03:20:31,933	44k	INFO	====> Epoch: 103, cost 27.30 s
2023-12-31 03:20:58,518	44k	INFO	====> Epoch: 104, cost 26.58 s
2023-12-31 03:21:25,062	44k	INFO	====> Epoch: 105, cost 26.54 s
2023-12-31 03:21:51,757	44k	INFO	====> Epoch: 106, cost 26.70 s
2023-12-31 03:22:18,333	44k	INFO	====> Epoch: 107, cost 26.58 s
2023-12-31 03:22:44,944	44k	INFO	====> Epoch: 108, cost 26.61 s
2023-12-31 03:23:00,991	44k	INFO	Train Epoch: 109 [57%]
2023-12-31 03:23:00,991	44k	INFO	Losses: [2.6440892219543457, 1.961550235748291, 3.053866147994995, 22.004535675048828, 0.9672760367393494], step: 3800, lr: 9.842493670173108e-05, reference_loss: 30.631317138671875
2023-12-31 03:23:12,493	44k	INFO	====> Epoch: 109, cost 27.55 s
2023-12-31 03:23:38,966	44k	INFO	====> Epoch: 110, cost 26.47 s
2023-12-31 03:24:05,431	44k	INFO	====> Epoch: 111, cost 26.47 s
2023-12-31 03:24:31,962	44k	INFO	====> Epoch: 112, cost 26.53 s
2023-12-31 03:24:58,410	44k	INFO	====> Epoch: 113, cost 26.45 s
2023-12-31 03:25:24,907	44k	INFO	====> Epoch: 114, cost 26.50 s
2023-12-31 03:25:33,331	44k	INFO	Train Epoch: 115 [29%]
2023-12-31 03:25:33,332	44k	INFO	Losses: [2.3862645626068115, 2.3561954498291016, 4.0740556716918945, 24.219722747802734, 1.1041208505630493], step: 4000, lr: 9.835114106370493e-05, reference_loss: 34.14036178588867
2023-12-31 03:25:40,510	44k	INFO	Saving model and optimizer state at iteration 115 to ./logs/44k/G_4000.pth
2023-12-31 03:25:42,030	44k	INFO	Saving model and optimizer state at iteration 115 to ./logs/44k/D_4000.pth
2023-12-31 03:25:42,807	44k	INFO	.. Free up space by deleting ckpt ./logs/44k/G_1600.pth
2023-12-31 03:25:42,864	44k	INFO	.. Free up space by deleting ckpt ./logs/44k/D_1600.pth
2023-12-31 03:26:00,935	44k	INFO	====> Epoch: 115, cost 36.03 s
2023-12-31 03:26:27,463	44k	INFO	====> Epoch: 116, cost 26.53 s
2023-12-31 03:26:54,118	44k	INFO	====> Epoch: 117, cost 26.65 s
2023-12-31 03:27:20,730	44k	INFO	====> Epoch: 118, cost 26.61 s
2023-12-31 03:27:47,216	44k	INFO	====> Epoch: 119, cost 26.49 s
2023-12-31 03:28:13,713	44k	INFO	====> Epoch: 120, cost 26.50 s
2023-12-31 03:28:14,488	44k	INFO	Train Epoch: 121 [0%]
2023-12-31 03:28:14,489	44k	INFO	Losses: [2.8694329261779785, 2.2673838138580322, 4.286513805389404, 24.569866180419922, 1.1434270143508911], step: 4200, lr: 9.827740075511432e-05, reference_loss: 35.13662338256836
2023-12-31 03:28:40,807	44k	INFO	====> Epoch: 121, cost 27.09 s
2023-12-31 03:29:07,610	44k	INFO	====> Epoch: 122, cost 26.80 s
2023-12-31 03:29:34,133	44k	INFO	====> Epoch: 123, cost 26.52 s
2023-12-31 03:30:00,643	44k	INFO	====> Epoch: 124, cost 26.51 s
2023-12-31 03:30:27,244	44k	INFO	====> Epoch: 125, cost 26.60 s
2023-12-31 03:30:46,750	44k	INFO	Train Epoch: 126 [71%]
2023-12-31 03:30:46,751	44k	INFO	Losses: [2.554579257965088, 2.24214243888855, 3.6388344764709473, 21.918861389160156, 0.9199371933937073], step: 4400, lr: 9.821599273356685e-05, reference_loss: 31.274354934692383
2023-12-31 03:30:53,935	44k	INFO	====> Epoch: 126, cost 26.69 s
2023-12-31 03:31:20,397	44k	INFO	====> Epoch: 127, cost 26.46 s
2023-12-31 03:31:46,894	44k	INFO	====> Epoch: 128, cost 26.50 s
2023-12-31 03:32:13,774	44k	INFO	====> Epoch: 129, cost 26.88 s
2023-12-31 03:32:40,365	44k	INFO	====> Epoch: 130, cost 26.59 s
2023-12-31 03:33:06,811	44k	INFO	====> Epoch: 131, cost 26.45 s
2023-12-31 03:33:18,863	44k	INFO	Train Epoch: 132 [43%]
2023-12-31 03:33:18,863	44k	INFO	Losses: [2.0887367725372314, 2.532480478286743, 5.362691879272461, 23.538002014160156, 1.0494132041931152], step: 4600, lr: 9.814235375455375e-05, reference_loss: 34.57132339477539
2023-12-31 03:33:33,802	44k	INFO	====> Epoch: 132, cost 26.99 s
2023-12-31 03:34:00,325	44k	INFO	====> Epoch: 133, cost 26.52 s
2023-12-31 03:34:26,694	44k	INFO	====> Epoch: 134, cost 26.37 s
2023-12-31 03:34:53,139	44k	INFO	====> Epoch: 135, cost 26.45 s
2023-12-31 03:35:20,016	44k	INFO	====> Epoch: 136, cost 26.88 s
2023-12-31 03:35:46,607	44k	INFO	====> Epoch: 137, cost 26.59 s
2023-12-31 03:35:51,208	44k	INFO	Train Epoch: 138 [14%]
2023-12-31 03:35:51,208	44k	INFO	Losses: [2.4764044284820557, 2.069962501525879, 3.999067544937134, 24.204940795898438, 0.9065488576889038], step: 4800, lr: 9.806876998751865e-05, reference_loss: 33.65692138671875
2023-12-31 03:35:58,362	44k	INFO	Saving model and optimizer state at iteration 138 to ./logs/44k/G_4800.pth
2023-12-31 03:35:59,553	44k	INFO	Saving model and optimizer state at iteration 138 to ./logs/44k/D_4800.pth
2023-12-31 03:36:00,326	44k	INFO	.. Free up space by deleting ckpt ./logs/44k/G_2400.pth
2023-12-31 03:36:00,382	44k	INFO	.. Free up space by deleting ckpt ./logs/44k/D_2400.pth
2023-12-31 03:36:22,363	44k	INFO	====> Epoch: 138, cost 35.76 s
2023-12-31 03:36:48,970	44k	INFO	====> Epoch: 139, cost 26.61 s
2023-12-31 03:37:15,410	44k	INFO	====> Epoch: 140, cost 26.44 s
2023-12-31 03:37:41,830	44k	INFO	====> Epoch: 141, cost 26.42 s
2023-12-31 03:38:08,642	44k	INFO	====> Epoch: 142, cost 26.81 s
2023-12-31 03:38:31,963	44k	INFO	Train Epoch: 143 [86%]
2023-12-31 03:38:31,963	44k	INFO	Losses: [2.6832876205444336, 2.174070119857788, 5.036392688751221, 24.043764114379883, 0.8170682787895203], step: 5000, lr: 9.800749232760646e-05, reference_loss: 34.75458526611328
2023-12-31 03:38:35,439	44k	INFO	====> Epoch: 143, cost 26.80 s
2023-12-31 03:39:01,996	44k	INFO	====> Epoch: 144, cost 26.56 s
2023-12-31 03:39:28,493	44k	INFO	====> Epoch: 145, cost 26.50 s
2023-12-31 03:39:55,028	44k	INFO	====> Epoch: 146, cost 26.53 s
2023-12-31 03:40:21,498	44k	INFO	====> Epoch: 147, cost 26.47 s
2023-12-31 03:40:48,062	44k	INFO	====> Epoch: 148, cost 26.56 s
2023-12-31 03:41:04,097	44k	INFO	Train Epoch: 149 [57%]
2023-12-31 03:41:04,097	44k	INFO	Losses: [2.2056217193603516, 2.3276352882385254, 4.882293224334717, 23.468196868896484, 0.9577615261077881], step: 5200, lr: 9.79340096750387e-05, reference_loss: 33.84150695800781
2023-12-31 03:41:15,445	44k	INFO	====> Epoch: 149, cost 27.38 s
2023-12-31 03:41:41,978	44k	INFO	====> Epoch: 150, cost 26.53 s
2023-12-31 03:42:08,474	44k	INFO	====> Epoch: 151, cost 26.50 s
2023-12-31 03:42:34,936	44k	INFO	====> Epoch: 152, cost 26.46 s
2023-12-31 03:43:01,459	44k	INFO	====> Epoch: 153, cost 26.52 s
2023-12-31 03:43:28,042	44k	INFO	====> Epoch: 154, cost 26.58 s
2023-12-31 03:43:36,477	44k	INFO	Train Epoch: 155 [29%]
2023-12-31 03:43:36,477	44k	INFO	Losses: [2.777224063873291, 2.620361566543579, 3.9834213256835938, 23.03413963317871, 1.1539250612258911], step: 5400, lr: 9.786058211724074e-05, reference_loss: 33.56907272338867
2023-12-31 03:43:55,355	44k	INFO	====> Epoch: 155, cost 27.31 s
2023-12-31 03:44:21,762	44k	INFO	====> Epoch: 156, cost 26.41 s
2023-12-31 03:44:48,389	44k	INFO	====> Epoch: 157, cost 26.63 s
2023-12-31 03:45:14,865	44k	INFO	====> Epoch: 158, cost 26.48 s
2023-12-31 03:45:41,264	44k	INFO	====> Epoch: 159, cost 26.40 s
2023-12-31 03:46:07,734	44k	INFO	====> Epoch: 160, cost 26.47 s
2023-12-31 03:46:08,496	44k	INFO	Train Epoch: 161 [0%]
2023-12-31 03:46:08,497	44k	INFO	Losses: [2.3528358936309814, 2.1854796409606934, 4.4831390380859375, 25.35540199279785, 0.8154336810112], step: 5600, lr: 9.778720961290439e-05, reference_loss: 35.192291259765625
2023-12-31 03:46:15,654	44k	INFO	Saving model and optimizer state at iteration 161 to ./logs/44k/G_5600.pth
2023-12-31 03:46:16,859	44k	INFO	Saving model and optimizer state at iteration 161 to ./logs/44k/D_5600.pth
2023-12-31 03:46:17,632	44k	INFO	.. Free up space by deleting ckpt ./logs/44k/G_3200.pth
2023-12-31 03:46:17,689	44k	INFO	.. Free up space by deleting ckpt ./logs/44k/D_3200.pth
2023-12-31 03:46:43,776	44k	INFO	====> Epoch: 161, cost 36.04 s
2023-12-31 03:47:10,360	44k	INFO	====> Epoch: 162, cost 26.58 s
2023-12-31 03:47:36,840	44k	INFO	====> Epoch: 163, cost 26.48 s
2023-12-31 03:48:03,374	44k	INFO	====> Epoch: 164, cost 26.53 s
2023-12-31 03:48:29,759	44k	INFO	====> Epoch: 165, cost 26.39 s
2023-12-31 03:48:49,533	44k	INFO	Train Epoch: 166 [71%]
2023-12-31 03:48:49,533	44k	INFO	Losses: [2.5114119052886963, 2.291395902633667, 4.864213943481445, 23.72030258178711, 0.9400205612182617], step: 5800, lr: 9.772610788423802e-05, reference_loss: 34.32734680175781
2023-12-31 03:48:56,927	44k	INFO	====> Epoch: 166, cost 27.17 s
2023-12-31 03:49:23,435	44k	INFO	====> Epoch: 167, cost 26.51 s
2023-12-31 03:49:49,867	44k	INFO	====> Epoch: 168, cost 26.43 s
2023-12-31 03:50:16,430	44k	INFO	====> Epoch: 169, cost 26.56 s
2023-12-31 03:50:43,222	44k	INFO	====> Epoch: 170, cost 26.79 s
2023-12-31 03:51:09,624	44k	INFO	====> Epoch: 171, cost 26.40 s
2023-12-31 03:51:21,652	44k	INFO	Train Epoch: 172 [43%]
2023-12-31 03:51:21,653	44k	INFO	Losses: [2.286736249923706, 2.0425775051116943, 4.260996341705322, 22.073144912719727, 0.9559341073036194], step: 6000, lr: 9.765283620406429e-05, reference_loss: 31.6193904876709
2023-12-31 03:51:36,569	44k	INFO	====> Epoch: 172, cost 26.94 s
2023-12-31 03:52:03,185	44k	INFO	====> Epoch: 173, cost 26.62 s
2023-12-31 03:52:29,660	44k	INFO	====> Epoch: 174, cost 26.48 s
2023-12-31 03:52:56,252	44k	INFO	====> Epoch: 175, cost 26.59 s
2023-12-31 03:53:22,812	44k	INFO	====> Epoch: 176, cost 26.56 s
2023-12-31 03:53:49,634	44k	INFO	====> Epoch: 177, cost 26.82 s
2023-12-31 03:53:54,276	44k	INFO	Train Epoch: 178 [14%]
2023-12-31 03:53:54,277	44k	INFO	Losses: [2.7350733280181885, 2.276388645172119, 4.119645595550537, 24.381486892700195, 1.0912741422653198], step: 6200, lr: 9.757961946048049e-05, reference_loss: 34.6038703918457
2023-12-31 03:54:16,769	44k	INFO	====> Epoch: 178, cost 27.13 s
2023-12-31 03:54:43,365	44k	INFO	====> Epoch: 179, cost 26.60 s
2023-12-31 03:55:09,914	44k	INFO	====> Epoch: 180, cost 26.55 s
2023-12-31 03:55:36,544	44k	INFO	====> Epoch: 181, cost 26.63 s
2023-12-31 03:56:03,007	44k	INFO	====> Epoch: 182, cost 26.46 s
2023-12-31 03:56:26,322	44k	INFO	Train Epoch: 183 [86%]
2023-12-31 03:56:26,323	44k	INFO	Losses: [2.5530025959014893, 2.3819518089294434, 4.369139671325684, 21.3426570892334, 0.7427768111228943], step: 6400, lr: 9.75186474432275e-05, reference_loss: 31.389528274536133
2023-12-31 03:56:33,748	44k	INFO	Saving model and optimizer state at iteration 183 to ./logs/44k/G_6400.pth
2023-12-31 03:56:34,926	44k	INFO	Saving model and optimizer state at iteration 183 to ./logs/44k/D_6400.pth
2023-12-31 03:56:35,703	44k	INFO	.. Free up space by deleting ckpt ./logs/44k/G_4000.pth
2023-12-31 03:56:35,761	44k	INFO	.. Free up space by deleting ckpt ./logs/44k/D_4000.pth
2023-12-31 03:56:38,805	44k	INFO	====> Epoch: 183, cost 35.80 s
2023-12-31 03:57:05,148	44k	INFO	====> Epoch: 184, cost 26.34 s
2023-12-31 03:57:31,627	44k	INFO	====> Epoch: 185, cost 26.48 s
2023-12-31 03:57:57,847	44k	INFO	====> Epoch: 186, cost 26.22 s
2023-12-31 03:58:24,536	44k	INFO	====> Epoch: 187, cost 26.69 s
2023-12-31 03:58:50,847	44k	INFO	====> Epoch: 188, cost 26.31 s
2023-12-31 03:59:06,648	44k	INFO	Train Epoch: 189 [57%]
2023-12-31 03:59:06,648	44k	INFO	Losses: [2.592385768890381, 2.0072436332702637, 3.2646028995513916, 20.967851638793945, 1.1098421812057495], step: 6600, lr: 9.744553130976908e-05, reference_loss: 29.941926956176758
2023-12-31 03:59:18,001	44k	INFO	====> Epoch: 189, cost 27.15 s
2023-12-31 03:59:44,495	44k	INFO	====> Epoch: 190, cost 26.49 s
2023-12-31 04:00:11,036	44k	INFO	====> Epoch: 191, cost 26.54 s
2023-12-31 04:00:37,586	44k	INFO	====> Epoch: 192, cost 26.55 s
2023-12-31 04:01:04,040	44k	INFO	====> Epoch: 193, cost 26.45 s
2023-12-31 04:01:30,533	44k	INFO	====> Epoch: 194, cost 26.49 s
2023-12-31 04:01:38,988	44k	INFO	Train Epoch: 195 [29%]
2023-12-31 04:01:38,989	44k	INFO	Losses: [2.4139111042022705, 2.1979875564575195, 4.3195624351501465, 22.44586181640625, 0.696085512638092], step: 6800, lr: 9.7372469996277e-05, reference_loss: 32.07341003417969
2023-12-31 04:01:57,532	44k	INFO	====> Epoch: 195, cost 27.00 s
2023-12-31 04:02:24,339	44k	INFO	====> Epoch: 196, cost 26.81 s
2023-12-31 04:02:50,839	44k	INFO	====> Epoch: 197, cost 26.50 s
2023-12-31 04:03:17,337	44k	INFO	====> Epoch: 198, cost 26.50 s
2023-12-31 04:03:43,790	44k	INFO	====> Epoch: 199, cost 26.45 s
2023-12-31 04:04:10,217	44k	INFO	====> Epoch: 200, cost 26.43 s
2023-12-31 04:04:10,983	44k	INFO	Train Epoch: 201 [0%]
2023-12-31 04:04:10,984	44k	INFO	Losses: [2.2310380935668945, 2.5691323280334473, 5.309297561645508, 21.70378303527832, 0.7872066497802734], step: 7000, lr: 9.729946346164919e-05, reference_loss: 32.60045623779297
2023-12-31 04:04:37,281	44k	INFO	====> Epoch: 201, cost 27.06 s
2023-12-31 04:05:03,937	44k	INFO	====> Epoch: 202, cost 26.66 s
2023-12-31 04:05:30,689	44k	INFO	====> Epoch: 203, cost 26.75 s
2023-12-31 04:05:57,205	44k	INFO	====> Epoch: 204, cost 26.52 s
2023-12-31 04:06:23,736	44k	INFO	====> Epoch: 205, cost 26.53 s
2023-12-31 04:06:43,482	44k	INFO	Train Epoch: 206 [71%]
2023-12-31 04:06:43,483	44k	INFO	Losses: [2.721409797668457, 1.9889171123504639, 2.854830265045166, 19.164653778076172, 1.1276273727416992], step: 7200, lr: 9.723866649812655e-05, reference_loss: 27.857437133789062
2023-12-31 04:06:50,611	44k	INFO	Saving model and optimizer state at iteration 206 to ./logs/44k/G_7200.pth
2023-12-31 04:06:51,789	44k	INFO	Saving model and optimizer state at iteration 206 to ./logs/44k/D_7200.pth
2023-12-31 04:06:52,566	44k	INFO	.. Free up space by deleting ckpt ./logs/44k/G_4800.pth
2023-12-31 04:06:52,622	44k	INFO	.. Free up space by deleting ckpt ./logs/44k/D_4800.pth
2023-12-31 04:06:59,353	44k	INFO	====> Epoch: 206, cost 35.62 s
2023-12-31 04:07:25,759	44k	INFO	====> Epoch: 207, cost 26.41 s
2023-12-31 04:07:52,271	44k	INFO	====> Epoch: 208, cost 26.51 s
2023-12-31 04:08:19,056	44k	INFO	====> Epoch: 209, cost 26.78 s
2023-12-31 04:08:45,639	44k	INFO	====> Epoch: 210, cost 26.58 s
2023-12-31 04:09:11,965	44k	INFO	====> Epoch: 211, cost 26.33 s
2023-12-31 04:09:24,189	44k	INFO	Train Epoch: 212 [43%]
2023-12-31 04:09:24,189	44k	INFO	Losses: [2.2861695289611816, 2.2287838459014893, 4.219061851501465, 22.827800750732422, 1.097371220588684], step: 7400, lr: 9.716576028476738e-05, reference_loss: 32.65918731689453
2023-12-31 04:09:38,963	44k	INFO	====> Epoch: 212, cost 27.00 s
2023-12-31 04:10:05,564	44k	INFO	====> Epoch: 213, cost 26.60 s
2023-12-31 04:10:32,103	44k	INFO	====> Epoch: 214, cost 26.54 s
2023-12-31 04:10:58,498	44k	INFO	====> Epoch: 215, cost 26.40 s
2023-12-31 04:11:25,025	44k	INFO	====> Epoch: 216, cost 26.53 s
2023-12-31 04:11:51,926	44k	INFO	====> Epoch: 217, cost 26.90 s
2023-12-31 04:11:56,532	44k	INFO	Train Epoch: 218 [14%]
2023-12-31 04:11:56,532	44k	INFO	Losses: [2.5994391441345215, 2.390442132949829, 4.81318473815918, 24.311283111572266, 0.8672403693199158], step: 7600, lr: 9.709290873398365e-05, reference_loss: 34.981590270996094
2023-12-31 04:12:19,246	44k	INFO	====> Epoch: 218, cost 27.32 s
2023-12-31 04:12:45,816	44k	INFO	====> Epoch: 219, cost 26.57 s
2023-12-31 04:13:12,291	44k	INFO	====> Epoch: 220, cost 26.48 s
2023-12-31 04:13:38,918	44k	INFO	====> Epoch: 221, cost 26.63 s
2023-12-31 04:14:05,376	44k	INFO	====> Epoch: 222, cost 26.46 s
2023-12-31 04:14:28,882	44k	INFO	Train Epoch: 223 [86%]
2023-12-31 04:14:28,883	44k	INFO	Losses: [2.459350109100342, 2.351107358932495, 5.673123359680176, 22.224956512451172, 0.9064956903457642], step: 7800, lr: 9.703224083489565e-05, reference_loss: 33.61503219604492
2023-12-31 04:14:32,797	44k	INFO	====> Epoch: 223, cost 27.42 s
2023-12-31 04:14:59,459	44k	INFO	====> Epoch: 224, cost 26.66 s
2023-12-31 04:15:26,058	44k	INFO	====> Epoch: 225, cost 26.60 s
2023-12-31 04:15:52,461	44k	INFO	====> Epoch: 226, cost 26.40 s
2023-12-31 04:16:18,984	44k	INFO	====> Epoch: 227, cost 26.52 s
2023-12-31 04:16:45,326	44k	INFO	====> Epoch: 228, cost 26.34 s
2023-12-31 04:17:01,297	44k	INFO	Train Epoch: 229 [57%]
2023-12-31 04:17:01,298	44k	INFO	Losses: [2.3900389671325684, 2.824512481689453, 6.2538042068481445, 24.690418243408203, 0.812725305557251], step: 8000, lr: 9.695948939241093e-05, reference_loss: 36.971500396728516
2023-12-31 04:17:08,437	44k	INFO	Saving model and optimizer state at iteration 229 to ./logs/44k/G_8000.pth
2023-12-31 04:17:09,934	44k	INFO	Saving model and optimizer state at iteration 229 to ./logs/44k/D_8000.pth
2023-12-31 04:17:10,704	44k	INFO	.. Free up space by deleting ckpt ./logs/44k/G_5600.pth
2023-12-31 04:17:10,761	44k	INFO	.. Free up space by deleting ckpt ./logs/44k/D_5600.pth
2023-12-31 04:17:21,315	44k	INFO	====> Epoch: 229, cost 35.99 s
2023-12-31 04:17:47,973	44k	INFO	====> Epoch: 230, cost 26.66 s
2023-12-31 04:18:14,524	44k	INFO	====> Epoch: 231, cost 26.55 s
2023-12-31 04:18:41,086	44k	INFO	====> Epoch: 232, cost 26.56 s
2023-12-31 04:19:07,359	44k	INFO	====> Epoch: 233, cost 26.27 s
2023-12-31 04:19:33,818	44k	INFO	====> Epoch: 234, cost 26.46 s
2023-12-31 04:19:42,163	44k	INFO	Train Epoch: 235 [29%]
2023-12-31 04:19:42,163	44k	INFO	Losses: [2.514955759048462, 2.147456169128418, 4.197932243347168, 21.96778106689453, 0.753957211971283], step: 8200, lr: 9.68867924964598e-05, reference_loss: 31.582082748413086
2023-12-31 04:20:00,684	44k	INFO	====> Epoch: 235, cost 26.87 s
2023-12-31 04:20:27,525	44k	INFO	====> Epoch: 236, cost 26.84 s
2023-12-31 04:20:54,094	44k	INFO	====> Epoch: 237, cost 26.57 s
2023-12-31 04:21:20,430	44k	INFO	====> Epoch: 238, cost 26.34 s
2023-12-31 04:21:46,689	44k	INFO	====> Epoch: 239, cost 26.26 s
2023-12-31 04:22:13,154	44k	INFO	====> Epoch: 240, cost 26.46 s
2023-12-31 04:22:13,922	44k	INFO	Train Epoch: 241 [0%]
2023-12-31 04:22:13,923	44k	INFO	Losses: [2.428443193435669, 2.2830350399017334, 4.834761142730713, 21.10637664794922, 0.9849429726600647], step: 8400, lr: 9.681415010614512e-05, reference_loss: 31.63755989074707
2023-12-31 04:22:40,169	44k	INFO	====> Epoch: 241, cost 27.02 s
2023-12-31 04:23:06,816	44k	INFO	====> Epoch: 242, cost 26.65 s
2023-12-31 04:23:33,670	44k	INFO	====> Epoch: 243, cost 26.85 s
2023-12-31 04:24:00,263	44k	INFO	====> Epoch: 244, cost 26.59 s
2023-12-31 04:24:26,683	44k	INFO	====> Epoch: 245, cost 26.42 s
2023-12-31 04:24:46,428	44k	INFO	Train Epoch: 246 [71%]
2023-12-31 04:24:46,428	44k	INFO	Losses: [2.2902464866638184, 2.4817450046539307, 5.92686653137207, 25.187902450561523, 1.0745247602462769], step: 8600, lr: 9.675365638764893e-05, reference_loss: 36.96128463745117
2023-12-31 04:24:53,641	44k	INFO	====> Epoch: 246, cost 26.96 s
2023-12-31 04:25:20,065	44k	INFO	====> Epoch: 247, cost 26.42 s
2023-12-31 04:25:46,482	44k	INFO	====> Epoch: 248, cost 26.42 s
2023-12-31 04:26:13,047	44k	INFO	====> Epoch: 249, cost 26.57 s
2023-12-31 04:26:39,902	44k	INFO	====> Epoch: 250, cost 26.86 s
2023-12-31 04:27:06,499	44k	INFO	====> Epoch: 251, cost 26.60 s
2023-12-31 04:27:18,672	44k	INFO	Train Epoch: 252 [43%]
2023-12-31 04:27:18,673	44k	INFO	Losses: [2.6129913330078125, 2.0294227600097656, 4.311921119689941, 21.016632080078125, 0.7289523482322693], step: 8800, lr: 9.668111381821731e-05, reference_loss: 30.699918746948242
2023-12-31 04:27:25,917	44k	INFO	Saving model and optimizer state at iteration 252 to ./logs/44k/G_8800.pth
2023-12-31 04:27:27,088	44k	INFO	Saving model and optimizer state at iteration 252 to ./logs/44k/D_8800.pth
2023-12-31 04:27:27,859	44k	INFO	.. Free up space by deleting ckpt ./logs/44k/G_6400.pth
2023-12-31 04:27:27,916	44k	INFO	.. Free up space by deleting ckpt ./logs/44k/D_6400.pth
2023-12-31 04:27:42,334	44k	INFO	====> Epoch: 252, cost 35.84 s
2023-12-31 04:28:08,807	44k	INFO	====> Epoch: 253, cost 26.47 s
2023-12-31 04:28:35,371	44k	INFO	====> Epoch: 254, cost 26.56 s
2023-12-31 04:29:01,746	44k	INFO	====> Epoch: 255, cost 26.37 s
2023-12-31 04:29:28,457	44k	INFO	====> Epoch: 256, cost 26.71 s
2023-12-31 04:29:55,102	44k	INFO	====> Epoch: 257, cost 26.65 s
2023-12-31 04:29:59,712	44k	INFO	Train Epoch: 258 [14%]
2023-12-31 04:29:59,712	44k	INFO	Losses: [2.446162462234497, 2.171355724334717, 4.414381980895996, 25.35523796081543, 0.960429310798645], step: 9000, lr: 9.660862563871342e-05, reference_loss: 35.34756851196289
2023-12-31 04:30:22,275	44k	INFO	====> Epoch: 258, cost 27.17 s
2023-12-31 04:30:48,783	44k	INFO	====> Epoch: 259, cost 26.51 s
2023-12-31 04:31:15,246	44k	INFO	====> Epoch: 260, cost 26.46 s
2023-12-31 04:31:41,818	44k	INFO	====> Epoch: 261, cost 26.57 s
2023-12-31 04:32:08,428	44k	INFO	====> Epoch: 262, cost 26.61 s
2023-12-31 04:32:31,925	44k	INFO	Train Epoch: 263 [86%]
2023-12-31 04:32:31,925	44k	INFO	Losses: [2.2595245838165283, 2.463090181350708, 6.882736682891846, 24.8547306060791, 1.1137837171554565], step: 9200, lr: 9.65482603409002e-05, reference_loss: 37.57386779785156
2023-12-31 04:32:35,692	44k	INFO	====> Epoch: 263, cost 27.26 s
2023-12-31 04:33:02,221	44k	INFO	====> Epoch: 264, cost 26.53 s
2023-12-31 04:33:28,836	44k	INFO	====> Epoch: 265, cost 26.62 s
2023-12-31 04:33:55,428	44k	INFO	====> Epoch: 266, cost 26.59 s
2023-12-31 04:34:21,965	44k	INFO	====> Epoch: 267, cost 26.54 s
2023-12-31 04:34:48,551	44k	INFO	====> Epoch: 268, cost 26.59 s
2023-12-31 04:35:04,606	44k	INFO	Train Epoch: 269 [57%]
2023-12-31 04:35:04,607	44k	INFO	Losses: [2.7391040325164795, 2.040724754333496, 3.085279941558838, 18.274599075317383, 0.7773904800415039], step: 9400, lr: 9.647587177037196e-05, reference_loss: 26.917098999023438
2023-12-31 04:35:15,706	44k	INFO	====> Epoch: 269, cost 27.16 s
2023-12-31 04:35:42,039	44k	INFO	====> Epoch: 270, cost 26.33 s
2023-12-31 04:36:08,196	44k	INFO	====> Epoch: 271, cost 26.16 s
2023-12-31 04:36:34,277	44k	INFO	====> Epoch: 272, cost 26.08 s
2023-12-31 04:37:00,475	44k	INFO	====> Epoch: 273, cost 26.20 s
2023-12-31 04:37:26,724	44k	INFO	====> Epoch: 274, cost 26.25 s
2023-12-31 04:37:35,047	44k	INFO	Train Epoch: 275 [29%]
2023-12-31 04:37:35,048	44k	INFO	Losses: [2.1547446250915527, 2.486299753189087, 5.4083781242370605, 22.821969985961914, 1.0068093538284302], step: 9600, lr: 9.640353747430838e-05, reference_loss: 33.87820053100586
2023-12-31 04:37:42,199	44k	INFO	Saving model and optimizer state at iteration 275 to ./logs/44k/G_9600.pth
2023-12-31 04:37:43,377	44k	INFO	Saving model and optimizer state at iteration 275 to ./logs/44k/D_9600.pth
2023-12-31 04:37:44,149	44k	INFO	.. Free up space by deleting ckpt ./logs/44k/G_7200.pth
2023-12-31 04:37:44,206	44k	INFO	.. Free up space by deleting ckpt ./logs/44k/D_7200.pth
2023-12-31 04:38:02,427	44k	INFO	====> Epoch: 275, cost 35.70 s
2023-12-31 04:38:28,632	44k	INFO	====> Epoch: 276, cost 26.21 s
2023-12-31 04:38:54,826	44k	INFO	====> Epoch: 277, cost 26.19 s
2023-12-31 04:39:20,986	44k	INFO	====> Epoch: 278, cost 26.16 s
2023-12-31 04:39:47,128	44k	INFO	====> Epoch: 279, cost 26.14 s
2023-12-31 04:40:13,179	44k	INFO	====> Epoch: 280, cost 26.05 s
2023-12-31 04:40:13,940	44k	INFO	Train Epoch: 281 [0%]
2023-12-31 04:40:13,940	44k	INFO	Losses: [2.3926620483398438, 2.4620487689971924, 5.211221218109131, 22.916271209716797, 0.8668245673179626], step: 9800, lr: 9.633125741201631e-05, reference_loss: 33.84902572631836
2023-12-31 04:40:39,775	44k	INFO	====> Epoch: 281, cost 26.60 s
2023-12-31 04:41:05,939	44k	INFO	====> Epoch: 282, cost 26.16 s
2023-12-31 04:41:32,053	44k	INFO	====> Epoch: 283, cost 26.11 s
2023-12-31 04:41:58,608	44k	INFO	====> Epoch: 284, cost 26.56 s
2023-12-31 04:42:25,200	44k	INFO	====> Epoch: 285, cost 26.59 s
2023-12-31 04:42:44,833	44k	INFO	Train Epoch: 286 [71%]
2023-12-31 04:42:44,833	44k	INFO	Losses: [2.4824061393737793, 2.2852632999420166, 4.540476322174072, 21.150997161865234, 0.8204848766326904], step: 10000, lr: 9.627106542601141e-05, reference_loss: 31.279626846313477
2023-12-31 04:42:52,129	44k	INFO	====> Epoch: 286, cost 26.93 s
2023-12-31 04:43:18,651	44k	INFO	====> Epoch: 287, cost 26.52 s
2023-12-31 04:43:45,303	44k	INFO	====> Epoch: 288, cost 26.65 s
2023-12-31 04:44:11,809	44k	INFO	====> Epoch: 289, cost 26.51 s
2023-12-31 04:44:38,391	44k	INFO	====> Epoch: 290, cost 26.58 s
2023-12-31 04:45:05,329	44k	INFO	====> Epoch: 291, cost 26.94 s
2023-12-31 04:45:17,400	44k	INFO	Train Epoch: 292 [43%]
2023-12-31 04:45:17,400	44k	INFO	Losses: [2.365832805633545, 2.3167178630828857, 5.818620204925537, 21.882980346679688, 0.8788045048713684], step: 10200, lr: 9.619888468671259e-05, reference_loss: 33.26295471191406
2023-12-31 04:45:32,302	44k	INFO	====> Epoch: 292, cost 26.97 s
2023-12-31 04:45:58,906	44k	INFO	====> Epoch: 293, cost 26.60 s
2023-12-31 04:46:25,524	44k	INFO	====> Epoch: 294, cost 26.62 s
2023-12-31 04:46:52,066	44k	INFO	====> Epoch: 295, cost 26.54 s
2023-12-31 04:47:18,689	44k	INFO	====> Epoch: 296, cost 26.62 s
2023-12-31 04:47:45,298	44k	INFO	====> Epoch: 297, cost 26.61 s
2023-12-31 04:47:49,910	44k	INFO	Train Epoch: 298 [14%]
2023-12-31 04:47:49,910	44k	INFO	Losses: [2.4719839096069336, 2.164680242538452, 4.866802215576172, 21.801136016845703, 0.8514136075973511], step: 10400, lr: 9.612675806605373e-05, reference_loss: 32.1560173034668
2023-12-31 04:47:57,375	44k	INFO	Saving model and optimizer state at iteration 298 to ./logs/44k/G_10400.pth
2023-12-31 04:47:58,539	44k	INFO	Saving model and optimizer state at iteration 298 to ./logs/44k/D_10400.pth
2023-12-31 04:47:59,315	44k	INFO	.. Free up space by deleting ckpt ./logs/44k/G_8000.pth
2023-12-31 04:47:59,371	44k	INFO	.. Free up space by deleting ckpt ./logs/44k/D_8000.pth
2023-12-31 04:48:21,221	44k	INFO	====> Epoch: 298, cost 35.92 s
2023-12-31 04:48:47,784	44k	INFO	====> Epoch: 299, cost 26.56 s
2023-12-31 04:49:14,530	44k	INFO	====> Epoch: 300, cost 26.75 s
2023-12-31 04:49:41,073	44k	INFO	====> Epoch: 301, cost 26.54 s
2023-12-31 04:50:07,714	44k	INFO	====> Epoch: 302, cost 26.64 s
2023-12-31 04:50:31,128	44k	INFO	Train Epoch: 303 [86%]
2023-12-31 04:50:31,128	44k	INFO	Losses: [2.5334677696228027, 2.2121119499206543, 5.693731784820557, 22.4276180267334, 0.6592006683349609], step: 10600, lr: 9.606669386019102e-05, reference_loss: 33.52613067626953
2023-12-31 04:50:34,928	44k	INFO	====> Epoch: 303, cost 27.21 s
2023-12-31 04:51:01,516	44k	INFO	====> Epoch: 304, cost 26.59 s
2023-12-31 04:51:28,085	44k	INFO	====> Epoch: 305, cost 26.57 s
2023-12-31 04:51:54,590	44k	INFO	====> Epoch: 306, cost 26.50 s
2023-12-31 04:52:21,001	44k	INFO	====> Epoch: 307, cost 26.41 s
2023-12-31 04:52:47,415	44k	INFO	====> Epoch: 308, cost 26.41 s
2023-12-31 04:53:03,341	44k	INFO	Train Epoch: 309 [57%]
2023-12-31 04:53:03,341	44k	INFO	Losses: [2.418534278869629, 2.0430359840393066, 4.849211692810059, 21.609067916870117, 0.8991509079933167], step: 10800, lr: 9.599466635167497e-05, reference_loss: 31.819000244140625
2023-12-31 04:53:14,473	44k	INFO	====> Epoch: 309, cost 27.06 s
2023-12-31 04:53:41,186	44k	INFO	====> Epoch: 310, cost 26.71 s
2023-12-31 04:54:07,726	44k	INFO	====> Epoch: 311, cost 26.54 s
2023-12-31 04:54:34,185	44k	INFO	====> Epoch: 312, cost 26.46 s
2023-12-31 04:55:00,626	44k	INFO	====> Epoch: 313, cost 26.44 s
2023-12-31 04:55:27,209	44k	INFO	====> Epoch: 314, cost 26.58 s
2023-12-31 04:55:35,552	44k	INFO	Train Epoch: 315 [29%]
2023-12-31 04:55:35,552	44k	INFO	Losses: [2.6467745304107666, 2.533221483230591, 4.200191497802734, 21.192750930786133, 0.9622876048088074], step: 11000, lr: 9.592269284691169e-05, reference_loss: 31.535226821899414
2023-12-31 04:55:54,045	44k	INFO	====> Epoch: 315, cost 26.84 s
2023-12-31 04:56:20,553	44k	INFO	====> Epoch: 316, cost 26.51 s
2023-12-31 04:56:47,021	44k	INFO	====> Epoch: 317, cost 26.47 s
2023-12-31 04:57:13,824	44k	INFO	====> Epoch: 318, cost 26.80 s
2023-12-31 04:57:40,278	44k	INFO	====> Epoch: 319, cost 26.45 s
2023-12-31 04:58:06,901	44k	INFO	====> Epoch: 320, cost 26.62 s
2023-12-31 04:58:07,662	44k	INFO	Train Epoch: 321 [0%]
2023-12-31 04:58:07,663	44k	INFO	Losses: [2.4204061031341553, 2.2096474170684814, 4.6882219314575195, 22.562911987304688, 0.6386126279830933], step: 11200, lr: 9.5850773305411e-05, reference_loss: 32.519798278808594
2023-12-31 04:58:14,983	44k	INFO	Saving model and optimizer state at iteration 321 to ./logs/44k/G_11200.pth
2023-12-31 04:58:16,146	44k	INFO	Saving model and optimizer state at iteration 321 to ./logs/44k/D_11200.pth
2023-12-31 04:58:16,920	44k	INFO	.. Free up space by deleting ckpt ./logs/44k/G_8800.pth
2023-12-31 04:58:16,977	44k	INFO	.. Free up space by deleting ckpt ./logs/44k/D_8800.pth
2023-12-31 04:58:42,689	44k	INFO	====> Epoch: 321, cost 35.79 s
2023-12-31 04:59:09,283	44k	INFO	====> Epoch: 322, cost 26.59 s
2023-12-31 04:59:36,056	44k	INFO	====> Epoch: 323, cost 26.77 s
2023-12-31 05:00:02,321	44k	INFO	====> Epoch: 324, cost 26.27 s
2023-12-31 05:00:28,860	44k	INFO	====> Epoch: 325, cost 26.54 s
2023-12-31 05:00:48,472	44k	INFO	Train Epoch: 326 [71%]
2023-12-31 05:00:48,473	44k	INFO	Losses: [2.4926888942718506, 2.51315975189209, 5.572938442230225, 22.267864227294922, 0.8060307502746582], step: 11400, lr: 9.579088154690645e-05, reference_loss: 33.652679443359375
2023-12-31 05:00:55,581	44k	INFO	====> Epoch: 326, cost 26.72 s
2023-12-31 05:01:21,724	44k	INFO	====> Epoch: 327, cost 26.14 s
2023-12-31 05:01:47,759	44k	INFO	====> Epoch: 328, cost 26.04 s
2023-12-31 05:02:13,893	44k	INFO	====> Epoch: 329, cost 26.13 s
2023-12-31 05:02:39,956	44k	INFO	====> Epoch: 330, cost 26.06 s
2023-12-31 05:03:06,304	44k	INFO	====> Epoch: 331, cost 26.35 s
2023-12-31 05:03:18,304	44k	INFO	Train Epoch: 332 [43%]
2023-12-31 05:03:18,305	44k	INFO	Losses: [2.2992944717407227, 2.4580676555633545, 4.830779075622559, 19.863143920898438, 0.7459530830383301], step: 11600, lr: 9.571906083299264e-05, reference_loss: 30.19723892211914
2023-12-31 05:03:32,929	44k	INFO	====> Epoch: 332, cost 26.63 s
2023-12-31 05:03:59,137	44k	INFO	====> Epoch: 333, cost 26.21 s
2023-12-31 05:04:25,270	44k	INFO	====> Epoch: 334, cost 26.13 s
2023-12-31 05:04:51,495	44k	INFO	====> Epoch: 335, cost 26.22 s
2023-12-31 05:05:17,627	44k	INFO	====> Epoch: 336, cost 26.13 s
2023-12-31 05:05:43,852	44k	INFO	====> Epoch: 337, cost 26.23 s
2023-12-31 05:05:48,400	44k	INFO	Train Epoch: 338 [14%]
2023-12-31 05:05:48,400	44k	INFO	Losses: [2.5103118419647217, 2.3490638732910156, 5.107309341430664, 23.36936378479004, 0.9957746863365173], step: 11800, lr: 9.564729396778408e-05, reference_loss: 34.33182144165039
2023-12-31 05:06:11,220	44k	INFO	====> Epoch: 338, cost 27.37 s
2023-12-31 05:06:37,787	44k	INFO	====> Epoch: 339, cost 26.57 s
2023-12-31 05:07:04,268	44k	INFO	====> Epoch: 340, cost 26.48 s
2023-12-31 05:07:30,730	44k	INFO	====> Epoch: 341, cost 26.46 s
2023-12-31 05:07:57,381	44k	INFO	====> Epoch: 342, cost 26.65 s
2023-12-31 05:08:20,923	44k	INFO	Train Epoch: 343 [86%]
2023-12-31 05:08:20,924	44k	INFO	Losses: [2.5091757774353027, 2.1526026725769043, 5.179955005645752, 20.784526824951172, 0.6407065987586975], step: 12000, lr: 9.558752935207586e-05, reference_loss: 31.2669677734375
2023-12-31 05:08:28,158	44k	INFO	Saving model and optimizer state at iteration 343 to ./logs/44k/G_12000.pth
2023-12-31 05:08:29,651	44k	INFO	Saving model and optimizer state at iteration 343 to ./logs/44k/D_12000.pth
2023-12-31 05:08:30,429	44k	INFO	.. Free up space by deleting ckpt ./logs/44k/G_9600.pth
2023-12-31 05:08:30,485	44k	INFO	.. Free up space by deleting ckpt ./logs/44k/D_9600.pth
2023-12-31 05:08:33,490	44k	INFO	====> Epoch: 343, cost 36.11 s
2023-12-31 05:09:00,048	44k	INFO	====> Epoch: 344, cost 26.56 s
2023-12-31 05:09:26,436	44k	INFO	====> Epoch: 345, cost 26.39 s
2023-12-31 05:09:52,991	44k	INFO	====> Epoch: 346, cost 26.55 s
2023-12-31 05:10:19,491	44k	INFO	====> Epoch: 347, cost 26.50 s
2023-12-31 05:10:46,014	44k	INFO	====> Epoch: 348, cost 26.52 s
2023-12-31 05:11:01,873	44k	INFO	Train Epoch: 349 [57%]
2023-12-31 05:11:01,873	44k	INFO	Losses: [2.559983968734741, 2.106821060180664, 3.7559597492218018, 19.32683753967285, 0.8770631551742554], step: 12200, lr: 9.551586110465545e-05, reference_loss: 28.626667022705078
2023-12-31 05:11:12,943	44k	INFO	====> Epoch: 349, cost 26.93 s
2023-12-31 05:11:39,668	44k	INFO	====> Epoch: 350, cost 26.73 s
2023-12-31 05:12:06,109	44k	INFO	====> Epoch: 351, cost 26.44 s
2023-12-31 05:12:32,662	44k	INFO	====> Epoch: 352, cost 26.55 s
2023-12-31 05:12:59,262	44k	INFO	====> Epoch: 353, cost 26.60 s
2023-12-31 05:13:25,812	44k	INFO	====> Epoch: 354, cost 26.55 s
2023-12-31 05:13:34,306	44k	INFO	Train Epoch: 355 [29%]
2023-12-31 05:13:34,307	44k	INFO	Losses: [2.3732099533081055, 2.31095814704895, 4.837459087371826, 21.74271583557129, 0.5704339742660522], step: 12400, lr: 9.544424659162614e-05, reference_loss: 31.83477783203125
2023-12-31 05:13:52,892	44k	INFO	====> Epoch: 355, cost 27.08 s
2023-12-31 05:14:19,203	44k	INFO	====> Epoch: 356, cost 26.31 s
2023-12-31 05:14:45,949	44k	INFO	====> Epoch: 357, cost 26.75 s
2023-12-31 05:15:12,477	44k	INFO	====> Epoch: 358, cost 26.53 s
2023-12-31 05:15:39,037	44k	INFO	====> Epoch: 359, cost 26.56 s
2023-12-31 05:16:05,541	44k	INFO	====> Epoch: 360, cost 26.50 s
2023-12-31 05:16:06,312	44k	INFO	Train Epoch: 361 [0%]
2023-12-31 05:16:06,312	44k	INFO	Losses: [2.2586264610290527, 2.3371856212615967, 5.355372428894043, 21.639225006103516, 0.5235809683799744], step: 12600, lr: 9.537268577269974e-05, reference_loss: 32.113990783691406
2023-12-31 05:16:32,575	44k	INFO	====> Epoch: 361, cost 27.03 s
2023-12-31 05:16:59,153	44k	INFO	====> Epoch: 362, cost 26.58 s
2023-12-31 05:17:25,709	44k	INFO	====> Epoch: 363, cost 26.56 s
2023-12-31 05:17:52,225	44k	INFO	====> Epoch: 364, cost 26.52 s
2023-12-31 05:18:18,983	44k	INFO	====> Epoch: 365, cost 26.76 s
2023-12-31 05:18:38,672	44k	INFO	Train Epoch: 366 [71%]
2023-12-31 05:18:38,672	44k	INFO	Losses: [2.676332473754883, 2.2406768798828125, 3.4213507175445557, 18.75551986694336, 0.9727822542190552], step: 12800, lr: 9.53130927442113e-05, reference_loss: 28.066661834716797
2023-12-31 05:18:45,788	44k	INFO	Saving model and optimizer state at iteration 366 to ./logs/44k/G_12800.pth
2023-12-31 05:18:46,985	44k	INFO	Saving model and optimizer state at iteration 366 to ./logs/44k/D_12800.pth
2023-12-31 05:18:47,756	44k	INFO	.. Free up space by deleting ckpt ./logs/44k/G_10400.pth
2023-12-31 05:18:47,812	44k	INFO	.. Free up space by deleting ckpt ./logs/44k/D_10400.pth
2023-12-31 05:18:54,615	44k	INFO	====> Epoch: 366, cost 35.63 s
2023-12-31 05:19:21,282	44k	INFO	====> Epoch: 367, cost 26.67 s
2023-12-31 05:19:47,817	44k	INFO	====> Epoch: 368, cost 26.54 s
2023-12-31 05:20:14,431	44k	INFO	====> Epoch: 369, cost 26.61 s
2023-12-31 05:20:41,319	44k	INFO	====> Epoch: 370, cost 26.89 s
2023-12-31 05:21:07,702	44k	INFO	====> Epoch: 371, cost 26.38 s
2023-12-31 05:21:19,805	44k	INFO	Train Epoch: 372 [43%]
2023-12-31 05:21:19,805	44k	INFO	Losses: [2.5111632347106934, 2.281663656234741, 4.27421236038208, 20.318357467651367, 1.089052677154541], step: 13000, lr: 9.524163025993642e-05, reference_loss: 30.474449157714844
2023-12-31 05:21:34,682	44k	INFO	====> Epoch: 372, cost 26.98 s
2023-12-31 05:22:01,348	44k	INFO	====> Epoch: 373, cost 26.67 s
2023-12-31 05:22:27,923	44k	INFO	====> Epoch: 374, cost 26.57 s
2023-12-31 05:22:54,435	44k	INFO	====> Epoch: 375, cost 26.51 s
2023-12-31 05:23:21,000	44k	INFO	====> Epoch: 376, cost 26.56 s
2023-12-31 05:23:47,513	44k	INFO	====> Epoch: 377, cost 26.51 s
2023-12-31 05:23:52,085	44k	INFO	Train Epoch: 378 [14%]
2023-12-31 05:23:52,086	44k	INFO	Losses: [2.594587564468384, 2.2583682537078857, 5.425506591796875, 23.745622634887695, 0.8093136548995972], step: 13200, lr: 9.517022135577851e-05, reference_loss: 34.83340072631836
2023-12-31 05:24:14,742	44k	INFO	====> Epoch: 378, cost 27.23 s
2023-12-31 05:24:41,175	44k	INFO	====> Epoch: 379, cost 26.43 s
2023-12-31 05:25:07,531	44k	INFO	====> Epoch: 380, cost 26.36 s
2023-12-31 05:25:33,903	44k	INFO	====> Epoch: 381, cost 26.37 s
2023-12-31 05:26:00,335	44k	INFO	====> Epoch: 382, cost 26.43 s
2023-12-31 05:26:23,945	44k	INFO	Train Epoch: 383 [86%]
2023-12-31 05:26:23,946	44k	INFO	Losses: [2.410398483276367, 2.4857735633850098, 6.641811370849609, 21.502538681030273, 0.8849626183509827], step: 13400, lr: 9.511075483591955e-05, reference_loss: 33.92548370361328
2023-12-31 05:26:27,410	44k	INFO	====> Epoch: 383, cost 27.07 s
2023-12-31 05:26:54,121	44k	INFO	====> Epoch: 384, cost 26.71 s
2023-12-31 05:27:20,437	44k	INFO	====> Epoch: 385, cost 26.32 s
2023-12-31 05:27:47,124	44k	INFO	====> Epoch: 386, cost 26.69 s
2023-12-31 05:28:13,799	44k	INFO	====> Epoch: 387, cost 26.68 s
2023-12-31 05:28:40,284	44k	INFO	====> Epoch: 388, cost 26.48 s
2023-12-31 05:28:56,306	44k	INFO	Train Epoch: 389 [57%]
2023-12-31 05:28:56,306	44k	INFO	Losses: [2.28244948387146, 2.4377284049987793, 6.2415618896484375, 21.63556480407715, 0.7027307152748108], step: 13600, lr: 9.503944405766085e-05, reference_loss: 33.3000373840332
2023-12-31 05:29:03,720	44k	INFO	Saving model and optimizer state at iteration 389 to ./logs/44k/G_13600.pth
2023-12-31 05:29:04,877	44k	INFO	Saving model and optimizer state at iteration 389 to ./logs/44k/D_13600.pth
2023-12-31 05:29:05,651	44k	INFO	.. Free up space by deleting ckpt ./logs/44k/G_11200.pth
2023-12-31 05:29:05,708	44k	INFO	.. Free up space by deleting ckpt ./logs/44k/D_11200.pth
2023-12-31 05:29:16,346	44k	INFO	====> Epoch: 389, cost 36.06 s
2023-12-31 05:29:43,341	44k	INFO	====> Epoch: 390, cost 26.99 s
2023-12-31 05:30:09,817	44k	INFO	====> Epoch: 391, cost 26.48 s
2023-12-31 05:30:36,399	44k	INFO	====> Epoch: 392, cost 26.58 s
2023-12-31 05:31:02,960	44k	INFO	====> Epoch: 393, cost 26.56 s
2023-12-31 05:31:29,433	44k	INFO	====> Epoch: 394, cost 26.47 s
2023-12-31 05:31:37,893	44k	INFO	Train Epoch: 395 [29%]
2023-12-31 05:31:37,894	44k	INFO	Losses: [2.588660955429077, 2.1937782764434814, 4.343956470489502, 20.769311904907227, 0.7788441777229309], step: 13800, lr: 9.496818674577514e-05, reference_loss: 30.67455291748047
2023-12-31 05:31:56,448	44k	INFO	====> Epoch: 395, cost 27.01 s
2023-12-31 05:32:22,935	44k	INFO	====> Epoch: 396, cost 26.49 s
2023-12-31 05:32:49,496	44k	INFO	====> Epoch: 397, cost 26.56 s
2023-12-31 05:33:16,171	44k	INFO	====> Epoch: 398, cost 26.68 s
2023-12-31 05:33:42,988	44k	INFO	====> Epoch: 399, cost 26.82 s
2023-12-31 05:34:09,529	44k	INFO	====> Epoch: 400, cost 26.54 s
2023-12-31 05:34:10,304	44k	INFO	Train Epoch: 401 [0%]
2023-12-31 05:34:10,305	44k	INFO	Losses: [2.4509754180908203, 2.202796697616577, 4.930495738983154, 20.113309860229492, 0.8166409730911255], step: 14000, lr: 9.489698286017521e-05, reference_loss: 30.514219284057617
2023-12-31 05:34:36,493	44k	INFO	====> Epoch: 401, cost 26.96 s
2023-12-31 05:35:02,940	44k	INFO	====> Epoch: 402, cost 26.45 s
2023-12-31 05:35:29,550	44k	INFO	====> Epoch: 403, cost 26.61 s
2023-12-31 05:35:55,927	44k	INFO	====> Epoch: 404, cost 26.38 s
2023-12-31 05:36:22,430	44k	INFO	====> Epoch: 405, cost 26.50 s
2023-12-31 05:36:42,321	44k	INFO	Train Epoch: 406 [71%]
2023-12-31 05:36:42,322	44k	INFO	Losses: [2.3352532386779785, 2.3961384296417236, 6.328274726867676, 23.421749114990234, 0.920018196105957], step: 14200, lr: 9.483768707168782e-05, reference_loss: 35.401432037353516
2023-12-31 05:36:49,544	44k	INFO	====> Epoch: 406, cost 27.11 s
2023-12-31 05:37:16,083	44k	INFO	====> Epoch: 407, cost 26.54 s
2023-12-31 05:37:42,641	44k	INFO	====> Epoch: 408, cost 26.56 s
2023-12-31 05:38:09,309	44k	INFO	====> Epoch: 409, cost 26.67 s
2023-12-31 05:38:35,924	44k	INFO	====> Epoch: 410, cost 26.61 s
2023-12-31 05:39:02,312	44k	INFO	====> Epoch: 411, cost 26.39 s
2023-12-31 05:39:14,616	44k	INFO	Train Epoch: 412 [43%]
2023-12-31 05:39:14,617	44k	INFO	Losses: [2.658785581588745, 1.9075849056243896, 4.721567630767822, 19.921052932739258, 0.5620859861373901], step: 14400, lr: 9.47665810302627e-05, reference_loss: 29.77107810974121
2023-12-31 05:39:22,102	44k	INFO	Saving model and optimizer state at iteration 412 to ./logs/44k/G_14400.pth
2023-12-31 05:39:23,259	44k	INFO	Saving model and optimizer state at iteration 412 to ./logs/44k/D_14400.pth
2023-12-31 05:39:24,042	44k	INFO	.. Free up space by deleting ckpt ./logs/44k/G_12000.pth
2023-12-31 05:39:24,098	44k	INFO	.. Free up space by deleting ckpt ./logs/44k/D_12000.pth
2023-12-31 05:39:38,516	44k	INFO	====> Epoch: 412, cost 36.20 s
2023-12-31 05:40:05,255	44k	INFO	====> Epoch: 413, cost 26.74 s
2023-12-31 05:40:31,699	44k	INFO	====> Epoch: 414, cost 26.44 s
2023-12-31 05:40:58,311	44k	INFO	====> Epoch: 415, cost 26.61 s
2023-12-31 05:41:24,790	44k	INFO	====> Epoch: 416, cost 26.48 s
2023-12-31 05:41:51,236	44k	INFO	====> Epoch: 417, cost 26.45 s
2023-12-31 05:41:55,843	44k	INFO	Train Epoch: 418 [14%]
2023-12-31 05:41:55,843	44k	INFO	Losses: [2.5560755729675293, 2.5526440143585205, 4.247404098510742, 20.61175537109375, 0.8392716646194458], step: 14600, lr: 9.469552830170594e-05, reference_loss: 30.807151794433594
2023-12-31 05:42:18,576	44k	INFO	====> Epoch: 418, cost 27.34 s
2023-12-31 05:42:45,286	44k	INFO	====> Epoch: 419, cost 26.71 s
2023-12-31 05:43:11,797	44k	INFO	====> Epoch: 420, cost 26.51 s
2023-12-31 05:43:38,229	44k	INFO	====> Epoch: 421, cost 26.43 s
2023-12-31 05:44:04,939	44k	INFO	====> Epoch: 422, cost 26.71 s
2023-12-31 05:44:28,337	44k	INFO	Train Epoch: 423 [86%]
2023-12-31 05:44:28,338	44k	INFO	Losses: [2.346824884414673, 2.434861660003662, 7.1900634765625, 24.080854415893555, 1.0656545162200928], step: 14800, lr: 9.463635839084426e-05, reference_loss: 37.11825942993164
2023-12-31 05:44:31,960	44k	INFO	====> Epoch: 423, cost 27.02 s
2023-12-31 05:44:58,301	44k	INFO	====> Epoch: 424, cost 26.34 s
2023-12-31 05:45:25,067	44k	INFO	====> Epoch: 425, cost 26.77 s
2023-12-31 05:45:51,575	44k	INFO	====> Epoch: 426, cost 26.51 s
2023-12-31 05:46:18,104	44k	INFO	====> Epoch: 427, cost 26.53 s
2023-12-31 05:46:44,552	44k	INFO	====> Epoch: 428, cost 26.45 s
2023-12-31 05:47:00,447	44k	INFO	Train Epoch: 429 [57%]
2023-12-31 05:47:00,448	44k	INFO	Losses: [2.7000629901885986, 2.242725133895874, 4.543426990509033, 19.929105758666992, 0.7064333558082581], step: 15000, lr: 9.456540329875122e-05, reference_loss: 30.121755599975586
2023-12-31 05:47:11,350	44k	INFO	====> Epoch: 429, cost 26.80 s
2023-12-31 05:47:37,817	44k	INFO	====> Epoch: 430, cost 26.47 s
2023-12-31 05:48:04,330	44k	INFO	====> Epoch: 431, cost 26.51 s
2023-12-31 05:48:31,143	44k	INFO	====> Epoch: 432, cost 26.81 s
2023-12-31 05:48:57,472	44k	INFO	====> Epoch: 433, cost 26.33 s
2023-12-31 05:49:23,884	44k	INFO	====> Epoch: 434, cost 26.41 s
2023-12-31 05:49:32,342	44k	INFO	Train Epoch: 435 [29%]
2023-12-31 05:49:32,343	44k	INFO	Losses: [2.3599371910095215, 2.2700788974761963, 6.268453598022461, 22.90951919555664, 1.0067051649093628], step: 15200, lr: 9.44945014063499e-05, reference_loss: 34.814697265625
2023-12-31 05:49:39,545	44k	INFO	Saving model and optimizer state at iteration 435 to ./logs/44k/G_15200.pth
2023-12-31 05:49:40,737	44k	INFO	Saving model and optimizer state at iteration 435 to ./logs/44k/D_15200.pth
2023-12-31 05:49:41,512	44k	INFO	.. Free up space by deleting ckpt ./logs/44k/G_12800.pth
2023-12-31 05:49:41,572	44k	INFO	.. Free up space by deleting ckpt ./logs/44k/D_12800.pth
2023-12-31 05:49:59,722	44k	INFO	====> Epoch: 435, cost 35.84 s
2023-12-31 05:50:26,282	44k	INFO	====> Epoch: 436, cost 26.56 s
2023-12-31 05:50:53,156	44k	INFO	====> Epoch: 437, cost 26.87 s
2023-12-31 05:51:19,733	44k	INFO	====> Epoch: 438, cost 26.58 s
2023-12-31 05:51:46,243	44k	INFO	====> Epoch: 439, cost 26.51 s
2023-12-31 05:52:12,837	44k	INFO	====> Epoch: 440, cost 26.59 s
2023-12-31 05:52:13,621	44k	INFO	Train Epoch: 441 [0%]
2023-12-31 05:52:13,622	44k	INFO	Losses: [2.325115442276001, 2.3314125537872314, 5.922833442687988, 21.592830657958984, 0.9049016237258911], step: 15400, lr: 9.442365267375304e-05, reference_loss: 33.07709503173828
2023-12-31 05:52:39,927	44k	INFO	====> Epoch: 441, cost 27.09 s
2023-12-31 05:53:06,428	44k	INFO	====> Epoch: 442, cost 26.50 s
2023-12-31 05:53:32,851	44k	INFO	====> Epoch: 443, cost 26.42 s
2023-12-31 05:53:59,418	44k	INFO	====> Epoch: 444, cost 26.57 s
2023-12-31 05:54:26,115	44k	INFO	====> Epoch: 445, cost 26.70 s
2023-12-31 05:54:46,081	44k	INFO	Train Epoch: 446 [71%]
2023-12-31 05:54:46,082	44k	INFO	Losses: [2.5745015144348145, 2.190084934234619, 4.51212739944458, 19.681598663330078, 0.7631389498710632], step: 15600, lr: 9.436465264268356e-05, reference_loss: 29.721452713012695
2023-12-31 05:54:53,417	44k	INFO	====> Epoch: 446, cost 27.30 s
2023-12-31 05:55:19,908	44k	INFO	====> Epoch: 447, cost 26.49 s
2023-12-31 05:55:46,460	44k	INFO	====> Epoch: 448, cost 26.55 s
2023-12-31 05:56:13,057	44k	INFO	====> Epoch: 449, cost 26.60 s
2023-12-31 05:56:39,678	44k	INFO	====> Epoch: 450, cost 26.62 s
2023-12-31 05:57:06,338	44k	INFO	====> Epoch: 451, cost 26.66 s
2023-12-31 05:57:18,464	44k	INFO	Train Epoch: 452 [43%]
2023-12-31 05:57:18,464	44k	INFO	Losses: [2.422966480255127, 2.3917322158813477, 6.228303909301758, 19.417879104614258, 0.8044874668121338], step: 15800, lr: 9.42939012662312e-05, reference_loss: 31.265369415283203
2023-12-31 05:57:33,550	44k	INFO	====> Epoch: 452, cost 27.21 s
2023-12-31 05:58:00,130	44k	INFO	====> Epoch: 453, cost 26.58 s
2023-12-31 05:58:26,716	44k	INFO	====> Epoch: 454, cost 26.59 s
2023-12-31 05:58:53,365	44k	INFO	====> Epoch: 455, cost 26.65 s
2023-12-31 05:59:19,710	44k	INFO	====> Epoch: 456, cost 26.34 s
2023-12-31 05:59:46,115	44k	INFO	====> Epoch: 457, cost 26.41 s
2023-12-31 05:59:50,714	44k	INFO	Train Epoch: 458 [14%]
2023-12-31 05:59:50,714	44k	INFO	Losses: [2.6455540657043457, 2.124932050704956, 5.316834926605225, 21.34211540222168, 0.7200685143470764], step: 16000, lr: 9.422320293673162e-05, reference_loss: 32.149505615234375
2023-12-31 05:59:57,886	44k	INFO	Saving model and optimizer state at iteration 458 to ./logs/44k/G_16000.pth
2023-12-31 05:59:59,368	44k	INFO	Saving model and optimizer state at iteration 458 to ./logs/44k/D_16000.pth
2023-12-31 06:00:00,148	44k	INFO	.. Free up space by deleting ckpt ./logs/44k/G_13600.pth
2023-12-31 06:00:00,206	44k	INFO	.. Free up space by deleting ckpt ./logs/44k/D_13600.pth
2023-12-31 06:00:22,161	44k	INFO	====> Epoch: 458, cost 36.05 s
2023-12-31 06:00:48,749	44k	INFO	====> Epoch: 459, cost 26.59 s
2023-12-31 06:01:15,271	44k	INFO	====> Epoch: 460, cost 26.52 s
2023-12-31 06:01:41,780	44k	INFO	====> Epoch: 461, cost 26.51 s
2023-12-31 06:02:08,227	44k	INFO	====> Epoch: 462, cost 26.45 s
2023-12-31 06:02:31,818	44k	INFO	Train Epoch: 463 [86%]
2023-12-31 06:02:31,819	44k	INFO	Losses: [2.6508281230926514, 2.5647037029266357, 6.533043384552002, 21.265413284301758, 0.6974456906318665], step: 16200, lr: 9.416432815543143e-05, reference_loss: 33.71143341064453
2023-12-31 06:02:35,290	44k	INFO	====> Epoch: 463, cost 27.06 s
2023-12-31 06:03:01,838	44k	INFO	====> Epoch: 464, cost 26.55 s
2023-12-31 06:03:28,469	44k	INFO	====> Epoch: 465, cost 26.63 s
2023-12-31 06:03:55,003	44k	INFO	====> Epoch: 466, cost 26.53 s
2023-12-31 06:04:21,629	44k	INFO	====> Epoch: 467, cost 26.63 s
2023-12-31 06:04:48,209	44k	INFO	====> Epoch: 468, cost 26.58 s
2023-12-31 06:05:04,161	44k	INFO	Train Epoch: 469 [57%]
2023-12-31 06:05:04,162	44k	INFO	Losses: [2.5438339710235596, 2.132111072540283, 5.237650394439697, 19.993906021118164, 0.838584840297699], step: 16400, lr: 9.409372697540131e-05, reference_loss: 30.74608612060547
2023-12-31 06:05:15,129	44k	INFO	====> Epoch: 469, cost 26.92 s
2023-12-31 06:05:41,594	44k	INFO	====> Epoch: 470, cost 26.47 s
2023-12-31 06:06:08,104	44k	INFO	====> Epoch: 471, cost 26.51 s
2023-12-31 06:06:34,785	44k	INFO	====> Epoch: 472, cost 26.68 s
2023-12-31 06:07:01,255	44k	INFO	====> Epoch: 473, cost 26.47 s
2023-12-31 06:07:27,721	44k	INFO	====> Epoch: 474, cost 26.47 s
2023-12-31 06:07:36,135	44k	INFO	Train Epoch: 475 [29%]
2023-12-31 06:07:36,136	44k	INFO	Losses: [2.9243438243865967, 2.253007173538208, 4.5220627784729, 19.665189743041992, 1.008920431137085], step: 16600, lr: 9.402317872971181e-05, reference_loss: 30.373523712158203
2023-12-31 06:07:54,627	44k	INFO	====> Epoch: 475, cost 26.91 s
2023-12-31 06:08:21,165	44k	INFO	====> Epoch: 476, cost 26.54 s
2023-12-31 06:08:47,544	44k	INFO	====> Epoch: 477, cost 26.38 s
2023-12-31 06:09:14,119	44k	INFO	====> Epoch: 478, cost 26.57 s
2023-12-31 06:09:41,015	44k	INFO	====> Epoch: 479, cost 26.90 s
2023-12-31 06:10:07,593	44k	INFO	====> Epoch: 480, cost 26.58 s
2023-12-31 06:10:08,352	44k	INFO	Train Epoch: 481 [0%]
2023-12-31 06:10:08,352	44k	INFO	Losses: [2.4054577350616455, 2.30022931098938, 5.1482086181640625, 20.332778930664062, 0.648359477519989], step: 16800, lr: 9.395268337867458e-05, reference_loss: 30.835033416748047
2023-12-31 06:10:15,615	44k	INFO	Saving model and optimizer state at iteration 481 to ./logs/44k/G_16800.pth
2023-12-31 06:10:16,785	44k	INFO	Saving model and optimizer state at iteration 481 to ./logs/44k/D_16800.pth
2023-12-31 06:10:17,556	44k	INFO	.. Free up space by deleting ckpt ./logs/44k/G_14400.pth
2023-12-31 06:10:17,613	44k	INFO	.. Free up space by deleting ckpt ./logs/44k/D_14400.pth
2023-12-31 06:10:43,394	44k	INFO	====> Epoch: 481, cost 35.80 s
2023-12-31 06:11:10,048	44k	INFO	====> Epoch: 482, cost 26.65 s
2023-12-31 06:11:36,510	44k	INFO	====> Epoch: 483, cost 26.46 s
2023-12-31 06:12:03,447	44k	INFO	====> Epoch: 484, cost 26.94 s
2023-12-31 06:12:30,028	44k	INFO	====> Epoch: 485, cost 26.58 s
2023-12-31 06:12:49,865	44k	INFO	Train Epoch: 486 [71%]
2023-12-31 06:12:49,865	44k	INFO	Losses: [2.480342388153076, 2.3938522338867188, 5.983141899108887, 20.552047729492188, 0.8358476758003235], step: 17000, lr: 9.389397762983476e-05, reference_loss: 32.24523162841797
2023-12-31 06:12:57,127	44k	INFO	====> Epoch: 486, cost 27.10 s
2023-12-31 06:13:23,717	44k	INFO	====> Epoch: 487, cost 26.59 s
2023-12-31 06:13:50,196	44k	INFO	====> Epoch: 488, cost 26.48 s
2023-12-31 06:14:16,743	44k	INFO	====> Epoch: 489, cost 26.55 s
2023-12-31 06:14:43,332	44k	INFO	====> Epoch: 490, cost 26.59 s
2023-12-31 06:15:09,722	44k	INFO	====> Epoch: 491, cost 26.39 s
2023-12-31 06:15:21,810	44k	INFO	Train Epoch: 492 [43%]
2023-12-31 06:15:21,811	44k	INFO	Losses: [2.346809148788452, 2.504737138748169, 4.960671424865723, 17.95585823059082, 0.821006715297699], step: 17200, lr: 9.382357914934599e-05, reference_loss: 28.589082717895508
2023-12-31 06:15:37,194	44k	INFO	====> Epoch: 492, cost 27.47 s
2023-12-31 06:16:03,724	44k	INFO	====> Epoch: 493, cost 26.53 s
2023-12-31 06:16:30,240	44k	INFO	====> Epoch: 494, cost 26.52 s
2023-12-31 06:16:56,824	44k	INFO	====> Epoch: 495, cost 26.58 s
2023-12-31 06:17:23,259	44k	INFO	====> Epoch: 496, cost 26.43 s
2023-12-31 06:17:49,768	44k	INFO	====> Epoch: 497, cost 26.51 s
2023-12-31 06:17:54,411	44k	INFO	Train Epoch: 498 [14%]
2023-12-31 06:17:54,412	44k	INFO	Losses: [2.664510726928711, 2.1892690658569336, 4.903390407562256, 21.366355895996094, 1.0201139450073242], step: 17400, lr: 9.37532334512207e-05, reference_loss: 32.143638610839844
2023-12-31 06:18:16,971	44k	INFO	====> Epoch: 498, cost 27.20 s
2023-12-31 06:18:43,444	44k	INFO	====> Epoch: 499, cost 26.47 s
2023-12-31 06:19:09,951	44k	INFO	====> Epoch: 500, cost 26.51 s
2023-12-31 06:19:36,304	44k	INFO	====> Epoch: 501, cost 26.35 s
2023-12-31 06:20:02,738	44k	INFO	====> Epoch: 502, cost 26.43 s
2023-12-31 06:20:26,093	44k	INFO	Train Epoch: 503 [86%]
2023-12-31 06:20:26,094	44k	INFO	Losses: [2.4922773838043213, 2.3575310707092285, 5.204905986785889, 19.156007766723633, 0.6525135040283203], step: 17600, lr: 9.36946523274254e-05, reference_loss: 29.863235473632812
2023-12-31 06:20:33,349	44k	INFO	Saving model and optimizer state at iteration 503 to ./logs/44k/G_17600.pth
2023-12-31 06:20:34,527	44k	INFO	Saving model and optimizer state at iteration 503 to ./logs/44k/D_17600.pth
2023-12-31 06:20:35,318	44k	INFO	.. Free up space by deleting ckpt ./logs/44k/G_15200.pth
2023-12-31 06:20:35,373	44k	INFO	.. Free up space by deleting ckpt ./logs/44k/D_15200.pth
2023-12-31 06:20:38,410	44k	INFO	====> Epoch: 503, cost 35.67 s
2023-12-31 06:21:05,305	44k	INFO	====> Epoch: 504, cost 26.89 s
2023-12-31 06:21:31,935	44k	INFO	====> Epoch: 505, cost 26.63 s
2023-12-31 06:21:58,542	44k	INFO	====> Epoch: 506, cost 26.61 s
2023-12-31 06:22:24,992	44k	INFO	====> Epoch: 507, cost 26.45 s
2023-12-31 06:22:51,653	44k	INFO	====> Epoch: 508, cost 26.66 s
2023-12-31 06:23:07,709	44k	INFO	Train Epoch: 509 [57%]
2023-12-31 06:23:07,709	44k	INFO	Losses: [2.5139925479888916, 1.9505671262741089, 4.774259090423584, 18.823226928710938, 0.8500522971153259], step: 17800, lr: 9.362440329420433e-05, reference_loss: 28.912097930908203
2023-12-31 06:23:18,863	44k	INFO	====> Epoch: 509, cost 27.21 s
2023-12-31 06:23:45,356	44k	INFO	====> Epoch: 510, cost 26.49 s
2023-12-31 06:24:11,967	44k	INFO	====> Epoch: 511, cost 26.61 s
2023-12-31 06:24:38,510	44k	INFO	====> Epoch: 512, cost 26.54 s
2023-12-31 06:25:05,118	44k	INFO	====> Epoch: 513, cost 26.61 s
2023-12-31 06:25:31,753	44k	INFO	====> Epoch: 514, cost 26.64 s
2023-12-31 06:25:40,153	44k	INFO	Train Epoch: 515 [29%]
2023-12-31 06:25:40,154	44k	INFO	Losses: [2.430793523788452, 2.206829786300659, 4.8228325843811035, 19.047882080078125, 0.5360535979270935], step: 18000, lr: 9.355420693129632e-05, reference_loss: 29.044391632080078
2023-12-31 06:25:58,502	44k	INFO	====> Epoch: 515, cost 26.75 s
2023-12-31 06:26:25,143	44k	INFO	====> Epoch: 516, cost 26.64 s
2023-12-31 06:26:51,678	44k	INFO	====> Epoch: 517, cost 26.53 s
2023-12-31 06:27:18,296	44k	INFO	====> Epoch: 518, cost 26.62 s
2023-12-31 06:27:44,979	44k	INFO	====> Epoch: 519, cost 26.68 s
2023-12-31 06:28:12,077	44k	INFO	====> Epoch: 520, cost 27.10 s
2023-12-31 06:28:12,854	44k	INFO	Train Epoch: 521 [0%]
2023-12-31 06:28:12,854	44k	INFO	Losses: [2.312512159347534, 2.355426788330078, 5.451400279998779, 18.26612663269043, 0.4447135925292969], step: 18200, lr: 9.348406319921095e-05, reference_loss: 28.83017921447754
2023-12-31 06:28:39,236	44k	INFO	====> Epoch: 521, cost 27.16 s
2023-12-31 06:29:05,891	44k	INFO	====> Epoch: 522, cost 26.66 s
2023-12-31 06:29:32,476	44k	INFO	====> Epoch: 523, cost 26.58 s
2023-12-31 06:29:59,142	44k	INFO	====> Epoch: 524, cost 26.67 s
2023-12-31 06:30:25,690	44k	INFO	====> Epoch: 525, cost 26.55 s
2023-12-31 06:30:45,366	44k	INFO	Train Epoch: 526 [71%]
2023-12-31 06:30:45,367	44k	INFO	Losses: [2.7683699131011963, 1.930389404296875, 3.3240718841552734, 16.00673484802246, 1.009424090385437], step: 18400, lr: 9.342565026477056e-05, reference_loss: 25.038990020751953
2023-12-31 06:30:52,977	44k	INFO	Saving model and optimizer state at iteration 526 to ./logs/44k/G_18400.pth
2023-12-31 06:30:54,153	44k	INFO	Saving model and optimizer state at iteration 526 to ./logs/44k/D_18400.pth
2023-12-31 06:30:54,931	44k	INFO	.. Free up space by deleting ckpt ./logs/44k/G_16000.pth
2023-12-31 06:30:54,987	44k	INFO	.. Free up space by deleting ckpt ./logs/44k/D_16000.pth
2023-12-31 06:31:01,784	44k	INFO	====> Epoch: 526, cost 36.09 s
2023-12-31 06:31:28,102	44k	INFO	====> Epoch: 527, cost 26.32 s
2023-12-31 06:31:54,481	44k	INFO	====> Epoch: 528, cost 26.38 s
2023-12-31 06:32:20,962	44k	INFO	====> Epoch: 529, cost 26.48 s
2023-12-31 06:32:47,473	44k	INFO	====> Epoch: 530, cost 26.51 s
2023-12-31 06:33:14,013	44k	INFO	====> Epoch: 531, cost 26.54 s
2023-12-31 06:33:26,047	44k	INFO	Train Epoch: 532 [43%]
2023-12-31 06:33:26,048	44k	INFO	Losses: [2.5228261947631836, 2.1310787200927734, 4.355351448059082, 18.165071487426758, 0.9877064824104309], step: 18600, lr: 9.335560292005964e-05, reference_loss: 28.16203498840332
2023-12-31 06:33:41,110	44k	INFO	====> Epoch: 532, cost 27.10 s
2023-12-31 06:34:07,480	44k	INFO	====> Epoch: 533, cost 26.37 s
2023-12-31 06:34:33,896	44k	INFO	====> Epoch: 534, cost 26.42 s
2023-12-31 06:35:00,335	44k	INFO	====> Epoch: 535, cost 26.44 s
2023-12-31 06:35:26,968	44k	INFO	====> Epoch: 536, cost 26.63 s
2023-12-31 06:35:53,566	44k	INFO	====> Epoch: 537, cost 26.60 s
2023-12-31 06:35:58,186	44k	INFO	Train Epoch: 538 [14%]
2023-12-31 06:35:58,186	44k	INFO	Losses: [2.4683661460876465, 2.272157669067383, 5.957644939422607, 21.719970703125, 0.7815455198287964], step: 18800, lr: 9.328560809444264e-05, reference_loss: 33.199684143066406
2023-12-31 06:36:20,719	44k	INFO	====> Epoch: 538, cost 27.15 s
2023-12-31 06:36:47,609	44k	INFO	====> Epoch: 539, cost 26.89 s
2023-12-31 06:37:14,148	44k	INFO	====> Epoch: 540, cost 26.54 s
2023-12-31 06:37:40,776	44k	INFO	====> Epoch: 541, cost 26.63 s
2023-12-31 06:38:07,202	44k	INFO	====> Epoch: 542, cost 26.43 s
2023-12-31 06:38:30,670	44k	INFO	Train Epoch: 543 [86%]
2023-12-31 06:38:30,670	44k	INFO	Losses: [2.35040283203125, 2.3737988471984863, 6.898583889007568, 19.332984924316406, 0.9008287787437439], step: 19000, lr: 9.322731916343797e-05, reference_loss: 31.856599807739258
2023-12-31 06:38:34,170	44k	INFO	====> Epoch: 543, cost 26.97 s
2023-12-31 06:39:00,563	44k	INFO	====> Epoch: 544, cost 26.39 s
2023-12-31 06:39:27,118	44k	INFO	====> Epoch: 545, cost 26.55 s
2023-12-31 06:39:53,998	44k	INFO	====> Epoch: 546, cost 26.88 s
2023-12-31 06:40:20,353	44k	INFO	====> Epoch: 547, cost 26.35 s
2023-12-31 06:40:46,842	44k	INFO	====> Epoch: 548, cost 26.49 s
2023-12-31 06:41:02,812	44k	INFO	Train Epoch: 549 [57%]
2023-12-31 06:41:02,813	44k	INFO	Losses: [2.30312442779541, 2.1939892768859863, 7.232613563537598, 22.014448165893555, 0.7398070693016052], step: 19200, lr: 9.315742052057694e-05, reference_loss: 34.48398208618164
2023-12-31 06:41:09,953	44k	INFO	Saving model and optimizer state at iteration 549 to ./logs/44k/G_19200.pth
2023-12-31 06:41:11,118	44k	INFO	Saving model and optimizer state at iteration 549 to ./logs/44k/D_19200.pth
2023-12-31 06:41:11,893	44k	INFO	.. Free up space by deleting ckpt ./logs/44k/G_16800.pth
2023-12-31 06:41:11,950	44k	INFO	.. Free up space by deleting ckpt ./logs/44k/D_16800.pth
2023-12-31 06:41:22,617	44k	INFO	====> Epoch: 549, cost 35.78 s
2023-12-31 06:41:49,122	44k	INFO	====> Epoch: 550, cost 26.50 s
2023-12-31 06:42:15,619	44k	INFO	====> Epoch: 551, cost 26.50 s
2023-12-31 06:42:42,424	44k	INFO	====> Epoch: 552, cost 26.81 s
2023-12-31 06:43:08,917	44k	INFO	====> Epoch: 553, cost 26.49 s
2023-12-31 06:43:35,371	44k	INFO	====> Epoch: 554, cost 26.45 s
2023-12-31 06:43:43,675	44k	INFO	Train Epoch: 555 [29%]
2023-12-31 06:43:43,675	44k	INFO	Losses: [2.4810080528259277, 2.333460807800293, 5.309926986694336, 19.495765686035156, 0.8989807963371277], step: 19400, lr: 9.30875742853183e-05, reference_loss: 30.519142150878906
2023-12-31 06:44:02,142	44k	INFO	====> Epoch: 555, cost 26.77 s
2023-12-31 06:44:28,578	44k	INFO	====> Epoch: 556, cost 26.44 s
2023-12-31 06:44:55,235	44k	INFO	====> Epoch: 557, cost 26.66 s
2023-12-31 06:45:21,880	44k	INFO	====> Epoch: 558, cost 26.64 s
2023-12-31 06:45:48,439	44k	INFO	====> Epoch: 559, cost 26.56 s
2023-12-31 06:46:15,183	44k	INFO	====> Epoch: 560, cost 26.74 s
2023-12-31 06:46:15,948	44k	INFO	Train Epoch: 561 [0%]
2023-12-31 06:46:15,949	44k	INFO	Losses: [2.6247522830963135, 2.0851476192474365, 4.547306060791016, 17.723432540893555, 0.7788399457931519], step: 19600, lr: 9.301778041836861e-05, reference_loss: 27.759477615356445
2023-12-31 06:46:42,232	44k	INFO	====> Epoch: 561, cost 27.05 s
2023-12-31 06:47:08,746	44k	INFO	====> Epoch: 562, cost 26.51 s
2023-12-31 06:47:35,127	44k	INFO	====> Epoch: 563, cost 26.38 s
2023-12-31 06:48:01,726	44k	INFO	====> Epoch: 564, cost 26.60 s
2023-12-31 06:48:28,204	44k	INFO	====> Epoch: 565, cost 26.48 s
2023-12-31 06:48:47,854	44k	INFO	Train Epoch: 566 [71%]
2023-12-31 06:48:47,854	44k	INFO	Losses: [2.3625407218933105, 2.5348806381225586, 6.591777801513672, 21.053804397583008, 0.9935386180877686], step: 19800, lr: 9.295965883781867e-05, reference_loss: 33.53654098510742
2023-12-31 06:48:55,341	44k	INFO	====> Epoch: 566, cost 27.14 s
2023-12-31 06:49:21,739	44k	INFO	====> Epoch: 567, cost 26.40 s
2023-12-31 06:49:48,052	44k	INFO	====> Epoch: 568, cost 26.31 s
2023-12-31 06:50:14,537	44k	INFO	====> Epoch: 569, cost 26.48 s
2023-12-31 06:50:41,089	44k	INFO	====> Epoch: 570, cost 26.55 s
2023-12-31 06:51:07,607	44k	INFO	====> Epoch: 571, cost 26.52 s
2023-12-31 06:51:19,740	44k	INFO	Train Epoch: 572 [43%]
2023-12-31 06:51:19,740	44k	INFO	Losses: [2.774615526199341, 1.9029680490493774, 4.836165904998779, 19.098464965820312, 0.5455649495124817], step: 20000, lr: 9.288996087747943e-05, reference_loss: 29.157779693603516
2023-12-31 06:51:26,883	44k	INFO	Saving model and optimizer state at iteration 572 to ./logs/44k/G_20000.pth
2023-12-31 06:51:28,368	44k	INFO	Saving model and optimizer state at iteration 572 to ./logs/44k/D_20000.pth
2023-12-31 06:51:29,141	44k	INFO	.. Free up space by deleting ckpt ./logs/44k/G_17600.pth
2023-12-31 06:51:29,198	44k	INFO	.. Free up space by deleting ckpt ./logs/44k/D_17600.pth
2023-12-31 06:51:43,440	44k	INFO	====> Epoch: 572, cost 35.83 s
2023-12-31 06:52:09,926	44k	INFO	====> Epoch: 573, cost 26.49 s
2023-12-31 06:52:36,559	44k	INFO	====> Epoch: 574, cost 26.63 s
2023-12-31 06:53:03,028	44k	INFO	====> Epoch: 575, cost 26.47 s
2023-12-31 06:53:29,462	44k	INFO	====> Epoch: 576, cost 26.43 s
2023-12-31 06:53:55,970	44k	INFO	====> Epoch: 577, cost 26.51 s
2023-12-31 06:54:00,554	44k	INFO	Train Epoch: 578 [14%]
2023-12-31 06:54:00,554	44k	INFO	Losses: [2.5437796115875244, 2.2306480407714844, 4.421215057373047, 20.636486053466797, 0.8932268619537354], step: 20200, lr: 9.282031517427769e-05, reference_loss: 30.72535514831543
2023-12-31 06:54:23,036	44k	INFO	====> Epoch: 578, cost 27.07 s
2023-12-31 06:54:49,809	44k	INFO	====> Epoch: 579, cost 26.77 s
2023-12-31 06:55:16,353	44k	INFO	====> Epoch: 580, cost 26.54 s
2023-12-31 06:55:42,752	44k	INFO	====> Epoch: 581, cost 26.40 s
2023-12-31 06:56:09,210	44k	INFO	====> Epoch: 582, cost 26.46 s
2023-12-31 06:56:32,518	44k	INFO	Train Epoch: 583 [86%]
2023-12-31 06:56:32,519	44k	INFO	Losses: [2.1485304832458496, 2.3762123584747314, 7.598405838012695, 21.628116607666016, 1.0314064025878906], step: 20400, lr: 9.276231697865521e-05, reference_loss: 34.78267288208008
2023-12-31 06:56:35,988	44k	INFO	====> Epoch: 583, cost 26.78 s
2023-12-31 06:57:02,403	44k	INFO	====> Epoch: 584, cost 26.41 s
2023-12-31 06:57:28,861	44k	INFO	====> Epoch: 585, cost 26.46 s
2023-12-31 06:57:55,804	44k	INFO	====> Epoch: 586, cost 26.94 s
2023-12-31 06:58:22,292	44k	INFO	====> Epoch: 587, cost 26.49 s
2023-12-31 06:58:48,918	44k	INFO	====> Epoch: 588, cost 26.63 s
2023-12-31 06:59:04,751	44k	INFO	Train Epoch: 589 [57%]
2023-12-31 06:59:04,751	44k	INFO	Losses: [2.7921011447906494, 2.170214891433716, 4.009557723999023, 17.974761962890625, 0.679306149482727], step: 20600, lr: 9.269276697846605e-05, reference_loss: 27.62594223022461
2023-12-31 06:59:15,816	44k	INFO	====> Epoch: 589, cost 26.90 s
2023-12-31 06:59:42,452	44k	INFO	====> Epoch: 590, cost 26.64 s
2023-12-31 07:00:09,172	44k	INFO	====> Epoch: 591, cost 26.72 s
2023-12-31 07:00:35,703	44k	INFO	====> Epoch: 592, cost 26.53 s
2023-12-31 07:01:02,533	44k	INFO	====> Epoch: 593, cost 26.83 s
2023-12-31 07:01:29,003	44k	INFO	====> Epoch: 594, cost 26.47 s
2023-12-31 07:01:37,399	44k	INFO	Train Epoch: 595 [29%]
2023-12-31 07:01:37,400	44k	INFO	Losses: [2.424952507019043, 2.176743984222412, 5.991600036621094, 20.663713455200195, 0.9542362093925476], step: 20800, lr: 9.262326912447895e-05, reference_loss: 32.211246490478516
2023-12-31 07:01:44,690	44k	INFO	Saving model and optimizer state at iteration 595 to ./logs/44k/G_20800.pth
2023-12-31 07:01:45,860	44k	INFO	Saving model and optimizer state at iteration 595 to ./logs/44k/D_20800.pth
2023-12-31 07:01:46,641	44k	INFO	.. Free up space by deleting ckpt ./logs/44k/G_18400.pth
2023-12-31 07:01:46,698	44k	INFO	.. Free up space by deleting ckpt ./logs/44k/D_18400.pth
2023-12-31 07:02:04,565	44k	INFO	====> Epoch: 595, cost 35.56 s
2023-12-31 07:02:31,176	44k	INFO	====> Epoch: 596, cost 26.61 s
2023-12-31 07:02:57,755	44k	INFO	====> Epoch: 597, cost 26.58 s
2023-12-31 07:03:24,415	44k	INFO	====> Epoch: 598, cost 26.66 s
2023-12-31 07:03:51,040	44k	INFO	====> Epoch: 599, cost 26.63 s
2023-12-31 07:04:17,615	44k	INFO	====> Epoch: 600, cost 26.57 s
2023-12-31 07:04:18,397	44k	INFO	Train Epoch: 601 [0%]
2023-12-31 07:04:18,397	44k	INFO	Losses: [2.594883441925049, 2.2582578659057617, 5.607880115509033, 19.55988121032715, 0.7901635766029358], step: 21000, lr: 9.255382337759651e-05, reference_loss: 30.811065673828125
2023-12-31 07:04:44,683	44k	INFO	====> Epoch: 601, cost 27.07 s
2023-12-31 07:05:11,253	44k	INFO	====> Epoch: 602, cost 26.57 s
2023-12-31 07:05:37,781	44k	INFO	====> Epoch: 603, cost 26.53 s
2023-12-31 07:06:04,371	44k	INFO	====> Epoch: 604, cost 26.59 s
2023-12-31 07:06:30,863	44k	INFO	====> Epoch: 605, cost 26.49 s
2023-12-31 07:06:50,561	44k	INFO	Train Epoch: 606 [71%]
2023-12-31 07:06:50,562	44k	INFO	Losses: [2.4516119956970215, 2.0514986515045166, 5.086838722229004, 18.41765594482422, 0.7167657613754272], step: 21200, lr: 9.249599169771281e-05, reference_loss: 28.72437286376953
2023-12-31 07:06:58,219	44k	INFO	====> Epoch: 606, cost 27.36 s
2023-12-31 07:07:24,583	44k	INFO	====> Epoch: 607, cost 26.36 s
2023-12-31 07:07:51,239	44k	INFO	====> Epoch: 608, cost 26.66 s
2023-12-31 07:08:17,915	44k	INFO	====> Epoch: 609, cost 26.68 s
2023-12-31 07:08:44,404	44k	INFO	====> Epoch: 610, cost 26.49 s
2023-12-31 07:09:10,896	44k	INFO	====> Epoch: 611, cost 26.49 s
2023-12-31 07:09:23,150	44k	INFO	Train Epoch: 612 [43%]
2023-12-31 07:09:23,150	44k	INFO	Losses: [2.4779531955718994, 2.5399272441864014, 6.59965705871582, 19.21535301208496, 0.7368351817131042], step: 21400, lr: 9.242664137907478e-05, reference_loss: 31.569725036621094
2023-12-31 07:09:38,124	44k	INFO	====> Epoch: 612, cost 27.23 s
2023-12-31 07:10:04,767	44k	INFO	====> Epoch: 613, cost 26.64 s
2023-12-31 07:10:31,213	44k	INFO	====> Epoch: 614, cost 26.45 s
2023-12-31 07:10:57,759	44k	INFO	====> Epoch: 615, cost 26.55 s
2023-12-31 07:11:24,312	44k	INFO	====> Epoch: 616, cost 26.55 s
2023-12-31 07:11:50,947	44k	INFO	====> Epoch: 617, cost 26.64 s
2023-12-31 07:11:55,563	44k	INFO	Train Epoch: 618 [14%]
2023-12-31 07:11:55,563	44k	INFO	Losses: [2.5547008514404297, 2.166191577911377, 5.2613444328308105, 19.553340911865234, 0.6781546473503113], step: 21600, lr: 9.235734305692444e-05, reference_loss: 30.21373176574707
2023-12-31 07:12:02,744	44k	INFO	Saving model and optimizer state at iteration 618 to ./logs/44k/G_21600.pth
2023-12-31 07:12:03,933	44k	INFO	Saving model and optimizer state at iteration 618 to ./logs/44k/D_21600.pth
2023-12-31 07:12:04,708	44k	INFO	.. Free up space by deleting ckpt ./logs/44k/G_19200.pth
2023-12-31 07:12:04,764	44k	INFO	.. Free up space by deleting ckpt ./logs/44k/D_19200.pth
2023-12-31 07:12:27,035	44k	INFO	====> Epoch: 618, cost 36.09 s
2023-12-31 07:12:53,660	44k	INFO	====> Epoch: 619, cost 26.62 s
2023-12-31 07:13:20,279	44k	INFO	====> Epoch: 620, cost 26.62 s
2023-12-31 07:13:46,788	44k	INFO	====> Epoch: 621, cost 26.51 s
2023-12-31 07:14:13,275	44k	INFO	====> Epoch: 622, cost 26.49 s
2023-12-31 07:14:36,809	44k	INFO	Train Epoch: 623 [86%]
2023-12-31 07:14:36,810	44k	INFO	Losses: [2.478750705718994, 2.4629251956939697, 6.747716426849365, 20.052047729492188, 0.5426396727561951], step: 21800, lr: 9.229963414654495e-05, reference_loss: 32.284080505371094
2023-12-31 07:14:40,451	44k	INFO	====> Epoch: 623, cost 27.18 s
2023-12-31 07:15:07,049	44k	INFO	====> Epoch: 624, cost 26.60 s
2023-12-31 07:15:33,678	44k	INFO	====> Epoch: 625, cost 26.63 s
2023-12-31 07:16:00,349	44k	INFO	====> Epoch: 626, cost 26.67 s
2023-12-31 07:16:27,053	44k	INFO	====> Epoch: 627, cost 26.70 s
2023-12-31 07:16:53,612	44k	INFO	====> Epoch: 628, cost 26.56 s
2023-12-31 07:17:09,524	44k	INFO	Train Epoch: 629 [57%]
2023-12-31 07:17:09,524	44k	INFO	Losses: [2.440417528152466, 2.3278608322143555, 5.139432907104492, 19.990556716918945, 0.7411754727363586], step: 22000, lr: 9.223043105005667e-05, reference_loss: 30.639442443847656
2023-12-31 07:17:20,499	44k	INFO	====> Epoch: 629, cost 26.89 s
2023-12-31 07:17:47,005	44k	INFO	====> Epoch: 630, cost 26.51 s
2023-12-31 07:18:13,691	44k	INFO	====> Epoch: 631, cost 26.69 s
2023-12-31 07:18:40,305	44k	INFO	====> Epoch: 632, cost 26.61 s
2023-12-31 07:19:06,929	44k	INFO	====> Epoch: 633, cost 26.62 s
2023-12-31 07:19:33,666	44k	INFO	====> Epoch: 634, cost 26.74 s
2023-12-31 07:19:42,063	44k	INFO	Train Epoch: 635 [29%]
2023-12-31 07:19:42,063	44k	INFO	Losses: [2.720053195953369, 1.8740322589874268, 4.591837406158447, 19.27048683166504, 0.9146570563316345], step: 22200, lr: 9.216127983967398e-05, reference_loss: 29.37106704711914
2023-12-31 07:20:00,575	44k	INFO	====> Epoch: 635, cost 26.91 s
2023-12-31 07:20:27,248	44k	INFO	====> Epoch: 636, cost 26.67 s
2023-12-31 07:20:53,806	44k	INFO	====> Epoch: 637, cost 26.56 s
2023-12-31 07:21:20,425	44k	INFO	====> Epoch: 638, cost 26.62 s
2023-12-31 07:21:47,001	44k	INFO	====> Epoch: 639, cost 26.58 s
2023-12-31 07:22:13,494	44k	INFO	====> Epoch: 640, cost 26.49 s
2023-12-31 07:22:14,263	44k	INFO	Train Epoch: 641 [0%]
2023-12-31 07:22:14,264	44k	INFO	Losses: [2.389336585998535, 2.259206771850586, 5.502501487731934, 19.253253936767578, 0.6062136888504028], step: 22400, lr: 9.209218047649445e-05, reference_loss: 30.010513305664062
2023-12-31 07:22:21,732	44k	INFO	Saving model and optimizer state at iteration 641 to ./logs/44k/G_22400.pth
2023-12-31 07:22:22,900	44k	INFO	Saving model and optimizer state at iteration 641 to ./logs/44k/D_22400.pth
2023-12-31 07:22:23,675	44k	INFO	.. Free up space by deleting ckpt ./logs/44k/G_20000.pth
2023-12-31 07:22:23,732	44k	INFO	.. Free up space by deleting ckpt ./logs/44k/D_20000.pth
2023-12-31 07:22:49,583	44k	INFO	====> Epoch: 641, cost 36.09 s
2023-12-31 07:23:16,094	44k	INFO	====> Epoch: 642, cost 26.51 s
2023-12-31 07:23:42,549	44k	INFO	====> Epoch: 643, cost 26.45 s
2023-12-31 07:24:08,954	44k	INFO	====> Epoch: 644, cost 26.41 s
2023-12-31 07:24:35,612	44k	INFO	====> Epoch: 645, cost 26.66 s
2023-12-31 07:24:55,485	44k	INFO	Train Epoch: 646 [71%]
2023-12-31 07:24:55,485	44k	INFO	Losses: [2.6045706272125244, 2.1668553352355957, 5.97829008102417, 19.775110244750977, 0.745635449886322], step: 22600, lr: 9.203463725130127e-05, reference_loss: 31.270462036132812
2023-12-31 07:25:02,984	44k	INFO	====> Epoch: 646, cost 27.37 s
2023-12-31 07:25:29,424	44k	INFO	====> Epoch: 647, cost 26.44 s
2023-12-31 07:25:55,801	44k	INFO	====> Epoch: 648, cost 26.38 s
2023-12-31 07:26:22,178	44k	INFO	====> Epoch: 649, cost 26.38 s
2023-12-31 07:26:48,778	44k	INFO	====> Epoch: 650, cost 26.60 s
2023-12-31 07:27:15,236	44k	INFO	====> Epoch: 651, cost 26.46 s
2023-12-31 07:27:27,447	44k	INFO	Train Epoch: 652 [43%]
2023-12-31 07:27:27,447	44k	INFO	Losses: [2.409785270690918, 2.453289270401001, 5.364068984985352, 17.51328468322754, 0.871637761592865], step: 22800, lr: 9.19656328403861e-05, reference_loss: 28.6120662689209
2023-12-31 07:27:42,339	44k	INFO	====> Epoch: 652, cost 27.10 s
2023-12-31 07:28:09,069	44k	INFO	====> Epoch: 653, cost 26.73 s
2023-12-31 07:28:35,349	44k	INFO	====> Epoch: 654, cost 26.28 s
2023-12-31 07:29:01,807	44k	INFO	====> Epoch: 655, cost 26.46 s
2023-12-31 07:29:28,319	44k	INFO	====> Epoch: 656, cost 26.51 s
2023-12-31 07:29:54,822	44k	INFO	====> Epoch: 657, cost 26.50 s
2023-12-31 07:29:59,404	44k	INFO	Train Epoch: 658 [14%]
2023-12-31 07:29:59,405	44k	INFO	Losses: [2.6488194465637207, 2.1805930137634277, 5.352760314941406, 21.159046173095703, 0.9099370837211609], step: 23000, lr: 9.189668016660891e-05, reference_loss: 32.251155853271484
2023-12-31 07:30:21,899	44k	INFO	====> Epoch: 658, cost 27.08 s
2023-12-31 07:30:48,606	44k	INFO	====> Epoch: 659, cost 26.71 s
2023-12-31 07:31:15,304	44k	INFO	====> Epoch: 660, cost 26.70 s
2023-12-31 07:31:41,616	44k	INFO	====> Epoch: 661, cost 26.31 s
2023-12-31 07:32:08,033	44k	INFO	====> Epoch: 662, cost 26.42 s
2023-12-31 07:32:31,516	44k	INFO	Train Epoch: 663 [86%]
2023-12-31 07:32:31,517	44k	INFO	Losses: [2.4636282920837402, 2.1411800384521484, 5.219951629638672, 18.35393714904785, 0.6328330636024475], step: 23200, lr: 9.183925909856629e-05, reference_loss: 28.81153106689453
2023-12-31 07:32:38,659	44k	INFO	Saving model and optimizer state at iteration 663 to ./logs/44k/G_23200.pth
2023-12-31 07:32:39,820	44k	INFO	Saving model and optimizer state at iteration 663 to ./logs/44k/D_23200.pth
2023-12-31 07:32:40,596	44k	INFO	.. Free up space by deleting ckpt ./logs/44k/G_20800.pth
2023-12-31 07:32:40,652	44k	INFO	.. Free up space by deleting ckpt ./logs/44k/D_20800.pth
2023-12-31 07:32:43,688	44k	INFO	====> Epoch: 663, cost 35.66 s
2023-12-31 07:33:10,163	44k	INFO	====> Epoch: 664, cost 26.47 s
2023-12-31 07:33:36,660	44k	INFO	====> Epoch: 665, cost 26.50 s
2023-12-31 07:34:03,447	44k	INFO	====> Epoch: 666, cost 26.79 s
2023-12-31 07:34:30,051	44k	INFO	====> Epoch: 667, cost 26.60 s
2023-12-31 07:34:56,566	44k	INFO	====> Epoch: 668, cost 26.51 s
2023-12-31 07:35:12,606	44k	INFO	Train Epoch: 669 [57%]
2023-12-31 07:35:12,607	44k	INFO	Losses: [2.4949896335601807, 2.1336824893951416, 4.796624183654785, 17.580766677856445, 0.8753998875617981], step: 23400, lr: 9.177040117548157e-05, reference_loss: 27.8814640045166
2023-12-31 07:35:23,652	44k	INFO	====> Epoch: 669, cost 27.09 s
2023-12-31 07:35:50,203	44k	INFO	====> Epoch: 670, cost 26.55 s
2023-12-31 07:36:16,816	44k	INFO	====> Epoch: 671, cost 26.61 s
2023-12-31 07:36:43,335	44k	INFO	====> Epoch: 672, cost 26.52 s
2023-12-31 07:37:09,758	44k	INFO	====> Epoch: 673, cost 26.42 s
2023-12-31 07:37:36,346	44k	INFO	====> Epoch: 674, cost 26.59 s
2023-12-31 07:37:44,741	44k	INFO	Train Epoch: 675 [29%]
2023-12-31 07:37:44,741	44k	INFO	Losses: [2.5568108558654785, 2.2042934894561768, 4.592853546142578, 18.259811401367188, 0.48018258810043335], step: 23600, lr: 9.170159487970326e-05, reference_loss: 28.093952178955078
2023-12-31 07:38:03,377	44k	INFO	====> Epoch: 675, cost 27.03 s
2023-12-31 07:38:29,860	44k	INFO	====> Epoch: 676, cost 26.48 s
2023-12-31 07:38:56,356	44k	INFO	====> Epoch: 677, cost 26.50 s
2023-12-31 07:39:22,934	44k	INFO	====> Epoch: 678, cost 26.58 s
2023-12-31 07:39:49,572	44k	INFO	====> Epoch: 679, cost 26.64 s
2023-12-31 07:40:16,206	44k	INFO	====> Epoch: 680, cost 26.63 s
2023-12-31 07:40:16,977	44k	INFO	Train Epoch: 681 [0%]
2023-12-31 07:40:16,978	44k	INFO	Losses: [2.480142116546631, 2.061903953552246, 4.985220909118652, 16.29104232788086, 0.3771631121635437], step: 23800, lr: 9.163284017252299e-05, reference_loss: 26.195472717285156
2023-12-31 07:40:43,509	44k	INFO	====> Epoch: 681, cost 27.30 s
2023-12-31 07:41:10,067	44k	INFO	====> Epoch: 682, cost 26.56 s
2023-12-31 07:41:36,684	44k	INFO	====> Epoch: 683, cost 26.62 s
2023-12-31 07:42:03,238	44k	INFO	====> Epoch: 684, cost 26.55 s
2023-12-31 07:42:29,499	44k	INFO	====> Epoch: 685, cost 26.26 s
2023-12-31 07:42:49,086	44k	INFO	Train Epoch: 686 [71%]
2023-12-31 07:42:49,087	44k	INFO	Losses: [2.8349947929382324, 2.1485137939453125, 3.6191048622131348, 16.325279235839844, 1.0142478942871094], step: 24000, lr: 9.157558396325682e-05, reference_loss: 25.942140579223633
2023-12-31 07:42:56,265	44k	INFO	Saving model and optimizer state at iteration 686 to ./logs/44k/G_24000.pth
2023-12-31 07:42:57,733	44k	INFO	Saving model and optimizer state at iteration 686 to ./logs/44k/D_24000.pth
2023-12-31 07:42:58,513	44k	INFO	.. Free up space by deleting ckpt ./logs/44k/G_21600.pth
2023-12-31 07:42:58,570	44k	INFO	.. Free up space by deleting ckpt ./logs/44k/D_21600.pth
2023-12-31 07:43:05,336	44k	INFO	====> Epoch: 686, cost 35.84 s
2023-12-31 07:43:31,571	44k	INFO	====> Epoch: 687, cost 26.23 s
2023-12-31 07:43:57,911	44k	INFO	====> Epoch: 688, cost 26.34 s
2023-12-31 07:44:24,541	44k	INFO	====> Epoch: 689, cost 26.63 s
2023-12-31 07:44:51,122	44k	INFO	====> Epoch: 690, cost 26.58 s
2023-12-31 07:45:17,691	44k	INFO	====> Epoch: 691, cost 26.57 s
2023-12-31 07:45:29,783	44k	INFO	Train Epoch: 692 [43%]
2023-12-31 07:45:29,783	44k	INFO	Losses: [2.6450037956237793, 2.0788161754608154, 4.246474266052246, 17.176395416259766, 1.1535142660140991], step: 24200, lr: 9.150692373473501e-05, reference_loss: 27.30020523071289
2023-12-31 07:45:44,584	44k	INFO	====> Epoch: 692, cost 26.89 s
2023-12-31 07:46:11,372	44k	INFO	====> Epoch: 693, cost 26.79 s
2023-12-31 07:46:37,914	44k	INFO	====> Epoch: 694, cost 26.54 s
2023-12-31 07:47:04,575	44k	INFO	====> Epoch: 695, cost 26.66 s
2023-12-31 07:47:31,016	44k	INFO	====> Epoch: 696, cost 26.44 s
2023-12-31 07:47:57,289	44k	INFO	====> Epoch: 697, cost 26.27 s
2023-12-31 07:48:01,852	44k	INFO	Train Epoch: 698 [14%]
2023-12-31 07:48:01,853	44k	INFO	Losses: [2.5821592807769775, 2.1304969787597656, 5.337740898132324, 19.774883270263672, 0.7181478142738342], step: 24400, lr: 9.143831498529502e-05, reference_loss: 30.543426513671875
2023-12-31 07:48:24,390	44k	INFO	====> Epoch: 698, cost 27.10 s
2023-12-31 07:48:50,770	44k	INFO	====> Epoch: 699, cost 26.38 s
2023-12-31 07:49:17,463	44k	INFO	====> Epoch: 700, cost 26.69 s
2023-12-31 07:49:43,875	44k	INFO	====> Epoch: 701, cost 26.41 s
2023-12-31 07:50:10,341	44k	INFO	====> Epoch: 702, cost 26.47 s
2023-12-31 07:50:33,824	44k	INFO	Train Epoch: 703 [86%]
2023-12-31 07:50:33,825	44k	INFO	Losses: [2.3773319721221924, 2.416930675506592, 7.205808639526367, 18.733448028564453, 0.7689931392669678], step: 24600, lr: 9.138118032388012e-05, reference_loss: 31.502513885498047
2023-12-31 07:50:37,464	44k	INFO	====> Epoch: 703, cost 27.12 s
2023-12-31 07:51:04,012	44k	INFO	====> Epoch: 704, cost 26.55 s
2023-12-31 07:51:30,714	44k	INFO	====> Epoch: 705, cost 26.70 s
2023-12-31 07:51:57,300	44k	INFO	====> Epoch: 706, cost 26.59 s
2023-12-31 07:52:23,855	44k	INFO	====> Epoch: 707, cost 26.55 s
2023-12-31 07:52:50,637	44k	INFO	====> Epoch: 708, cost 26.78 s
2023-12-31 07:53:06,576	44k	INFO	Train Epoch: 709 [57%]
2023-12-31 07:53:06,577	44k	INFO	Losses: [2.311140537261963, 2.1887834072113037, 5.9684576988220215, 19.893564224243164, 0.7032315135002136], step: 24800, lr: 9.13126658525321e-05, reference_loss: 31.06517791748047
2023-12-31 07:53:13,873	44k	INFO	Saving model and optimizer state at iteration 709 to ./logs/44k/G_24800.pth
2023-12-31 07:53:15,041	44k	INFO	Saving model and optimizer state at iteration 709 to ./logs/44k/D_24800.pth
2023-12-31 07:53:15,811	44k	INFO	.. Free up space by deleting ckpt ./logs/44k/G_22400.pth
2023-12-31 07:53:15,868	44k	INFO	.. Free up space by deleting ckpt ./logs/44k/D_22400.pth
2023-12-31 07:53:26,484	44k	INFO	====> Epoch: 709, cost 35.85 s
2023-12-31 07:53:52,867	44k	INFO	====> Epoch: 710, cost 26.38 s
2023-12-31 07:54:19,449	44k	INFO	====> Epoch: 711, cost 26.58 s
2023-12-31 07:54:46,016	44k	INFO	====> Epoch: 712, cost 26.57 s
2023-12-31 07:55:12,707	44k	INFO	====> Epoch: 713, cost 26.69 s
2023-12-31 07:55:39,029	44k	INFO	====> Epoch: 714, cost 26.32 s
2023-12-31 07:55:47,364	44k	INFO	Train Epoch: 715 [29%]
2023-12-31 07:55:47,365	44k	INFO	Losses: [2.5672693252563477, 2.174380302429199, 5.439319133758545, 18.45043182373047, 0.6781942248344421], step: 25000, lr: 9.124420275098216e-05, reference_loss: 29.309595108032227
2023-12-31 07:56:05,819	44k	INFO	====> Epoch: 715, cost 26.79 s
2023-12-31 07:56:32,447	44k	INFO	====> Epoch: 716, cost 26.63 s
2023-12-31 07:56:59,079	44k	INFO	====> Epoch: 717, cost 26.63 s
2023-12-31 07:57:25,558	44k	INFO	====> Epoch: 718, cost 26.48 s
2023-12-31 07:57:52,123	44k	INFO	====> Epoch: 719, cost 26.56 s
2023-12-31 07:58:18,708	44k	INFO	====> Epoch: 720, cost 26.58 s
2023-12-31 07:58:19,479	44k	INFO	Train Epoch: 721 [0%]
2023-12-31 07:58:19,480	44k	INFO	Losses: [2.547325611114502, 2.04060697555542, 4.731929779052734, 17.10010528564453, 0.7528742551803589], step: 25200, lr: 9.117579098071503e-05, reference_loss: 27.172842025756836
2023-12-31 07:58:46,128	44k	INFO	====> Epoch: 721, cost 27.42 s
2023-12-31 07:59:12,677	44k	INFO	====> Epoch: 722, cost 26.55 s
2023-12-31 07:59:39,253	44k	INFO	====> Epoch: 723, cost 26.58 s
2023-12-31 08:00:05,887	44k	INFO	====> Epoch: 724, cost 26.63 s
2023-12-31 08:00:32,486	44k	INFO	====> Epoch: 725, cost 26.60 s
2023-12-31 08:00:52,246	44k	INFO	Train Epoch: 726 [71%]
2023-12-31 08:00:52,247	44k	INFO	Losses: [2.3929855823516846, 2.5337460041046143, 7.192417144775391, 20.965904235839844, 0.8755711317062378], step: 25400, lr: 9.111882035578874e-05, reference_loss: 33.96062469482422
2023-12-31 08:00:59,519	44k	INFO	====> Epoch: 726, cost 27.03 s
2023-12-31 08:01:26,342	44k	INFO	====> Epoch: 727, cost 26.82 s
2023-12-31 08:01:52,745	44k	INFO	====> Epoch: 728, cost 26.40 s
2023-12-31 08:02:19,166	44k	INFO	====> Epoch: 729, cost 26.42 s
2023-12-31 08:02:45,747	44k	INFO	====> Epoch: 730, cost 26.58 s
2023-12-31 08:03:12,376	44k	INFO	====> Epoch: 731, cost 26.63 s
2023-12-31 08:03:24,615	44k	INFO	Train Epoch: 732 [43%]
2023-12-31 08:03:24,616	44k	INFO	Losses: [2.7883780002593994, 2.194929838180542, 5.343609809875488, 18.051687240600586, 0.6852308511734009], step: 25600, lr: 9.10505025929364e-05, reference_loss: 29.06383514404297
2023-12-31 08:03:31,905	44k	INFO	Saving model and optimizer state at iteration 732 to ./logs/44k/G_25600.pth
2023-12-31 08:03:33,086	44k	INFO	Saving model and optimizer state at iteration 732 to ./logs/44k/D_25600.pth
2023-12-31 08:03:33,857	44k	INFO	.. Free up space by deleting ckpt ./logs/44k/G_23200.pth
2023-12-31 08:03:33,913	44k	INFO	.. Free up space by deleting ckpt ./logs/44k/D_23200.pth
2023-12-31 08:03:48,670	44k	INFO	====> Epoch: 732, cost 36.29 s
2023-12-31 08:04:14,980	44k	INFO	====> Epoch: 733, cost 26.31 s
2023-12-31 08:04:41,550	44k	INFO	====> Epoch: 734, cost 26.57 s
2023-12-31 08:05:08,115	44k	INFO	====> Epoch: 735, cost 26.56 s
2023-12-31 08:05:34,803	44k	INFO	====> Epoch: 736, cost 26.69 s
2023-12-31 08:06:01,432	44k	INFO	====> Epoch: 737, cost 26.63 s
2023-12-31 08:06:06,038	44k	INFO	Train Epoch: 738 [14%]
2023-12-31 08:06:06,038	44k	INFO	Losses: [2.4644486904144287, 2.2447240352630615, 5.178922176361084, 21.25619888305664, 0.773263692855835], step: 25800, lr: 9.098223605239689e-05, reference_loss: 31.917558670043945
2023-12-31 08:06:28,495	44k	INFO	====> Epoch: 738, cost 27.06 s
2023-12-31 08:06:54,904	44k	INFO	====> Epoch: 739, cost 26.41 s
2023-12-31 08:07:21,366	44k	INFO	====> Epoch: 740, cost 26.46 s
2023-12-31 08:07:48,037	44k	INFO	====> Epoch: 741, cost 26.67 s
2023-12-31 08:08:14,511	44k	INFO	====> Epoch: 742, cost 26.47 s
2023-12-31 08:08:37,772	44k	INFO	Train Epoch: 743 [86%]
2023-12-31 08:08:37,773	44k	INFO	Losses: [2.2889175415039062, 2.1657309532165527, 7.845996856689453, 20.756031036376953, 0.9932013154029846], step: 26000, lr: 9.092538636906162e-05, reference_loss: 34.04988098144531
2023-12-31 08:08:41,269	44k	INFO	====> Epoch: 743, cost 26.76 s
2023-12-31 08:09:07,788	44k	INFO	====> Epoch: 744, cost 26.52 s
2023-12-31 08:09:34,332	44k	INFO	====> Epoch: 745, cost 26.54 s
2023-12-31 08:10:00,784	44k	INFO	====> Epoch: 746, cost 26.45 s
2023-12-31 08:10:27,427	44k	INFO	====> Epoch: 747, cost 26.64 s
2023-12-31 08:10:54,032	44k	INFO	====> Epoch: 748, cost 26.60 s
2023-12-31 08:11:10,208	44k	INFO	Train Epoch: 749 [57%]
2023-12-31 08:11:10,209	44k	INFO	Losses: [2.851947784423828, 2.0614013671875, 4.074249744415283, 16.46481704711914, 0.6746528148651123], step: 26200, lr: 9.085721363637077e-05, reference_loss: 26.12706756591797
2023-12-31 08:11:21,304	44k	INFO	====> Epoch: 749, cost 27.27 s
2023-12-31 08:11:47,789	44k	INFO	====> Epoch: 750, cost 26.49 s
2023-12-31 08:12:14,079	44k	INFO	====> Epoch: 751, cost 26.29 s
2023-12-31 08:12:40,652	44k	INFO	====> Epoch: 752, cost 26.57 s
2023-12-31 08:13:07,093	44k	INFO	====> Epoch: 753, cost 26.44 s
2023-12-31 08:13:33,604	44k	INFO	====> Epoch: 754, cost 26.51 s
2023-12-31 08:13:42,030	44k	INFO	Train Epoch: 755 [29%]
2023-12-31 08:13:42,030	44k	INFO	Losses: [2.5031561851501465, 2.168484687805176, 6.104036808013916, 20.212072372436523, 0.8607897162437439], step: 26400, lr: 9.078909201725413e-05, reference_loss: 31.848541259765625
2023-12-31 08:13:49,606	44k	INFO	Saving model and optimizer state at iteration 755 to ./logs/44k/G_26400.pth
2023-12-31 08:13:50,787	44k	INFO	Saving model and optimizer state at iteration 755 to ./logs/44k/D_26400.pth
2023-12-31 08:13:51,563	44k	INFO	.. Free up space by deleting ckpt ./logs/44k/G_24000.pth
2023-12-31 08:13:51,619	44k	INFO	.. Free up space by deleting ckpt ./logs/44k/D_24000.pth
2023-12-31 08:14:09,723	44k	INFO	====> Epoch: 755, cost 36.12 s
2023-12-31 08:14:36,080	44k	INFO	====> Epoch: 756, cost 26.36 s
2023-12-31 08:15:02,484	44k	INFO	====> Epoch: 757, cost 26.40 s
2023-12-31 08:15:28,947	44k	INFO	====> Epoch: 758, cost 26.46 s
2023-12-31 08:15:55,442	44k	INFO	====> Epoch: 759, cost 26.50 s
2023-12-31 08:16:21,957	44k	INFO	====> Epoch: 760, cost 26.51 s
2023-12-31 08:16:22,730	44k	INFO	Train Epoch: 761 [0%]
2023-12-31 08:16:22,730	44k	INFO	Losses: [2.530170440673828, 2.2791457176208496, 6.371140956878662, 19.10381317138672, 0.7510326504707336], step: 26600, lr: 9.072102147338848e-05, reference_loss: 31.035303115844727
2023-12-31 08:16:49,253	44k	INFO	====> Epoch: 761, cost 27.30 s
2023-12-31 08:17:15,770	44k	INFO	====> Epoch: 762, cost 26.52 s
2023-12-31 08:17:42,083	44k	INFO	====> Epoch: 763, cost 26.31 s
2023-12-31 08:18:08,626	44k	INFO	====> Epoch: 764, cost 26.54 s
2023-12-31 08:18:35,137	44k	INFO	====> Epoch: 765, cost 26.51 s
2023-12-31 08:18:54,751	44k	INFO	Train Epoch: 766 [71%]
2023-12-31 08:18:54,751	44k	INFO	Losses: [2.612168312072754, 2.0020086765289307, 4.804279804229736, 17.946521759033203, 0.6929933428764343], step: 26800, lr: 9.066433500835542e-05, reference_loss: 28.057971954345703
2023-12-31 08:19:01,877	44k	INFO	====> Epoch: 766, cost 26.74 s
2023-12-31 08:19:28,360	44k	INFO	====> Epoch: 767, cost 26.48 s
2023-12-31 08:19:54,967	44k	INFO	====> Epoch: 768, cost 26.61 s
2023-12-31 08:20:21,427	44k	INFO	====> Epoch: 769, cost 26.46 s
2023-12-31 08:20:47,946	44k	INFO	====> Epoch: 770, cost 26.52 s
2023-12-31 08:21:14,437	44k	INFO	====> Epoch: 771, cost 26.49 s
2023-12-31 08:21:26,541	44k	INFO	Train Epoch: 772 [43%]
2023-12-31 08:21:26,542	44k	INFO	Losses: [2.660601854324341, 2.118988513946533, 5.959946632385254, 17.8870906829834, 0.7036480903625488], step: 27000, lr: 9.059635800301143e-05, reference_loss: 29.330276489257812
2023-12-31 08:21:41,386	44k	INFO	====> Epoch: 772, cost 26.95 s
2023-12-31 08:22:07,798	44k	INFO	====> Epoch: 773, cost 26.41 s
2023-12-31 08:22:34,337	44k	INFO	====> Epoch: 774, cost 26.54 s
2023-12-31 08:23:01,083	44k	INFO	====> Epoch: 775, cost 26.75 s
2023-12-31 08:23:27,414	44k	INFO	====> Epoch: 776, cost 26.33 s
2023-12-31 08:23:54,062	44k	INFO	====> Epoch: 777, cost 26.65 s
2023-12-31 08:23:58,667	44k	INFO	Train Epoch: 778 [14%]
2023-12-31 08:23:58,668	44k	INFO	Losses: [2.6613986492156982, 1.989997148513794, 5.191485404968262, 18.83673858642578, 0.6058685779571533], step: 27200, lr: 9.052843196449197e-05, reference_loss: 29.28548812866211
2023-12-31 08:24:05,988	44k	INFO	Saving model and optimizer state at iteration 778 to ./logs/44k/G_27200.pth
2023-12-31 08:24:07,172	44k	INFO	Saving model and optimizer state at iteration 778 to ./logs/44k/D_27200.pth
2023-12-31 08:24:07,948	44k	INFO	.. Free up space by deleting ckpt ./logs/44k/G_24800.pth
2023-12-31 08:24:08,004	44k	INFO	.. Free up space by deleting ckpt ./logs/44k/D_24800.pth
2023-12-31 08:24:29,950	44k	INFO	====> Epoch: 778, cost 35.89 s
2023-12-31 08:24:56,584	44k	INFO	====> Epoch: 779, cost 26.63 s
2023-12-31 08:25:23,407	44k	INFO	====> Epoch: 780, cost 26.82 s
2023-12-31 08:25:49,818	44k	INFO	====> Epoch: 781, cost 26.41 s
2023-12-31 08:26:16,264	44k	INFO	====> Epoch: 782, cost 26.45 s
2023-12-31 08:26:39,783	44k	INFO	Train Epoch: 783 [86%]
2023-12-31 08:26:39,784	44k	INFO	Losses: [2.3944292068481445, 2.4044179916381836, 6.757989406585693, 18.65877914428711, 0.6060110330581665], step: 27400, lr: 9.04718658378136e-05, reference_loss: 30.821626663208008
2023-12-31 08:26:43,288	44k	INFO	====> Epoch: 783, cost 27.02 s
2023-12-31 08:27:09,741	44k	INFO	====> Epoch: 784, cost 26.45 s
2023-12-31 08:27:36,410	44k	INFO	====> Epoch: 785, cost 26.67 s
2023-12-31 08:28:03,084	44k	INFO	====> Epoch: 786, cost 26.67 s
2023-12-31 08:28:29,532	44k	INFO	====> Epoch: 787, cost 26.45 s
2023-12-31 08:28:56,186	44k	INFO	====> Epoch: 788, cost 26.65 s
2023-12-31 08:29:12,353	44k	INFO	Train Epoch: 789 [57%]
2023-12-31 08:29:12,354	44k	INFO	Losses: [2.59507417678833, 2.151242971420288, 5.4933977127075195, 18.015029907226562, 0.7542266845703125], step: 27600, lr: 9.040403313924505e-05, reference_loss: 29.00897216796875
2023-12-31 08:29:23,324	44k	INFO	====> Epoch: 789, cost 27.14 s
2023-12-31 08:29:49,860	44k	INFO	====> Epoch: 790, cost 26.54 s
2023-12-31 08:30:16,470	44k	INFO	====> Epoch: 791, cost 26.61 s
2023-12-31 08:30:42,999	44k	INFO	====> Epoch: 792, cost 26.53 s
2023-12-31 08:31:09,614	44k	INFO	====> Epoch: 793, cost 26.62 s
2023-12-31 08:31:36,167	44k	INFO	====> Epoch: 794, cost 26.55 s
2023-12-31 08:31:44,661	44k	INFO	Train Epoch: 795 [29%]
2023-12-31 08:31:44,661	44k	INFO	Losses: [2.5996594429016113, 2.162583112716675, 5.586745262145996, 19.16681480407715, 0.8866826891899109], step: 27800, lr: 9.033625129930478e-05, reference_loss: 30.402484893798828
2023-12-31 08:32:03,773	44k	INFO	====> Epoch: 795, cost 27.61 s
2023-12-31 08:32:30,251	44k	INFO	====> Epoch: 796, cost 26.48 s
2023-12-31 08:32:56,856	44k	INFO	====> Epoch: 797, cost 26.60 s
2023-12-31 08:33:23,415	44k	INFO	====> Epoch: 798, cost 26.56 s
2023-12-31 08:33:50,109	44k	INFO	====> Epoch: 799, cost 26.69 s
2023-12-31 08:34:16,698	44k	INFO	====> Epoch: 800, cost 26.59 s
2023-12-31 08:34:17,470	44k	INFO	Train Epoch: 801 [0%]
2023-12-31 08:34:17,470	44k	INFO	Losses: [2.391655683517456, 2.3859453201293945, 5.646207809448242, 18.661298751831055, 0.558521032333374], step: 28000, lr: 9.026852027986074e-05, reference_loss: 29.64362907409668
2023-12-31 08:34:24,633	44k	INFO	Saving model and optimizer state at iteration 801 to ./logs/44k/G_28000.pth
2023-12-31 08:34:26,098	44k	INFO	Saving model and optimizer state at iteration 801 to ./logs/44k/D_28000.pth
2023-12-31 08:34:26,876	44k	INFO	.. Free up space by deleting ckpt ./logs/44k/G_25600.pth
2023-12-31 08:34:26,933	44k	INFO	.. Free up space by deleting ckpt ./logs/44k/D_25600.pth
2023-12-31 08:34:52,587	44k	INFO	====> Epoch: 801, cost 35.89 s
2023-12-31 08:35:18,912	44k	INFO	====> Epoch: 802, cost 26.33 s
2023-12-31 08:35:45,258	44k	INFO	====> Epoch: 803, cost 26.35 s
2023-12-31 08:36:11,670	44k	INFO	====> Epoch: 804, cost 26.41 s
2023-12-31 08:36:38,292	44k	INFO	====> Epoch: 805, cost 26.62 s
2023-12-31 08:36:58,143	44k	INFO	Train Epoch: 806 [71%]
2023-12-31 08:36:58,143	44k	INFO	Losses: [2.5218911170959473, 2.11452317237854, 5.93723201751709, 18.136987686157227, 0.7937880754470825], step: 28200, lr: 9.021211655737914e-05, reference_loss: 29.504423141479492
2023-12-31 08:37:05,318	44k	INFO	====> Epoch: 806, cost 27.03 s
2023-12-31 08:37:32,153	44k	INFO	====> Epoch: 807, cost 26.84 s
2023-12-31 08:37:58,662	44k	INFO	====> Epoch: 808, cost 26.51 s
2023-12-31 08:38:25,269	44k	INFO	====> Epoch: 809, cost 26.61 s
2023-12-31 08:38:51,589	44k	INFO	====> Epoch: 810, cost 26.32 s
2023-12-31 08:39:18,147	44k	INFO	====> Epoch: 811, cost 26.56 s
2023-12-31 08:39:30,221	44k	INFO	Train Epoch: 812 [43%]
2023-12-31 08:39:30,221	44k	INFO	Losses: [2.345759391784668, 2.480844497680664, 5.549395561218262, 17.024595260620117, 0.7034380435943604], step: 28400, lr: 9.014447860990232e-05, reference_loss: 28.104032516479492
2023-12-31 08:39:44,961	44k	INFO	====> Epoch: 812, cost 26.81 s
2023-12-31 08:40:11,504	44k	INFO	====> Epoch: 813, cost 26.54 s
2023-12-31 08:40:38,149	44k	INFO	====> Epoch: 814, cost 26.64 s
2023-12-31 08:41:04,954	44k	INFO	====> Epoch: 815, cost 26.81 s
2023-12-31 08:41:31,594	44k	INFO	====> Epoch: 816, cost 26.64 s
2023-12-31 08:41:58,226	44k	INFO	====> Epoch: 817, cost 26.63 s
2023-12-31 08:42:02,889	44k	INFO	Train Epoch: 818 [14%]
2023-12-31 08:42:02,890	44k	INFO	Losses: [2.368894338607788, 2.2178807258605957, 5.823229789733887, 19.82215118408203, 0.9137821197509766], step: 28600, lr: 9.007689137503609e-05, reference_loss: 31.145936965942383
2023-12-31 08:42:25,371	44k	INFO	====> Epoch: 818, cost 27.14 s
2023-12-31 08:42:51,975	44k	INFO	====> Epoch: 819, cost 26.60 s
2023-12-31 08:43:18,348	44k	INFO	====> Epoch: 820, cost 26.37 s
2023-12-31 08:43:44,930	44k	INFO	====> Epoch: 821, cost 26.58 s
2023-12-31 08:44:11,645	44k	INFO	====> Epoch: 822, cost 26.72 s
2023-12-31 08:44:35,173	44k	INFO	Train Epoch: 823 [86%]
2023-12-31 08:44:35,174	44k	INFO	Losses: [2.5543019771575928, 2.1764211654663086, 5.5364990234375, 17.5284366607666, 0.5531796216964722], step: 28800, lr: 9.002060739068175e-05, reference_loss: 28.34883689880371
2023-12-31 08:44:42,307	44k	INFO	Saving model and optimizer state at iteration 823 to ./logs/44k/G_28800.pth
2023-12-31 08:44:43,486	44k	INFO	Saving model and optimizer state at iteration 823 to ./logs/44k/D_28800.pth
2023-12-31 08:44:44,259	44k	INFO	.. Free up space by deleting ckpt ./logs/44k/G_26400.pth
2023-12-31 08:44:44,315	44k	INFO	.. Free up space by deleting ckpt ./logs/44k/D_26400.pth
2023-12-31 08:44:47,336	44k	INFO	====> Epoch: 823, cost 35.69 s
2023-12-31 08:45:13,949	44k	INFO	====> Epoch: 824, cost 26.61 s
2023-12-31 08:45:40,418	44k	INFO	====> Epoch: 825, cost 26.47 s
2023-12-31 08:46:07,054	44k	INFO	====> Epoch: 826, cost 26.64 s
2023-12-31 08:46:33,830	44k	INFO	====> Epoch: 827, cost 26.78 s
2023-12-31 08:47:00,069	44k	INFO	====> Epoch: 828, cost 26.24 s
2023-12-31 08:47:15,928	44k	INFO	Train Epoch: 829 [57%]
2023-12-31 08:47:15,928	44k	INFO	Losses: [2.690016746520996, 1.9303572177886963, 4.65349006652832, 16.010211944580078, 0.7611927390098572], step: 29000, lr: 8.995311303020248e-05, reference_loss: 26.045269012451172
2023-12-31 08:47:27,054	44k	INFO	====> Epoch: 829, cost 26.98 s
2023-12-31 08:47:53,723	44k	INFO	====> Epoch: 830, cost 26.67 s
2023-12-31 08:48:20,244	44k	INFO	====> Epoch: 831, cost 26.52 s
2023-12-31 08:48:46,941	44k	INFO	====> Epoch: 832, cost 26.70 s
2023-12-31 08:49:13,460	44k	INFO	====> Epoch: 833, cost 26.52 s
2023-12-31 08:49:39,748	44k	INFO	====> Epoch: 834, cost 26.29 s
2023-12-31 08:49:48,007	44k	INFO	Train Epoch: 835 [29%]
2023-12-31 08:49:48,008	44k	INFO	Losses: [2.563284158706665, 2.2867887020111084, 4.6831464767456055, 17.9478759765625, 0.5815536975860596], step: 29200, lr: 8.98856692746772e-05, reference_loss: 28.06264877319336
2023-12-31 08:50:06,712	44k	INFO	====> Epoch: 835, cost 26.96 s
2023-12-31 08:50:33,262	44k	INFO	====> Epoch: 836, cost 26.55 s
2023-12-31 08:50:59,638	44k	INFO	====> Epoch: 837, cost 26.38 s
2023-12-31 08:51:26,305	44k	INFO	====> Epoch: 838, cost 26.67 s
2023-12-31 08:51:52,848	44k	INFO	====> Epoch: 839, cost 26.54 s
2023-12-31 08:52:19,277	44k	INFO	====> Epoch: 840, cost 26.43 s
2023-12-31 08:52:20,051	44k	INFO	Train Epoch: 841 [0%]
2023-12-31 08:52:20,052	44k	INFO	Losses: [2.511251449584961, 2.1200180053710938, 5.493348121643066, 16.08483123779297, 0.3673035800457001], step: 29400, lr: 8.981827608616408e-05, reference_loss: 26.576753616333008
2023-12-31 08:52:46,691	44k	INFO	====> Epoch: 841, cost 27.41 s
2023-12-31 08:53:13,120	44k	INFO	====> Epoch: 842, cost 26.43 s
2023-12-31 08:53:39,733	44k	INFO	====> Epoch: 843, cost 26.61 s
2023-12-31 08:54:06,322	44k	INFO	====> Epoch: 844, cost 26.59 s
2023-12-31 08:54:32,832	44k	INFO	====> Epoch: 845, cost 26.51 s
2023-12-31 08:54:52,443	44k	INFO	Train Epoch: 846 [71%]
2023-12-31 08:54:52,444	44k	INFO	Losses: [2.6735727787017822, 1.8748703002929688, 3.996142625808716, 15.738614082336426, 0.9067438244819641], step: 29600, lr: 8.976215369596169e-05, reference_loss: 25.189943313598633
2023-12-31 08:54:59,646	44k	INFO	Saving model and optimizer state at iteration 846 to ./logs/44k/G_29600.pth
2023-12-31 08:55:00,854	44k	INFO	Saving model and optimizer state at iteration 846 to ./logs/44k/D_29600.pth
2023-12-31 08:55:01,628	44k	INFO	.. Free up space by deleting ckpt ./logs/44k/G_27200.pth
2023-12-31 08:55:01,684	44k	INFO	.. Free up space by deleting ckpt ./logs/44k/D_27200.pth
2023-12-31 08:55:08,457	44k	INFO	====> Epoch: 846, cost 35.62 s
2023-12-31 08:55:35,102	44k	INFO	====> Epoch: 847, cost 26.65 s
2023-12-31 08:56:01,495	44k	INFO	====> Epoch: 848, cost 26.39 s
2023-12-31 08:56:28,109	44k	INFO	====> Epoch: 849, cost 26.61 s
2023-12-31 08:56:54,659	44k	INFO	====> Epoch: 850, cost 26.55 s
2023-12-31 08:57:21,072	44k	INFO	====> Epoch: 851, cost 26.41 s
2023-12-31 08:57:33,312	44k	INFO	Train Epoch: 852 [43%]
2023-12-31 08:57:33,312	44k	INFO	Losses: [2.5695888996124268, 2.180673599243164, 4.632673740386963, 17.006380081176758, 0.984893798828125], step: 29800, lr: 8.969485311518848e-05, reference_loss: 27.374210357666016
2023-12-31 08:57:48,263	44k	INFO	====> Epoch: 852, cost 27.19 s
2023-12-31 08:58:14,560	44k	INFO	====> Epoch: 853, cost 26.30 s
2023-12-31 08:58:41,184	44k	INFO	====> Epoch: 854, cost 26.62 s
2023-12-31 08:59:07,743	44k	INFO	====> Epoch: 855, cost 26.56 s
2023-12-31 08:59:34,547	44k	INFO	====> Epoch: 856, cost 26.80 s
2023-12-31 09:00:01,119	44k	INFO	====> Epoch: 857, cost 26.57 s
2023-12-31 09:00:05,690	44k	INFO	Train Epoch: 858 [14%]
2023-12-31 09:00:05,691	44k	INFO	Losses: [2.6247918605804443, 2.1940550804138184, 5.65757417678833, 19.55132484436035, 0.6733936071395874], step: 30000, lr: 8.962760299407988e-05, reference_loss: 30.701139450073242
2023-12-31 09:00:28,072	44k	INFO	====> Epoch: 858, cost 26.95 s
2023-12-31 09:00:54,548	44k	INFO	====> Epoch: 859, cost 26.48 s
2023-12-31 09:01:20,958	44k	INFO	====> Epoch: 860, cost 26.41 s
2023-12-31 09:01:47,195	44k	INFO	====> Epoch: 861, cost 26.24 s
2023-12-31 09:02:13,817	44k	INFO	====> Epoch: 862, cost 26.62 s
2023-12-31 09:02:37,629	44k	INFO	Train Epoch: 863 [86%]
2023-12-31 09:02:37,629	44k	INFO	Losses: [2.432610034942627, 2.389993667602539, 7.373623371124268, 17.902088165283203, 0.7104656100273132], step: 30200, lr: 8.957159974477111e-05, reference_loss: 30.80878257751465
2023-12-31 09:02:41,121	44k	INFO	====> Epoch: 863, cost 27.30 s
2023-12-31 09:03:07,692	44k	INFO	====> Epoch: 864, cost 26.57 s
2023-12-31 09:03:34,157	44k	INFO	====> Epoch: 865, cost 26.46 s
2023-12-31 09:04:00,824	44k	INFO	====> Epoch: 866, cost 26.67 s
2023-12-31 09:04:27,406	44k	INFO	====> Epoch: 867, cost 26.58 s
2023-12-31 09:04:54,042	44k	INFO	====> Epoch: 868, cost 26.64 s
2023-12-31 09:05:09,991	44k	INFO	Train Epoch: 869 [57%]
2023-12-31 09:05:09,991	44k	INFO	Losses: [2.3049306869506836, 2.4337756633758545, 6.821710109710693, 18.860933303833008, 0.6902307271957397], step: 30400, lr: 8.950444203480763e-05, reference_loss: 31.111581802368164
2023-12-31 09:05:17,474	44k	INFO	Saving model and optimizer state at iteration 869 to ./logs/44k/G_30400.pth
2023-12-31 09:05:18,657	44k	INFO	Saving model and optimizer state at iteration 869 to ./logs/44k/D_30400.pth
2023-12-31 09:05:19,432	44k	INFO	.. Free up space by deleting ckpt ./logs/44k/G_28000.pth
2023-12-31 09:05:19,489	44k	INFO	.. Free up space by deleting ckpt ./logs/44k/D_28000.pth
2023-12-31 09:05:29,996	44k	INFO	====> Epoch: 869, cost 35.95 s
2023-12-31 09:05:56,313	44k	INFO	====> Epoch: 870, cost 26.32 s
2023-12-31 09:06:22,793	44k	INFO	====> Epoch: 871, cost 26.48 s
2023-12-31 09:06:49,395	44k	INFO	====> Epoch: 872, cost 26.60 s
2023-12-31 09:07:15,931	44k	INFO	====> Epoch: 873, cost 26.54 s
2023-12-31 09:07:42,624	44k	INFO	====> Epoch: 874, cost 26.69 s
2023-12-31 09:07:51,100	44k	INFO	Train Epoch: 875 [29%]
2023-12-31 09:07:51,101	44k	INFO	Losses: [2.406433582305908, 2.3141531944274902, 5.820498943328857, 17.68812370300293, 0.6218425035476685], step: 30600, lr: 8.943733467738917e-05, reference_loss: 28.85105323791504
2023-12-31 09:08:09,904	44k	INFO	====> Epoch: 875, cost 27.28 s
2023-12-31 09:08:36,532	44k	INFO	====> Epoch: 876, cost 26.63 s
2023-12-31 09:09:03,141	44k	INFO	====> Epoch: 877, cost 26.61 s
2023-12-31 09:09:29,835	44k	INFO	====> Epoch: 878, cost 26.69 s
2023-12-31 09:09:56,434	44k	INFO	====> Epoch: 879, cost 26.60 s
2023-12-31 09:10:23,000	44k	INFO	====> Epoch: 880, cost 26.57 s
2023-12-31 09:10:23,774	44k	INFO	Train Epoch: 881 [0%]
2023-12-31 09:10:23,775	44k	INFO	Losses: [2.7019848823547363, 2.064608335494995, 4.747188568115234, 15.816417694091797, 0.819520890712738], step: 30800, lr: 8.93702776347631e-05, reference_loss: 26.149721145629883
2023-12-31 09:10:50,292	44k	INFO	====> Epoch: 881, cost 27.29 s
2023-12-31 09:11:16,975	44k	INFO	====> Epoch: 882, cost 26.68 s
2023-12-31 09:11:43,551	44k	INFO	====> Epoch: 883, cost 26.58 s
2023-12-31 09:12:10,291	44k	INFO	====> Epoch: 884, cost 26.74 s
2023-12-31 09:12:36,748	44k	INFO	====> Epoch: 885, cost 26.46 s
2023-12-31 09:12:56,509	44k	INFO	Train Epoch: 886 [71%]
2023-12-31 09:12:56,509	44k	INFO	Losses: [2.4704809188842773, 2.771623134613037, 7.81687593460083, 20.288490295410156, 0.9706034660339355], step: 31000, lr: 8.931443517360183e-05, reference_loss: 34.31807327270508
2023-12-31 09:13:03,760	44k	INFO	====> Epoch: 886, cost 27.01 s
2023-12-31 09:13:30,314	44k	INFO	====> Epoch: 887, cost 26.55 s
2023-12-31 09:13:57,001	44k	INFO	====> Epoch: 888, cost 26.69 s
2023-12-31 09:14:23,809	44k	INFO	====> Epoch: 889, cost 26.81 s
2023-12-31 09:14:50,342	44k	INFO	====> Epoch: 890, cost 26.53 s
2023-12-31 09:15:16,783	44k	INFO	====> Epoch: 891, cost 26.44 s
2023-12-31 09:15:28,923	44k	INFO	Train Epoch: 892 [43%]
2023-12-31 09:15:28,924	44k	INFO	Losses: [2.4958858489990234, 2.315330743789673, 5.924323081970215, 17.478622436523438, 0.7960568070411682], step: 31200, lr: 8.924747027680382e-05, reference_loss: 29.010217666625977
2023-12-31 09:15:36,035	44k	INFO	Saving model and optimizer state at iteration 892 to ./logs/44k/G_31200.pth
2023-12-31 09:15:37,202	44k	INFO	Saving model and optimizer state at iteration 892 to ./logs/44k/D_31200.pth
2023-12-31 09:15:37,978	44k	INFO	.. Free up space by deleting ckpt ./logs/44k/G_28800.pth
2023-12-31 09:15:38,035	44k	INFO	.. Free up space by deleting ckpt ./logs/44k/D_28800.pth
2023-12-31 09:15:52,488	44k	INFO	====> Epoch: 892, cost 35.70 s
2023-12-31 09:16:19,134	44k	INFO	====> Epoch: 893, cost 26.65 s
2023-12-31 09:16:46,089	44k	INFO	====> Epoch: 894, cost 26.95 s
2023-12-31 09:17:12,734	44k	INFO	====> Epoch: 895, cost 26.64 s
2023-12-31 09:17:39,188	44k	INFO	====> Epoch: 896, cost 26.45 s
2023-12-31 09:18:05,735	44k	INFO	====> Epoch: 897, cost 26.55 s
2023-12-31 09:18:10,334	44k	INFO	Train Epoch: 898 [14%]
2023-12-31 09:18:10,335	44k	INFO	Losses: [2.4835479259490967, 2.162680149078369, 4.777454853057861, 18.868938446044922, 0.812382698059082], step: 31400, lr: 8.918055558798614e-05, reference_loss: 29.105003356933594
2023-12-31 09:18:32,685	44k	INFO	====> Epoch: 898, cost 26.95 s
2023-12-31 09:18:59,057	44k	INFO	====> Epoch: 899, cost 26.37 s
2023-12-31 09:19:25,647	44k	INFO	====> Epoch: 900, cost 26.59 s
2023-12-31 09:19:52,183	44k	INFO	====> Epoch: 901, cost 26.54 s
2023-12-31 09:20:18,631	44k	INFO	====> Epoch: 902, cost 26.45 s
2023-12-31 09:20:42,493	44k	INFO	Train Epoch: 903 [86%]
2023-12-31 09:20:42,494	44k	INFO	Losses: [2.2286925315856934, 2.3310306072235107, 8.372627258300781, 20.83437728881836, 0.9921059608459473], step: 31600, lr: 8.912483167346374e-05, reference_loss: 34.75883483886719
2023-12-31 09:20:46,119	44k	INFO	====> Epoch: 903, cost 27.49 s
2023-12-31 09:21:12,524	44k	INFO	====> Epoch: 904, cost 26.41 s
2023-12-31 09:21:38,895	44k	INFO	====> Epoch: 905, cost 26.37 s
2023-12-31 09:22:05,304	44k	INFO	====> Epoch: 906, cost 26.41 s
2023-12-31 09:22:31,809	44k	INFO	====> Epoch: 907, cost 26.51 s
2023-12-31 09:22:58,354	44k	INFO	====> Epoch: 908, cost 26.55 s
2023-12-31 09:23:14,311	44k	INFO	Train Epoch: 909 [57%]
2023-12-31 09:23:14,311	44k	INFO	Losses: [2.820068597793579, 2.1434762477874756, 4.263114929199219, 15.852729797363281, 0.6629238724708557], step: 31800, lr: 8.90580089348599e-05, reference_loss: 25.742313385009766
2023-12-31 09:23:25,728	44k	INFO	====> Epoch: 909, cost 27.37 s
2023-12-31 09:23:52,402	44k	INFO	====> Epoch: 910, cost 26.67 s
2023-12-31 09:24:19,056	44k	INFO	====> Epoch: 911, cost 26.65 s
2023-12-31 09:24:45,551	44k	INFO	====> Epoch: 912, cost 26.50 s
2023-12-31 09:25:11,974	44k	INFO	====> Epoch: 913, cost 26.42 s
2023-12-31 09:25:38,652	44k	INFO	====> Epoch: 914, cost 26.68 s
2023-12-31 09:25:47,079	44k	INFO	Train Epoch: 915 [29%]
2023-12-31 09:25:47,079	44k	INFO	Losses: [2.4461655616760254, 2.16111421585083, 5.896726131439209, 18.86615753173828, 0.9235544204711914], step: 32000, lr: 8.899123629765109e-05, reference_loss: 30.293716430664062
2023-12-31 09:25:54,263	44k	INFO	Saving model and optimizer state at iteration 915 to ./logs/44k/G_32000.pth
2023-12-31 09:25:55,762	44k	INFO	Saving model and optimizer state at iteration 915 to ./logs/44k/D_32000.pth
2023-12-31 09:25:56,543	44k	INFO	.. Free up space by deleting ckpt ./logs/44k/G_29600.pth
2023-12-31 09:25:56,599	44k	INFO	.. Free up space by deleting ckpt ./logs/44k/D_29600.pth
2023-12-31 09:26:14,683	44k	INFO	====> Epoch: 915, cost 36.03 s
2023-12-31 09:26:41,150	44k	INFO	====> Epoch: 916, cost 26.47 s
2023-12-31 09:27:07,734	44k	INFO	====> Epoch: 917, cost 26.58 s
2023-12-31 09:27:34,252	44k	INFO	====> Epoch: 918, cost 26.52 s
2023-12-31 09:28:00,635	44k	INFO	====> Epoch: 919, cost 26.38 s
2023-12-31 09:28:27,254	44k	INFO	====> Epoch: 920, cost 26.62 s
2023-12-31 09:28:28,029	44k	INFO	Train Epoch: 921 [0%]
2023-12-31 09:28:28,030	44k	INFO	Losses: [2.538731336593628, 2.272268533706665, 7.034689903259277, 19.280895233154297, 0.769457221031189], step: 32200, lr: 8.892451372427295e-05, reference_loss: 31.896041870117188
2023-12-31 09:28:54,378	44k	INFO	====> Epoch: 921, cost 27.12 s
2023-12-31 09:29:21,291	44k	INFO	====> Epoch: 922, cost 26.91 s
2023-12-31 09:29:47,821	44k	INFO	====> Epoch: 923, cost 26.53 s
2023-12-31 09:30:14,381	44k	INFO	====> Epoch: 924, cost 26.56 s
2023-12-31 09:30:40,801	44k	INFO	====> Epoch: 925, cost 26.42 s
2023-12-31 09:31:00,364	44k	INFO	Train Epoch: 926 [71%]
2023-12-31 09:31:00,365	44k	INFO	Losses: [2.537776231765747, 2.212744951248169, 5.7732625007629395, 18.043506622314453, 0.6977242231369019], step: 32400, lr: 8.886894979591382e-05, reference_loss: 29.2650146484375
2023-12-31 09:31:07,539	44k	INFO	====> Epoch: 926, cost 26.74 s
2023-12-31 09:31:33,987	44k	INFO	====> Epoch: 927, cost 26.45 s
2023-12-31 09:32:00,704	44k	INFO	====> Epoch: 928, cost 26.72 s
2023-12-31 09:32:27,443	44k	INFO	====> Epoch: 929, cost 26.74 s
2023-12-31 09:32:53,874	44k	INFO	====> Epoch: 930, cost 26.43 s
2023-12-31 09:33:20,476	44k	INFO	====> Epoch: 931, cost 26.60 s
2023-12-31 09:33:32,598	44k	INFO	Train Epoch: 932 [43%]
2023-12-31 09:33:32,599	44k	INFO	Losses: [2.4311769008636475, 2.3917315006256104, 6.5552167892456055, 17.48482322692871, 0.8063991665840149], step: 32600, lr: 8.880231890875586e-05, reference_loss: 29.669347763061523
2023-12-31 09:33:47,645	44k	INFO	====> Epoch: 932, cost 27.17 s
2023-12-31 09:34:14,266	44k	INFO	====> Epoch: 933, cost 26.62 s
2023-12-31 09:34:40,820	44k	INFO	====> Epoch: 934, cost 26.55 s
2023-12-31 09:35:07,360	44k	INFO	====> Epoch: 935, cost 26.54 s
2023-12-31 09:35:34,074	44k	INFO	====> Epoch: 936, cost 26.71 s
2023-12-31 09:36:00,780	44k	INFO	====> Epoch: 937, cost 26.71 s
2023-12-31 09:36:05,399	44k	INFO	Train Epoch: 938 [14%]
2023-12-31 09:36:05,400	44k	INFO	Losses: [2.3728513717651367, 2.1453945636749268, 5.847026348114014, 19.558834075927734, 0.6367416977882385], step: 32800, lr: 8.873573797914925e-05, reference_loss: 30.560848236083984
2023-12-31 09:36:12,663	44k	INFO	Saving model and optimizer state at iteration 938 to ./logs/44k/G_32800.pth
2023-12-31 09:36:13,828	44k	INFO	Saving model and optimizer state at iteration 938 to ./logs/44k/D_32800.pth
2023-12-31 09:36:14,620	44k	INFO	.. Free up space by deleting ckpt ./logs/44k/G_30400.pth
2023-12-31 09:36:14,676	44k	INFO	.. Free up space by deleting ckpt ./logs/44k/D_30400.pth
2023-12-31 09:36:36,613	44k	INFO	====> Epoch: 938, cost 35.83 s
2023-12-31 09:37:03,217	44k	INFO	====> Epoch: 939, cost 26.60 s
2023-12-31 09:37:29,587	44k	INFO	====> Epoch: 940, cost 26.37 s
2023-12-31 09:37:56,023	44k	INFO	====> Epoch: 941, cost 26.44 s
2023-12-31 09:38:23,002	44k	INFO	====> Epoch: 942, cost 26.98 s
2023-12-31 09:38:46,495	44k	INFO	Train Epoch: 943 [86%]
2023-12-31 09:38:46,495	44k	INFO	Losses: [2.5331497192382812, 2.3687708377838135, 7.339625358581543, 18.08348846435547, 0.654914140701294], step: 33000, lr: 8.868029200613832e-05, reference_loss: 30.979949951171875
2023-12-31 09:38:50,096	44k	INFO	====> Epoch: 943, cost 27.09 s
2023-12-31 09:39:16,578	44k	INFO	====> Epoch: 944, cost 26.48 s
2023-12-31 09:39:43,240	44k	INFO	====> Epoch: 945, cost 26.66 s
2023-12-31 09:40:09,861	44k	INFO	====> Epoch: 946, cost 26.62 s
2023-12-31 09:40:36,468	44k	INFO	====> Epoch: 947, cost 26.61 s
2023-12-31 09:41:02,907	44k	INFO	====> Epoch: 948, cost 26.44 s
2023-12-31 09:41:18,866	44k	INFO	Train Epoch: 949 [57%]
2023-12-31 09:41:18,866	44k	INFO	Losses: [2.5284125804901123, 2.153019428253174, 5.268339157104492, 17.93828773498535, 0.6732190251350403], step: 33200, lr: 8.861380256811337e-05, reference_loss: 28.561277389526367
2023-12-31 09:41:30,157	44k	INFO	====> Epoch: 949, cost 27.25 s
2023-12-31 09:41:56,672	44k	INFO	====> Epoch: 950, cost 26.52 s
2023-12-31 09:42:23,149	44k	INFO	====> Epoch: 951, cost 26.48 s
2023-12-31 09:42:49,637	44k	INFO	====> Epoch: 952, cost 26.49 s
2023-12-31 09:43:16,181	44k	INFO	====> Epoch: 953, cost 26.54 s
2023-12-31 09:43:42,874	44k	INFO	====> Epoch: 954, cost 26.69 s
2023-12-31 09:43:51,327	44k	INFO	Train Epoch: 955 [29%]
2023-12-31 09:43:51,328	44k	INFO	Losses: [2.675982713699341, 2.1502931118011475, 5.459482669830322, 18.554004669189453, 0.8541486263275146], step: 33400, lr: 8.854736298158609e-05, reference_loss: 29.693912506103516
2023-12-31 09:44:10,298	44k	INFO	====> Epoch: 955, cost 27.42 s
2023-12-31 09:44:36,689	44k	INFO	====> Epoch: 956, cost 26.39 s
2023-12-31 09:45:03,051	44k	INFO	====> Epoch: 957, cost 26.36 s
2023-12-31 09:45:29,485	44k	INFO	====> Epoch: 958, cost 26.43 s
2023-12-31 09:45:56,074	44k	INFO	====> Epoch: 959, cost 26.59 s
2023-12-31 09:46:22,444	44k	INFO	====> Epoch: 960, cost 26.37 s
2023-12-31 09:46:23,204	44k	INFO	Train Epoch: 961 [0%]
2023-12-31 09:46:23,205	44k	INFO	Losses: [2.5259530544281006, 2.309079170227051, 6.032222270965576, 18.587646484375, 0.6455173492431641], step: 33600, lr: 8.848097320917952e-05, reference_loss: 30.100418090820312
2023-12-31 09:46:30,320	44k	INFO	Saving model and optimizer state at iteration 961 to ./logs/44k/G_33600.pth
2023-12-31 09:46:31,516	44k	INFO	Saving model and optimizer state at iteration 961 to ./logs/44k/D_33600.pth
2023-12-31 09:46:32,295	44k	INFO	.. Free up space by deleting ckpt ./logs/44k/G_31200.pth
2023-12-31 09:46:32,351	44k	INFO	.. Free up space by deleting ckpt ./logs/44k/D_31200.pth
2023-12-31 09:46:58,276	44k	INFO	====> Epoch: 961, cost 35.83 s
2023-12-31 09:47:24,992	44k	INFO	====> Epoch: 962, cost 26.72 s
2023-12-31 09:47:51,549	44k	INFO	====> Epoch: 963, cost 26.56 s
2023-12-31 09:48:17,996	44k	INFO	====> Epoch: 964, cost 26.45 s
2023-12-31 09:48:44,293	44k	INFO	====> Epoch: 965, cost 26.30 s
2023-12-31 09:49:03,869	44k	INFO	Train Epoch: 966 [71%]
2023-12-31 09:49:03,869	44k	INFO	Losses: [2.484476327896118, 2.168121814727783, 7.110320568084717, 18.69659423828125, 0.6584028601646423], step: 33800, lr: 8.842568642434779e-05, reference_loss: 31.117916107177734
2023-12-31 09:49:11,089	44k	INFO	====> Epoch: 966, cost 26.80 s
2023-12-31 09:49:37,614	44k	INFO	====> Epoch: 967, cost 26.53 s
2023-12-31 09:50:04,315	44k	INFO	====> Epoch: 968, cost 26.70 s
2023-12-31 09:50:30,853	44k	INFO	====> Epoch: 969, cost 26.54 s
2023-12-31 09:50:57,728	44k	INFO	====> Epoch: 970, cost 26.88 s
2023-12-31 09:51:24,258	44k	INFO	====> Epoch: 971, cost 26.53 s
2023-12-31 09:51:36,324	44k	INFO	Train Epoch: 972 [43%]
2023-12-31 09:51:36,325	44k	INFO	Losses: [2.5347704887390137, 2.206714630126953, 5.51162052154541, 15.914459228515625, 0.6842943429946899], step: 34000, lr: 8.835938788084596e-05, reference_loss: 26.851858139038086
2023-12-31 09:51:51,233	44k	INFO	====> Epoch: 972, cost 26.97 s
2023-12-31 09:52:17,778	44k	INFO	====> Epoch: 973, cost 26.55 s
2023-12-31 09:52:44,407	44k	INFO	====> Epoch: 974, cost 26.63 s
2023-12-31 09:53:10,986	44k	INFO	====> Epoch: 975, cost 26.58 s
2023-12-31 09:53:37,520	44k	INFO	====> Epoch: 976, cost 26.53 s
2023-12-31 09:54:04,221	44k	INFO	====> Epoch: 977, cost 26.70 s
2023-12-31 09:54:08,856	44k	INFO	Train Epoch: 978 [14%]
2023-12-31 09:54:08,856	44k	INFO	Losses: [2.6155428886413574, 2.1231632232666016, 5.215024471282959, 18.293838500976562, 0.884378969669342], step: 34200, lr: 8.829313904571562e-05, reference_loss: 29.131948471069336
2023-12-31 09:54:31,375	44k	INFO	====> Epoch: 978, cost 27.15 s
2023-12-31 09:54:57,779	44k	INFO	====> Epoch: 979, cost 26.40 s
2023-12-31 09:55:24,206	44k	INFO	====> Epoch: 980, cost 26.43 s
2023-12-31 09:55:50,865	44k	INFO	====> Epoch: 981, cost 26.66 s
2023-12-31 09:56:17,507	44k	INFO	====> Epoch: 982, cost 26.64 s
2023-12-31 09:56:40,977	44k	INFO	Train Epoch: 983 [86%]
2023-12-31 09:56:40,978	44k	INFO	Losses: [2.55385160446167, 2.380859851837158, 5.944241046905518, 17.608978271484375, 0.47860202193260193], step: 34400, lr: 8.823796962789062e-05, reference_loss: 28.96653175354004
2023-12-31 09:56:48,587	44k	INFO	Saving model and optimizer state at iteration 983 to ./logs/44k/G_34400.pth
2023-12-31 09:56:49,784	44k	INFO	Saving model and optimizer state at iteration 983 to ./logs/44k/D_34400.pth
2023-12-31 09:56:50,564	44k	INFO	.. Free up space by deleting ckpt ./logs/44k/G_32000.pth
2023-12-31 09:56:50,620	44k	INFO	.. Free up space by deleting ckpt ./logs/44k/D_32000.pth
2023-12-31 09:56:53,636	44k	INFO	====> Epoch: 983, cost 36.13 s
2023-12-31 09:57:20,066	44k	INFO	====> Epoch: 984, cost 26.43 s
2023-12-31 09:57:46,629	44k	INFO	====> Epoch: 985, cost 26.56 s
2023-12-31 09:58:13,121	44k	INFO	====> Epoch: 986, cost 26.49 s
2023-12-31 09:58:39,694	44k	INFO	====> Epoch: 987, cost 26.57 s
2023-12-31 09:59:06,104	44k	INFO	====> Epoch: 988, cost 26.41 s
2023-12-31 09:59:21,917	44k	INFO	Train Epoch: 989 [57%]
2023-12-31 09:59:21,917	44k	INFO	Losses: [2.6329474449157715, 1.9451904296875, 4.446025371551514, 15.371552467346191, 0.7736061811447144], step: 34600, lr: 8.817181182799734e-05, reference_loss: 25.169321060180664
2023-12-31 09:59:33,190	44k	INFO	====> Epoch: 989, cost 27.09 s
2023-12-31 09:59:59,616	44k	INFO	====> Epoch: 990, cost 26.43 s
2023-12-31 10:00:26,094	44k	INFO	====> Epoch: 991, cost 26.48 s
2023-12-31 10:00:52,539	44k	INFO	====> Epoch: 992, cost 26.44 s
2023-12-31 10:01:19,121	44k	INFO	====> Epoch: 993, cost 26.58 s
2023-12-31 10:01:45,635	44k	INFO	====> Epoch: 994, cost 26.51 s
2023-12-31 10:01:53,994	44k	INFO	Train Epoch: 995 [29%]
2023-12-31 10:01:53,995	44k	INFO	Losses: [2.647662401199341, 2.1504533290863037, 5.22490119934082, 17.06780242919922, 0.46742066740989685], step: 34800, lr: 8.810570363095084e-05, reference_loss: 27.55824089050293
2023-12-31 10:02:12,289	44k	INFO	====> Epoch: 995, cost 26.65 s
2023-12-31 10:02:39,128	44k	INFO	====> Epoch: 996, cost 26.84 s
2023-12-31 10:03:05,698	44k	INFO	====> Epoch: 997, cost 26.57 s
2023-12-31 10:03:32,253	44k	INFO	====> Epoch: 998, cost 26.56 s
2023-12-31 10:03:58,655	44k	INFO	====> Epoch: 999, cost 26.40 s
2023-12-31 10:04:25,181	44k	INFO	====> Epoch: 1000, cost 26.53 s
2023-12-31 10:04:25,957	44k	INFO	Train Epoch: 1001 [0%]
2023-12-31 10:04:25,958	44k	INFO	Losses: [2.540109395980835, 2.001952886581421, 5.705558776855469, 15.786401748657227, 0.3607601523399353], step: 35000, lr: 8.803964499956059e-05, reference_loss: 26.39478302001953
2023-12-31 10:04:52,268	44k	INFO	====> Epoch: 1001, cost 27.09 s
2023-12-31 10:05:18,853	44k	INFO	====> Epoch: 1002, cost 26.58 s
2023-12-31 10:05:45,864	44k	INFO	====> Epoch: 1003, cost 27.01 s
2023-12-31 10:06:12,412	44k	INFO	====> Epoch: 1004, cost 26.55 s
2023-12-31 10:06:39,060	44k	INFO	====> Epoch: 1005, cost 26.65 s
2023-12-31 10:06:58,705	44k	INFO	Train Epoch: 1006 [71%]
2023-12-31 10:06:58,705	44k	INFO	Losses: [2.6900017261505127, 2.091838836669922, 4.4398956298828125, 15.348422050476074, 0.8187547326087952], step: 35200, lr: 8.798463397591094e-05, reference_loss: 25.388912200927734
2023-12-31 10:07:05,900	44k	INFO	Saving model and optimizer state at iteration 1006 to ./logs/44k/G_35200.pth
2023-12-31 10:07:07,085	44k	INFO	Saving model and optimizer state at iteration 1006 to ./logs/44k/D_35200.pth
2023-12-31 10:07:07,854	44k	INFO	.. Free up space by deleting ckpt ./logs/44k/G_32800.pth
2023-12-31 10:07:07,910	44k	INFO	.. Free up space by deleting ckpt ./logs/44k/D_32800.pth
2023-12-31 10:07:14,686	44k	INFO	====> Epoch: 1006, cost 35.63 s
2023-12-31 10:07:41,111	44k	INFO	====> Epoch: 1007, cost 26.43 s
2023-12-31 10:08:07,640	44k	INFO	====> Epoch: 1008, cost 26.53 s
2023-12-31 10:08:34,472	44k	INFO	====> Epoch: 1009, cost 26.83 s
2023-12-31 10:09:01,051	44k	INFO	====> Epoch: 1010, cost 26.58 s
2023-12-31 10:09:27,725	44k	INFO	====> Epoch: 1011, cost 26.67 s
2023-12-31 10:09:39,922	44k	INFO	Train Epoch: 1012 [43%]
2023-12-31 10:09:39,922	44k	INFO	Losses: [2.6002116203308105, 2.0489954948425293, 5.067268371582031, 16.0057315826416, 0.867317259311676], step: 35400, lr: 8.7918666118391e-05, reference_loss: 26.589523315429688
2023-12-31 10:09:54,739	44k	INFO	====> Epoch: 1012, cost 27.01 s
2023-12-31 10:10:21,309	44k	INFO	====> Epoch: 1013, cost 26.57 s
2023-12-31 10:10:47,866	44k	INFO	====> Epoch: 1014, cost 26.56 s
2023-12-31 10:11:14,447	44k	INFO	====> Epoch: 1015, cost 26.58 s
2023-12-31 10:11:40,879	44k	INFO	====> Epoch: 1016, cost 26.43 s
2023-12-31 10:12:07,690	44k	INFO	====> Epoch: 1017, cost 26.81 s
2023-12-31 10:12:12,295	44k	INFO	Train Epoch: 1018 [14%]
2023-12-31 10:12:12,296	44k	INFO	Losses: [2.611675262451172, 2.0145182609558105, 5.9770331382751465, 20.230331420898438, 0.7693006992340088], step: 35600, lr: 8.785274772130558e-05, reference_loss: 31.60285758972168
2023-12-31 10:12:34,785	44k	INFO	====> Epoch: 1018, cost 27.10 s
2023-12-31 10:13:01,402	44k	INFO	====> Epoch: 1019, cost 26.62 s
2023-12-31 10:13:27,894	44k	INFO	====> Epoch: 1020, cost 26.49 s
2023-12-31 10:13:54,439	44k	INFO	====> Epoch: 1021, cost 26.55 s
2023-12-31 10:14:20,993	44k	INFO	====> Epoch: 1022, cost 26.55 s
2023-12-31 10:14:44,461	44k	INFO	Train Epoch: 1023 [86%]
2023-12-31 10:14:44,461	44k	INFO	Losses: [2.4665749073028564, 2.3239002227783203, 7.473958969116211, 17.209625244140625, 0.7091199159622192], step: 35800, lr: 8.779785347925579e-05, reference_loss: 30.18317985534668
2023-12-31 10:14:48,257	44k	INFO	====> Epoch: 1023, cost 27.26 s
2023-12-31 10:15:14,823	44k	INFO	====> Epoch: 1024, cost 26.57 s
2023-12-31 10:15:41,453	44k	INFO	====> Epoch: 1025, cost 26.63 s
2023-12-31 10:16:08,038	44k	INFO	====> Epoch: 1026, cost 26.59 s
2023-12-31 10:16:34,616	44k	INFO	====> Epoch: 1027, cost 26.58 s
2023-12-31 10:17:01,134	44k	INFO	====> Epoch: 1028, cost 26.52 s
2023-12-31 10:17:17,138	44k	INFO	Train Epoch: 1029 [57%]
2023-12-31 10:17:17,139	44k	INFO	Losses: [2.3091001510620117, 2.303565263748169, 7.992535591125488, 19.677942276000977, 0.6212170720100403], step: 36000, lr: 8.773202566333896e-05, reference_loss: 32.90435791015625
2023-12-31 10:17:24,323	44k	INFO	Saving model and optimizer state at iteration 1029 to ./logs/44k/G_36000.pth
2023-12-31 10:17:25,826	44k	INFO	Saving model and optimizer state at iteration 1029 to ./logs/44k/D_36000.pth
2023-12-31 10:17:26,597	44k	INFO	.. Free up space by deleting ckpt ./logs/44k/G_33600.pth
2023-12-31 10:17:26,654	44k	INFO	.. Free up space by deleting ckpt ./logs/44k/D_33600.pth
2023-12-31 10:17:37,200	44k	INFO	====> Epoch: 1029, cost 36.07 s
2023-12-31 10:18:03,672	44k	INFO	====> Epoch: 1030, cost 26.47 s
2023-12-31 10:18:30,020	44k	INFO	====> Epoch: 1031, cost 26.35 s
2023-12-31 10:18:56,369	44k	INFO	====> Epoch: 1032, cost 26.35 s
2023-12-31 10:19:22,965	44k	INFO	====> Epoch: 1033, cost 26.60 s
2023-12-31 10:19:49,564	44k	INFO	====> Epoch: 1034, cost 26.60 s
2023-12-31 10:19:58,049	44k	INFO	Train Epoch: 1035 [29%]
2023-12-31 10:19:58,050	44k	INFO	Losses: [2.621793031692505, 2.042896270751953, 5.569535255432129, 16.869441986083984, 0.6750932335853577], step: 36200, lr: 8.766624720285824e-05, reference_loss: 27.778759002685547
2023-12-31 10:20:16,711	44k	INFO	====> Epoch: 1035, cost 27.15 s
2023-12-31 10:20:43,498	44k	INFO	====> Epoch: 1036, cost 26.79 s
2023-12-31 10:21:10,099	44k	INFO	====> Epoch: 1037, cost 26.60 s
2023-12-31 10:21:36,662	44k	INFO	====> Epoch: 1038, cost 26.56 s
2023-12-31 10:22:02,819	44k	INFO	====> Epoch: 1039, cost 26.16 s
2023-12-31 10:22:29,218	44k	INFO	====> Epoch: 1040, cost 26.40 s
2023-12-31 10:22:29,993	44k	INFO	Train Epoch: 1041 [0%]
2023-12-31 10:22:29,993	44k	INFO	Losses: [2.5711755752563477, 2.2282052040100098, 5.636524677276611, 16.911258697509766, 0.64871746301651], step: 36400, lr: 8.760051806080861e-05, reference_loss: 27.995882034301758
2023-12-31 10:22:56,199	44k	INFO	====> Epoch: 1041, cost 26.98 s
2023-12-31 10:23:22,565	44k	INFO	====> Epoch: 1042, cost 26.37 s
2023-12-31 10:23:49,289	44k	INFO	====> Epoch: 1043, cost 26.72 s
2023-12-31 10:24:15,817	44k	INFO	====> Epoch: 1044, cost 26.53 s
2023-12-31 10:24:42,322	44k	INFO	====> Epoch: 1045, cost 26.51 s
2023-12-31 10:25:02,037	44k	INFO	Train Epoch: 1046 [71%]
2023-12-31 10:25:02,038	44k	INFO	Losses: [2.365726947784424, 2.3216845989227295, 7.468996047973633, 19.318571090698242, 0.8592994809150696], step: 36600, lr: 8.75457814228907e-05, reference_loss: 32.33427810668945
2023-12-31 10:25:09,196	44k	INFO	====> Epoch: 1046, cost 26.87 s
2023-12-31 10:25:35,771	44k	INFO	====> Epoch: 1047, cost 26.57 s
2023-12-31 10:26:02,267	44k	INFO	====> Epoch: 1048, cost 26.50 s
2023-12-31 10:26:28,860	44k	INFO	====> Epoch: 1049, cost 26.59 s
2023-12-31 10:26:55,820	44k	INFO	====> Epoch: 1050, cost 26.96 s
2023-12-31 10:27:22,475	44k	INFO	====> Epoch: 1051, cost 26.66 s
2023-12-31 10:27:34,677	44k	INFO	Train Epoch: 1052 [43%]
2023-12-31 10:27:34,678	44k	INFO	Losses: [2.645120143890381, 2.1457393169403076, 6.026473045349121, 17.286949157714844, 0.5539108514785767], step: 36800, lr: 8.748014260194659e-05, reference_loss: 28.658193588256836
2023-12-31 10:27:41,909	44k	INFO	Saving model and optimizer state at iteration 1052 to ./logs/44k/G_36800.pth
2023-12-31 10:27:43,085	44k	INFO	Saving model and optimizer state at iteration 1052 to ./logs/44k/D_36800.pth
2023-12-31 10:27:43,856	44k	INFO	.. Free up space by deleting ckpt ./logs/44k/G_34400.pth
2023-12-31 10:27:43,913	44k	INFO	.. Free up space by deleting ckpt ./logs/44k/D_34400.pth
2023-12-31 10:27:58,376	44k	INFO	====> Epoch: 1052, cost 35.90 s
2023-12-31 10:28:24,764	44k	INFO	====> Epoch: 1053, cost 26.39 s
2023-12-31 10:28:51,189	44k	INFO	====> Epoch: 1054, cost 26.43 s
2023-12-31 10:29:17,753	44k	INFO	====> Epoch: 1055, cost 26.56 s
2023-12-31 10:29:44,551	44k	INFO	====> Epoch: 1056, cost 26.80 s
2023-12-31 10:30:10,933	44k	INFO	====> Epoch: 1057, cost 26.38 s
2023-12-31 10:30:15,533	44k	INFO	Train Epoch: 1058 [14%]
2023-12-31 10:30:15,533	44k	INFO	Losses: [2.435133218765259, 2.053701639175415, 5.072443008422852, 19.596921920776367, 0.818565845489502], step: 37000, lr: 8.741455299473667e-05, reference_loss: 29.976764678955078
2023-12-31 10:30:38,078	44k	INFO	====> Epoch: 1058, cost 27.15 s
2023-12-31 10:31:04,594	44k	INFO	====> Epoch: 1059, cost 26.52 s
2023-12-31 10:31:31,145	44k	INFO	====> Epoch: 1060, cost 26.55 s
2023-12-31 10:31:57,393	44k	INFO	====> Epoch: 1061, cost 26.25 s
2023-12-31 10:32:24,167	44k	INFO	====> Epoch: 1062, cost 26.77 s
2023-12-31 10:32:47,956	44k	INFO	Train Epoch: 1063 [86%]
2023-12-31 10:32:47,956	44k	INFO	Losses: [2.376537799835205, 2.507063388824463, 8.6788969039917, 19.826976776123047, 0.9313569068908691], step: 37200, lr: 8.735993255593163e-05, reference_loss: 34.320831298828125
2023-12-31 10:32:51,761	44k	INFO	====> Epoch: 1063, cost 27.59 s
2023-12-31 10:33:18,160	44k	INFO	====> Epoch: 1064, cost 26.40 s
2023-12-31 10:33:44,805	44k	INFO	====> Epoch: 1065, cost 26.64 s
2023-12-31 10:34:11,451	44k	INFO	====> Epoch: 1066, cost 26.65 s
2023-12-31 10:34:38,216	44k	INFO	====> Epoch: 1067, cost 26.76 s
2023-12-31 10:35:04,740	44k	INFO	====> Epoch: 1068, cost 26.52 s
2023-12-31 10:35:20,521	44k	INFO	Train Epoch: 1069 [57%]
2023-12-31 10:35:20,521	44k	INFO	Losses: [2.7206475734710693, 2.2197911739349365, 5.388597011566162, 16.727718353271484, 0.5754183530807495], step: 37400, lr: 8.729443307808668e-05, reference_loss: 27.632173538208008
2023-12-31 10:35:31,624	44k	INFO	====> Epoch: 1069, cost 26.88 s
2023-12-31 10:35:58,529	44k	INFO	====> Epoch: 1070, cost 26.91 s
2023-12-31 10:36:25,126	44k	INFO	====> Epoch: 1071, cost 26.60 s
2023-12-31 10:36:51,742	44k	INFO	====> Epoch: 1072, cost 26.62 s
2023-12-31 10:37:18,351	44k	INFO	====> Epoch: 1073, cost 26.61 s
2023-12-31 10:37:44,746	44k	INFO	====> Epoch: 1074, cost 26.39 s
2023-12-31 10:37:53,131	44k	INFO	Train Epoch: 1075 [29%]
2023-12-31 10:37:53,132	44k	INFO	Losses: [2.584491729736328, 2.0628294944763184, 6.303991317749023, 19.604476928710938, 0.7963449954986572], step: 37600, lr: 8.722898270950122e-05, reference_loss: 31.352134704589844
2023-12-31 10:38:00,415	44k	INFO	Saving model and optimizer state at iteration 1075 to ./logs/44k/G_37600.pth
2023-12-31 10:38:01,629	44k	INFO	Saving model and optimizer state at iteration 1075 to ./logs/44k/D_37600.pth
2023-12-31 10:38:02,412	44k	INFO	.. Free up space by deleting ckpt ./logs/44k/G_35200.pth
2023-12-31 10:38:02,470	44k	INFO	.. Free up space by deleting ckpt ./logs/44k/D_35200.pth
2023-12-31 10:38:20,771	44k	INFO	====> Epoch: 1075, cost 36.03 s
2023-12-31 10:38:47,294	44k	INFO	====> Epoch: 1076, cost 26.52 s
2023-12-31 10:39:13,585	44k	INFO	====> Epoch: 1077, cost 26.29 s
2023-12-31 10:39:40,108	44k	INFO	====> Epoch: 1078, cost 26.52 s
2023-12-31 10:40:06,518	44k	INFO	====> Epoch: 1079, cost 26.41 s
2023-12-31 10:40:32,825	44k	INFO	====> Epoch: 1080, cost 26.31 s
2023-12-31 10:40:33,599	44k	INFO	Train Epoch: 1081 [0%]
2023-12-31 10:40:33,599	44k	INFO	Losses: [2.4173154830932617, 2.2962958812713623, 5.935914993286133, 17.57929039001465, 0.7171410322189331], step: 37800, lr: 8.716358141335484e-05, reference_loss: 28.94595718383789
2023-12-31 10:40:59,854	44k	INFO	====> Epoch: 1081, cost 27.03 s
2023-12-31 10:41:26,510	44k	INFO	====> Epoch: 1082, cost 26.66 s
2023-12-31 10:41:52,882	44k	INFO	====> Epoch: 1083, cost 26.37 s
2023-12-31 10:42:19,611	44k	INFO	====> Epoch: 1084, cost 26.73 s
2023-12-31 10:42:46,447	44k	INFO	====> Epoch: 1085, cost 26.84 s
2023-12-31 10:43:06,260	44k	INFO	Train Epoch: 1086 [71%]
2023-12-31 10:43:06,260	44k	INFO	Losses: [2.6561503410339355, 1.88020658493042, 5.008326530456543, 16.302459716796875, 0.7444322109222412], step: 38000, lr: 8.710911779257877e-05, reference_loss: 26.591575622558594
2023-12-31 10:43:13,501	44k	INFO	====> Epoch: 1086, cost 27.05 s
2023-12-31 10:43:40,148	44k	INFO	====> Epoch: 1087, cost 26.65 s
2023-12-31 10:44:06,684	44k	INFO	====> Epoch: 1088, cost 26.54 s
2023-12-31 10:44:33,407	44k	INFO	====> Epoch: 1089, cost 26.72 s
2023-12-31 10:44:59,974	44k	INFO	====> Epoch: 1090, cost 26.57 s
2023-12-31 10:45:26,634	44k	INFO	====> Epoch: 1091, cost 26.66 s
2023-12-31 10:45:38,727	44k	INFO	Train Epoch: 1092 [43%]
2023-12-31 10:45:38,727	44k	INFO	Losses: [2.5000593662261963, 2.2021644115448, 7.141624450683594, 17.454591751098633, 0.7616719007492065], step: 38200, lr: 8.704380636703143e-05, reference_loss: 30.060110092163086
2023-12-31 10:45:53,560	44k	INFO	====> Epoch: 1092, cost 26.93 s
2023-12-31 10:46:19,917	44k	INFO	====> Epoch: 1093, cost 26.36 s
2023-12-31 10:46:46,380	44k	INFO	====> Epoch: 1094, cost 26.46 s
2023-12-31 10:47:13,047	44k	INFO	====> Epoch: 1095, cost 26.67 s
2023-12-31 10:47:39,607	44k	INFO	====> Epoch: 1096, cost 26.56 s
2023-12-31 10:48:06,046	44k	INFO	====> Epoch: 1097, cost 26.44 s
2023-12-31 10:48:10,685	44k	INFO	Train Epoch: 1098 [14%]
2023-12-31 10:48:10,686	44k	INFO	Losses: [2.569551467895508, 2.0739357471466064, 6.9928669929504395, 20.229076385498047, 0.6764428019523621], step: 38400, lr: 8.697854390974844e-05, reference_loss: 32.541873931884766
2023-12-31 10:48:18,101	44k	INFO	Saving model and optimizer state at iteration 1098 to ./logs/44k/G_38400.pth
2023-12-31 10:48:19,318	44k	INFO	Saving model and optimizer state at iteration 1098 to ./logs/44k/D_38400.pth
2023-12-31 10:48:20,099	44k	INFO	.. Free up space by deleting ckpt ./logs/44k/G_36000.pth
2023-12-31 10:48:20,156	44k	INFO	.. Free up space by deleting ckpt ./logs/44k/D_36000.pth
2023-12-31 10:48:42,039	44k	INFO	====> Epoch: 1098, cost 35.99 s
2023-12-31 10:49:08,532	44k	INFO	====> Epoch: 1099, cost 26.49 s
2023-12-31 10:49:34,771	44k	INFO	====> Epoch: 1100, cost 26.24 s
2023-12-31 10:50:01,274	44k	INFO	====> Epoch: 1101, cost 26.50 s
2023-12-31 10:50:27,610	44k	INFO	====> Epoch: 1102, cost 26.34 s
2023-12-31 10:50:51,143	44k	INFO	Train Epoch: 1103 [86%]
2023-12-31 10:50:51,143	44k	INFO	Losses: [2.4397807121276855, 2.5284667015075684, 7.681996822357178, 17.820205688476562, 0.5336111187934875], step: 38600, lr: 8.692419590850362e-05, reference_loss: 31.004060745239258
2023-12-31 10:50:54,944	44k	INFO	====> Epoch: 1103, cost 27.33 s
2023-12-31 10:51:21,376	44k	INFO	====> Epoch: 1104, cost 26.43 s
2023-12-31 10:51:47,965	44k	INFO	====> Epoch: 1105, cost 26.59 s
2023-12-31 10:52:14,671	44k	INFO	====> Epoch: 1106, cost 26.71 s
2023-12-31 10:52:41,192	44k	INFO	====> Epoch: 1107, cost 26.52 s
2023-12-31 10:53:07,796	44k	INFO	====> Epoch: 1108, cost 26.60 s
2023-12-31 10:53:23,670	44k	INFO	Train Epoch: 1109 [57%]
2023-12-31 10:53:23,670	44k	INFO	Losses: [2.538179397583008, 2.472269058227539, 5.885288238525391, 17.792713165283203, 0.6967347860336304], step: 38800, lr: 8.68590231310355e-05, reference_loss: 29.38518524169922
2023-12-31 10:53:34,866	44k	INFO	====> Epoch: 1109, cost 27.07 s
2023-12-31 10:54:01,560	44k	INFO	====> Epoch: 1110, cost 26.69 s
2023-12-31 10:54:28,163	44k	INFO	====> Epoch: 1111, cost 26.60 s
2023-12-31 10:54:54,561	44k	INFO	====> Epoch: 1112, cost 26.40 s
2023-12-31 10:55:20,957	44k	INFO	====> Epoch: 1113, cost 26.40 s
2023-12-31 10:55:47,349	44k	INFO	====> Epoch: 1114, cost 26.39 s
2023-12-31 10:55:55,767	44k	INFO	Train Epoch: 1115 [29%]
2023-12-31 10:55:55,767	44k	INFO	Losses: [2.673149585723877, 2.1108219623565674, 5.636071681976318, 17.548925399780273, 0.8445448279380798], step: 39000, lr: 8.679389921787813e-05, reference_loss: 28.813514709472656
2023-12-31 10:56:14,439	44k	INFO	====> Epoch: 1115, cost 27.09 s
2023-12-31 10:56:41,085	44k	INFO	====> Epoch: 1116, cost 26.65 s
2023-12-31 10:57:07,600	44k	INFO	====> Epoch: 1117, cost 26.52 s
2023-12-31 10:57:34,373	44k	INFO	====> Epoch: 1118, cost 26.77 s
2023-12-31 10:58:00,821	44k	INFO	====> Epoch: 1119, cost 26.45 s
2023-12-31 10:58:27,417	44k	INFO	====> Epoch: 1120, cost 26.60 s
2023-12-31 10:58:28,202	44k	INFO	Train Epoch: 1121 [0%]
2023-12-31 10:58:28,202	44k	INFO	Losses: [2.5076162815093994, 2.2196340560913086, 5.960120677947998, 16.821334838867188, 0.5357269644737244], step: 39200, lr: 8.672882413239476e-05, reference_loss: 28.044431686401367
2023-12-31 10:58:35,496	44k	INFO	Saving model and optimizer state at iteration 1121 to ./logs/44k/G_39200.pth
2023-12-31 10:58:36,673	44k	INFO	Saving model and optimizer state at iteration 1121 to ./logs/44k/D_39200.pth
2023-12-31 10:58:37,473	44k	INFO	.. Free up space by deleting ckpt ./logs/44k/G_36800.pth
2023-12-31 10:58:37,531	44k	INFO	.. Free up space by deleting ckpt ./logs/44k/D_36800.pth
2023-12-31 10:59:03,316	44k	INFO	====> Epoch: 1121, cost 35.90 s
2023-12-31 10:59:29,767	44k	INFO	====> Epoch: 1122, cost 26.45 s
2023-12-31 10:59:56,601	44k	INFO	====> Epoch: 1123, cost 26.83 s
2023-12-31 11:00:23,001	44k	INFO	====> Epoch: 1124, cost 26.40 s
2023-12-31 11:00:49,401	44k	INFO	====> Epoch: 1125, cost 26.40 s
2023-12-31 11:01:09,280	44k	INFO	Train Epoch: 1126 [71%]
2023-12-31 11:01:09,281	44k	INFO	Losses: [2.5125975608825684, 2.1834137439727783, 7.034286975860596, 18.71828269958496, 0.7399468421936035], step: 39400, lr: 8.667463216699696e-05, reference_loss: 31.188528060913086
2023-12-31 11:01:16,501	44k	INFO	====> Epoch: 1126, cost 27.10 s
2023-12-31 11:01:43,068	44k	INFO	====> Epoch: 1127, cost 26.57 s
2023-12-31 11:02:09,684	44k	INFO	====> Epoch: 1128, cost 26.62 s
2023-12-31 11:02:36,301	44k	INFO	====> Epoch: 1129, cost 26.62 s
2023-12-31 11:03:02,871	44k	INFO	====> Epoch: 1130, cost 26.57 s
2023-12-31 11:03:29,585	44k	INFO	====> Epoch: 1131, cost 26.71 s
2023-12-31 11:03:41,715	44k	INFO	Train Epoch: 1132 [43%]
2023-12-31 11:03:41,715	44k	INFO	Losses: [2.4579901695251465, 2.5498692989349365, 5.679391860961914, 15.748372077941895, 0.6583945155143738], step: 39600, lr: 8.66096465038532e-05, reference_loss: 27.094017028808594
2023-12-31 11:03:56,693	44k	INFO	====> Epoch: 1132, cost 27.11 s
2023-12-31 11:04:23,289	44k	INFO	====> Epoch: 1133, cost 26.60 s
2023-12-31 11:04:49,833	44k	INFO	====> Epoch: 1134, cost 26.54 s
2023-12-31 11:05:16,162	44k	INFO	====> Epoch: 1135, cost 26.33 s
2023-12-31 11:05:42,820	44k	INFO	====> Epoch: 1136, cost 26.66 s
2023-12-31 11:06:09,403	44k	INFO	====> Epoch: 1137, cost 26.58 s
2023-12-31 11:06:14,046	44k	INFO	Train Epoch: 1138 [14%]
2023-12-31 11:06:14,046	44k	INFO	Losses: [2.4087471961975098, 2.2300612926483154, 6.430726528167725, 18.41869354248047, 0.8717213273048401], step: 39800, lr: 8.654470956472831e-05, reference_loss: 30.359949111938477
2023-12-31 11:06:36,785	44k	INFO	====> Epoch: 1138, cost 27.38 s
2023-12-31 11:07:03,353	44k	INFO	====> Epoch: 1139, cost 26.57 s
2023-12-31 11:07:29,730	44k	INFO	====> Epoch: 1140, cost 26.38 s
2023-12-31 11:07:56,142	44k	INFO	====> Epoch: 1141, cost 26.41 s
2023-12-31 11:08:22,514	44k	INFO	====> Epoch: 1142, cost 26.37 s
2023-12-31 11:08:46,028	44k	INFO	Train Epoch: 1143 [86%]
2023-12-31 11:08:46,029	44k	INFO	Losses: [2.5641491413116455, 2.3710389137268066, 6.155447483062744, 17.04971694946289, 0.5450931787490845], step: 40000, lr: 8.649063264217098e-05, reference_loss: 28.68544578552246
2023-12-31 11:08:53,301	44k	INFO	Saving model and optimizer state at iteration 1143 to ./logs/44k/G_40000.pth
2023-12-31 11:08:54,790	44k	INFO	Saving model and optimizer state at iteration 1143 to ./logs/44k/D_40000.pth
2023-12-31 11:08:55,565	44k	INFO	.. Free up space by deleting ckpt ./logs/44k/G_37600.pth
2023-12-31 11:08:55,622	44k	INFO	.. Free up space by deleting ckpt ./logs/44k/D_37600.pth
2023-12-31 11:08:58,646	44k	INFO	====> Epoch: 1143, cost 36.13 s
2023-12-31 11:09:25,325	44k	INFO	====> Epoch: 1144, cost 26.68 s
2023-12-31 11:09:51,952	44k	INFO	====> Epoch: 1145, cost 26.63 s
2023-12-31 11:10:18,624	44k	INFO	====> Epoch: 1146, cost 26.67 s
2023-12-31 11:10:45,081	44k	INFO	====> Epoch: 1147, cost 26.46 s
2023-12-31 11:11:11,360	44k	INFO	====> Epoch: 1148, cost 26.28 s
2023-12-31 11:11:27,342	44k	INFO	Train Epoch: 1149 [57%]
2023-12-31 11:11:27,342	44k	INFO	Losses: [2.5723180770874023, 2.1039583683013916, 5.225404262542725, 15.490204811096191, 0.8293313980102539], step: 40200, lr: 8.642578493555313e-05, reference_loss: 26.221214294433594
2023-12-31 11:11:38,512	44k	INFO	====> Epoch: 1149, cost 27.15 s
2023-12-31 11:12:05,431	44k	INFO	====> Epoch: 1150, cost 26.92 s
2023-12-31 11:12:31,685	44k	INFO	====> Epoch: 1151, cost 26.25 s
2023-12-31 11:12:58,165	44k	INFO	====> Epoch: 1152, cost 26.48 s
2023-12-31 11:13:24,450	44k	INFO	====> Epoch: 1153, cost 26.29 s
2023-12-31 11:13:50,898	44k	INFO	====> Epoch: 1154, cost 26.45 s
2023-12-31 11:13:59,306	44k	INFO	Train Epoch: 1155 [29%]
2023-12-31 11:13:59,307	44k	INFO	Losses: [2.538235664367676, 2.0970399379730225, 5.390906810760498, 16.828401565551758, 0.47911861538887024], step: 40400, lr: 8.636098584951912e-05, reference_loss: 27.333702087402344
2023-12-31 11:14:17,826	44k	INFO	====> Epoch: 1155, cost 26.93 s
2023-12-31 11:14:44,371	44k	INFO	====> Epoch: 1156, cost 26.54 s
2023-12-31 11:15:11,334	44k	INFO	====> Epoch: 1157, cost 26.96 s
2023-12-31 11:15:37,896	44k	INFO	====> Epoch: 1158, cost 26.56 s
2023-12-31 11:16:04,273	44k	INFO	====> Epoch: 1159, cost 26.38 s
2023-12-31 11:16:30,706	44k	INFO	====> Epoch: 1160, cost 26.43 s
2023-12-31 11:16:31,480	44k	INFO	Train Epoch: 1161 [0%]
2023-12-31 11:16:31,480	44k	INFO	Losses: [2.5111124515533447, 2.073608636856079, 6.134515762329102, 16.252445220947266, 0.33842405676841736], step: 40600, lr: 8.629623534761487e-05, reference_loss: 27.31010627746582
2023-12-31 11:16:57,571	44k	INFO	====> Epoch: 1161, cost 26.86 s
2023-12-31 11:17:24,060	44k	INFO	====> Epoch: 1162, cost 26.49 s
2023-12-31 11:17:50,596	44k	INFO	====> Epoch: 1163, cost 26.54 s
2023-12-31 11:18:17,189	44k	INFO	====> Epoch: 1164, cost 26.59 s
2023-12-31 11:18:43,904	44k	INFO	====> Epoch: 1165, cost 26.72 s
2023-12-31 11:19:03,753	44k	INFO	Train Epoch: 1166 [71%]
2023-12-31 11:19:03,754	44k	INFO	Losses: [2.7578413486480713, 1.8950934410095215, 4.317193508148193, 14.865592002868652, 0.9577007293701172], step: 40800, lr: 8.624231368262399e-05, reference_loss: 24.793420791625977
2023-12-31 11:19:10,891	44k	INFO	Saving model and optimizer state at iteration 1166 to ./logs/44k/G_40800.pth
2023-12-31 11:19:12,060	44k	INFO	Saving model and optimizer state at iteration 1166 to ./logs/44k/D_40800.pth
2023-12-31 11:19:12,833	44k	INFO	.. Free up space by deleting ckpt ./logs/44k/G_38400.pth
2023-12-31 11:19:12,890	44k	INFO	.. Free up space by deleting ckpt ./logs/44k/D_38400.pth
2023-12-31 11:19:19,627	44k	INFO	====> Epoch: 1166, cost 35.72 s
2023-12-31 11:19:46,255	44k	INFO	====> Epoch: 1167, cost 26.63 s
2023-12-31 11:20:12,895	44k	INFO	====> Epoch: 1168, cost 26.64 s
2023-12-31 11:20:39,598	44k	INFO	====> Epoch: 1169, cost 26.70 s
2023-12-31 11:21:06,388	44k	INFO	====> Epoch: 1170, cost 26.79 s
2023-12-31 11:21:32,965	44k	INFO	====> Epoch: 1171, cost 26.58 s
2023-12-31 11:21:45,137	44k	INFO	Train Epoch: 1172 [43%]
2023-12-31 11:21:45,138	44k	INFO	Losses: [2.6402573585510254, 2.116373062133789, 5.172130107879639, 15.835269927978516, 0.9195883870124817], step: 41000, lr: 8.617765215703574e-05, reference_loss: 26.683618545532227
2023-12-31 11:21:59,997	44k	INFO	====> Epoch: 1172, cost 27.03 s
2023-12-31 11:22:26,519	44k	INFO	====> Epoch: 1173, cost 26.52 s
2023-12-31 11:22:53,229	44k	INFO	====> Epoch: 1174, cost 26.71 s
2023-12-31 11:23:19,753	44k	INFO	====> Epoch: 1175, cost 26.52 s
2023-12-31 11:23:46,299	44k	INFO	====> Epoch: 1176, cost 26.55 s
2023-12-31 11:24:13,005	44k	INFO	====> Epoch: 1177, cost 26.71 s
2023-12-31 11:24:17,602	44k	INFO	Train Epoch: 1178 [14%]
2023-12-31 11:24:17,602	44k	INFO	Losses: [2.530630111694336, 2.1382696628570557, 6.2607102394104, 19.590097427368164, 0.6386646628379822], step: 41200, lr: 8.611303911243917e-05, reference_loss: 31.158370971679688
2023-12-31 11:24:40,251	44k	INFO	====> Epoch: 1178, cost 27.25 s
2023-12-31 11:25:06,899	44k	INFO	====> Epoch: 1179, cost 26.65 s
2023-12-31 11:25:33,403	44k	INFO	====> Epoch: 1180, cost 26.50 s
2023-12-31 11:25:59,772	44k	INFO	====> Epoch: 1181, cost 26.37 s
2023-12-31 11:26:26,415	44k	INFO	====> Epoch: 1182, cost 26.64 s
2023-12-31 11:26:49,933	44k	INFO	Train Epoch: 1183 [86%]
2023-12-31 11:26:49,934	44k	INFO	Losses: [2.591524600982666, 2.3208327293395996, 7.769404411315918, 16.84772491455078, 0.65629643201828], step: 41400, lr: 8.605923191647444e-05, reference_loss: 30.18578338623047
2023-12-31 11:26:53,462	44k	INFO	====> Epoch: 1183, cost 27.05 s
2023-12-31 11:27:20,416	44k	INFO	====> Epoch: 1184, cost 26.95 s
2023-12-31 11:27:46,947	44k	INFO	====> Epoch: 1185, cost 26.53 s
2023-12-31 11:28:13,458	44k	INFO	====> Epoch: 1186, cost 26.51 s
2023-12-31 11:28:39,968	44k	INFO	====> Epoch: 1187, cost 26.51 s
2023-12-31 11:29:06,466	44k	INFO	====> Epoch: 1188, cost 26.50 s
2023-12-31 11:29:22,481	44k	INFO	Train Epoch: 1189 [57%]
2023-12-31 11:29:22,481	44k	INFO	Losses: [2.4056971073150635, 2.495649576187134, 6.838470935821533, 17.032276153564453, 0.6727160811424255], step: 41600, lr: 8.599470765930816e-05, reference_loss: 29.44481086730957
2023-12-31 11:29:29,731	44k	INFO	Saving model and optimizer state at iteration 1189 to ./logs/44k/G_41600.pth
2023-12-31 11:29:30,893	44k	INFO	Saving model and optimizer state at iteration 1189 to ./logs/44k/D_41600.pth
2023-12-31 11:29:31,664	44k	INFO	.. Free up space by deleting ckpt ./logs/44k/G_39200.pth
2023-12-31 11:29:31,720	44k	INFO	.. Free up space by deleting ckpt ./logs/44k/D_39200.pth
2023-12-31 11:29:42,369	44k	INFO	====> Epoch: 1189, cost 35.90 s
2023-12-31 11:30:09,175	44k	INFO	====> Epoch: 1190, cost 26.81 s
2023-12-31 11:30:35,507	44k	INFO	====> Epoch: 1191, cost 26.33 s
2023-12-31 11:31:01,926	44k	INFO	====> Epoch: 1192, cost 26.42 s
2023-12-31 11:31:28,563	44k	INFO	====> Epoch: 1193, cost 26.64 s
2023-12-31 11:31:54,983	44k	INFO	====> Epoch: 1194, cost 26.42 s
2023-12-31 11:32:03,337	44k	INFO	Train Epoch: 1195 [29%]
2023-12-31 11:32:03,338	44k	INFO	Losses: [2.66047739982605, 1.9537378549575806, 5.2327561378479, 16.479576110839844, 0.5849004983901978], step: 41800, lr: 8.593023178021442e-05, reference_loss: 26.911447525024414
2023-12-31 11:32:21,834	44k	INFO	====> Epoch: 1195, cost 26.85 s
2023-12-31 11:32:48,454	44k	INFO	====> Epoch: 1196, cost 26.62 s
2023-12-31 11:33:15,043	44k	INFO	====> Epoch: 1197, cost 26.59 s
2023-12-31 11:33:41,635	44k	INFO	====> Epoch: 1198, cost 26.59 s
2023-12-31 11:34:08,303	44k	INFO	====> Epoch: 1199, cost 26.67 s
2023-12-31 11:34:34,587	44k	INFO	====> Epoch: 1200, cost 26.28 s
2023-12-31 11:34:35,359	44k	INFO	Train Epoch: 1201 [0%]
2023-12-31 11:34:35,360	44k	INFO	Losses: [2.708878993988037, 2.1468098163604736, 5.687728404998779, 16.083269119262695, 0.6510279774665833], step: 42000, lr: 8.586580424292098e-05, reference_loss: 27.277713775634766
2023-12-31 11:35:01,796	44k	INFO	====> Epoch: 1201, cost 27.21 s
2023-12-31 11:35:28,297	44k	INFO	====> Epoch: 1202, cost 26.50 s
2023-12-31 11:35:54,577	44k	INFO	====> Epoch: 1203, cost 26.28 s
2023-12-31 11:36:21,167	44k	INFO	====> Epoch: 1204, cost 26.59 s
2023-12-31 11:36:47,716	44k	INFO	====> Epoch: 1205, cost 26.55 s
2023-12-31 11:37:07,819	44k	INFO	Train Epoch: 1206 [71%]
2023-12-31 11:37:07,819	44k	INFO	Losses: [2.4903690814971924, 2.1460440158843994, 7.794682502746582, 19.736154556274414, 0.8336433172225952], step: 42200, lr: 8.581215153012409e-05, reference_loss: 33.00089645385742
2023-12-31 11:37:15,019	44k	INFO	====> Epoch: 1206, cost 27.30 s
2023-12-31 11:37:41,415	44k	INFO	====> Epoch: 1207, cost 26.40 s
2023-12-31 11:38:07,991	44k	INFO	====> Epoch: 1208, cost 26.58 s
2023-12-31 11:38:34,622	44k	INFO	====> Epoch: 1209, cost 26.63 s
2023-12-31 11:39:01,245	44k	INFO	====> Epoch: 1210, cost 26.62 s
2023-12-31 11:39:27,860	44k	INFO	====> Epoch: 1211, cost 26.61 s
2023-12-31 11:39:40,092	44k	INFO	Train Epoch: 1212 [43%]
2023-12-31 11:39:40,092	44k	INFO	Losses: [2.6489858627319336, 2.116081953048706, 5.937402725219727, 16.67072868347168, 0.483771413564682], step: 42400, lr: 8.574781252534775e-05, reference_loss: 27.856971740722656
2023-12-31 11:39:47,710	44k	INFO	Saving model and optimizer state at iteration 1212 to ./logs/44k/G_42400.pth
2023-12-31 11:39:48,896	44k	INFO	Saving model and optimizer state at iteration 1212 to ./logs/44k/D_42400.pth
2023-12-31 11:39:49,664	44k	INFO	.. Free up space by deleting ckpt ./logs/44k/G_40000.pth
2023-12-31 11:39:49,721	44k	INFO	.. Free up space by deleting ckpt ./logs/44k/D_40000.pth
2023-12-31 11:40:04,173	44k	INFO	====> Epoch: 1212, cost 36.31 s
2023-12-31 11:40:30,902	44k	INFO	====> Epoch: 1213, cost 26.73 s
2023-12-31 11:40:57,427	44k	INFO	====> Epoch: 1214, cost 26.52 s
2023-12-31 11:41:24,018	44k	INFO	====> Epoch: 1215, cost 26.59 s
2023-12-31 11:41:50,636	44k	INFO	====> Epoch: 1216, cost 26.62 s
2023-12-31 11:42:17,208	44k	INFO	====> Epoch: 1217, cost 26.57 s
2023-12-31 11:42:21,862	44k	INFO	Train Epoch: 1218 [14%]
2023-12-31 11:42:21,863	44k	INFO	Losses: [2.3157858848571777, 2.213686943054199, 6.241931438446045, 18.655794143676758, 0.7016446590423584], step: 42600, lr: 8.568352175974806e-05, reference_loss: 30.128843307495117
2023-12-31 11:42:44,525	44k	INFO	====> Epoch: 1218, cost 27.32 s
2023-12-31 11:43:10,976	44k	INFO	====> Epoch: 1219, cost 26.45 s
2023-12-31 11:43:37,425	44k	INFO	====> Epoch: 1220, cost 26.45 s
2023-12-31 11:44:03,803	44k	INFO	====> Epoch: 1221, cost 26.38 s
2023-12-31 11:44:30,311	44k	INFO	====> Epoch: 1222, cost 26.51 s
2023-12-31 11:44:53,741	44k	INFO	Train Epoch: 1223 [86%]
2023-12-31 11:44:53,742	44k	INFO	Losses: [2.3068130016326904, 2.4351518154144287, 8.978713035583496, 19.785669326782227, 0.8878746628761292], step: 42800, lr: 8.562998294502507e-05, reference_loss: 34.394222259521484
2023-12-31 11:44:57,193	44k	INFO	====> Epoch: 1223, cost 26.88 s
2023-12-31 11:45:23,692	44k	INFO	====> Epoch: 1224, cost 26.50 s
2023-12-31 11:45:50,637	44k	INFO	====> Epoch: 1225, cost 26.94 s
2023-12-31 11:46:17,198	44k	INFO	====> Epoch: 1226, cost 26.56 s
2023-12-31 11:46:43,652	44k	INFO	====> Epoch: 1227, cost 26.45 s
2023-12-31 11:47:10,225	44k	INFO	====> Epoch: 1228, cost 26.57 s
2023-12-31 11:47:26,220	44k	INFO	Train Epoch: 1229 [57%]
2023-12-31 11:47:26,220	44k	INFO	Losses: [2.694953680038452, 2.105332136154175, 5.16530179977417, 16.117536544799805, 0.5388626456260681], step: 43000, lr: 8.556578052399892e-05, reference_loss: 26.621986389160156
2023-12-31 11:47:37,331	44k	INFO	====> Epoch: 1229, cost 27.11 s
2023-12-31 11:48:03,997	44k	INFO	====> Epoch: 1230, cost 26.67 s
2023-12-31 11:48:30,643	44k	INFO	====> Epoch: 1231, cost 26.65 s
2023-12-31 11:48:57,501	44k	INFO	====> Epoch: 1232, cost 26.86 s
2023-12-31 11:49:23,916	44k	INFO	====> Epoch: 1233, cost 26.42 s
2023-12-31 11:49:50,444	44k	INFO	====> Epoch: 1234, cost 26.53 s
2023-12-31 11:49:58,958	44k	INFO	Train Epoch: 1235 [29%]
2023-12-31 11:49:58,958	44k	INFO	Losses: [2.543351411819458, 2.3233890533447266, 6.693163871765137, 18.509458541870117, 0.8284732699394226], step: 43200, lr: 8.550162623974361e-05, reference_loss: 30.897836685180664
2023-12-31 11:50:06,069	44k	INFO	Saving model and optimizer state at iteration 1235 to ./logs/44k/G_43200.pth
2023-12-31 11:50:07,254	44k	INFO	Saving model and optimizer state at iteration 1235 to ./logs/44k/D_43200.pth
2023-12-31 11:50:08,023	44k	INFO	.. Free up space by deleting ckpt ./logs/44k/G_40800.pth
2023-12-31 11:50:08,079	44k	INFO	.. Free up space by deleting ckpt ./logs/44k/D_40800.pth
2023-12-31 11:50:26,385	44k	INFO	====> Epoch: 1235, cost 35.94 s
2023-12-31 11:50:53,036	44k	INFO	====> Epoch: 1236, cost 26.65 s
2023-12-31 11:51:20,050	44k	INFO	====> Epoch: 1237, cost 27.01 s
2023-12-31 11:51:46,883	44k	INFO	====> Epoch: 1238, cost 26.83 s
2023-12-31 11:52:13,535	44k	INFO	====> Epoch: 1239, cost 26.65 s
2023-12-31 11:52:39,967	44k	INFO	====> Epoch: 1240, cost 26.43 s
2023-12-31 11:52:40,740	44k	INFO	Train Epoch: 1241 [0%]
2023-12-31 11:52:40,740	44k	INFO	Losses: [2.301326274871826, 2.523761510848999, 7.191089153289795, 17.239810943603516, 0.6926484704017639], step: 43400, lr: 8.543752005616782e-05, reference_loss: 29.94863510131836
2023-12-31 11:53:07,236	44k	INFO	====> Epoch: 1241, cost 27.27 s
2023-12-31 11:53:33,685	44k	INFO	====> Epoch: 1242, cost 26.45 s
2023-12-31 11:54:00,182	44k	INFO	====> Epoch: 1243, cost 26.50 s
2023-12-31 11:54:26,735	44k	INFO	====> Epoch: 1244, cost 26.55 s
2023-12-31 11:54:53,455	44k	INFO	====> Epoch: 1245, cost 26.72 s
2023-12-31 11:55:13,484	44k	INFO	Train Epoch: 1246 [71%]
2023-12-31 11:55:13,485	44k	INFO	Losses: [2.771198034286499, 2.1253249645233154, 5.420302867889404, 16.491771697998047, 0.6626167297363281], step: 43600, lr: 8.538413495407661e-05, reference_loss: 27.471214294433594
2023-12-31 11:55:20,753	44k	INFO	====> Epoch: 1246, cost 27.30 s
2023-12-31 11:55:47,200	44k	INFO	====> Epoch: 1247, cost 26.45 s
2023-12-31 11:56:13,711	44k	INFO	====> Epoch: 1248, cost 26.51 s
2023-12-31 11:56:40,281	44k	INFO	====> Epoch: 1249, cost 26.57 s
2023-12-31 11:57:06,839	44k	INFO	====> Epoch: 1250, cost 26.56 s
2023-12-31 11:57:33,464	44k	INFO	====> Epoch: 1251, cost 26.63 s
2023-12-31 11:57:45,688	44k	INFO	Train Epoch: 1252 [43%]
2023-12-31 11:57:45,689	44k	INFO	Losses: [2.712829351425171, 2.153960704803467, 7.417222499847412, 17.327613830566406, 0.6324182748794556], step: 43800, lr: 8.532011686143266e-05, reference_loss: 30.244043350219727
2023-12-31 11:58:00,967	44k	INFO	====> Epoch: 1252, cost 27.50 s
2023-12-31 11:58:27,565	44k	INFO	====> Epoch: 1253, cost 26.60 s
2023-12-31 11:58:54,386	44k	INFO	====> Epoch: 1254, cost 26.82 s
2023-12-31 11:59:20,910	44k	INFO	====> Epoch: 1255, cost 26.52 s
2023-12-31 11:59:47,297	44k	INFO	====> Epoch: 1256, cost 26.39 s
2023-12-31 12:00:13,944	44k	INFO	====> Epoch: 1257, cost 26.65 s
2023-12-31 12:00:18,570	44k	INFO	Train Epoch: 1258 [14%]
2023-12-31 12:00:18,570	44k	INFO	Losses: [2.447406053543091, 2.150296926498413, 7.035515308380127, 19.480188369750977, 0.569417417049408], step: 44000, lr: 8.525614676735643e-05, reference_loss: 31.682825088500977
2023-12-31 12:00:25,738	44k	INFO	Saving model and optimizer state at iteration 1258 to ./logs/44k/G_44000.pth
2023-12-31 12:00:27,228	44k	INFO	Saving model and optimizer state at iteration 1258 to ./logs/44k/D_44000.pth
2023-12-31 12:00:28,004	44k	INFO	.. Free up space by deleting ckpt ./logs/44k/G_41600.pth
2023-12-31 12:00:28,061	44k	INFO	.. Free up space by deleting ckpt ./logs/44k/D_41600.pth
2023-12-31 12:00:49,959	44k	INFO	====> Epoch: 1258, cost 36.02 s
2023-12-31 12:01:16,601	44k	INFO	====> Epoch: 1259, cost 26.64 s
2023-12-31 12:01:43,341	44k	INFO	====> Epoch: 1260, cost 26.74 s
2023-12-31 12:02:10,007	44k	INFO	====> Epoch: 1261, cost 26.67 s
2023-12-31 12:02:36,588	44k	INFO	====> Epoch: 1262, cost 26.58 s
2023-12-31 12:03:00,154	44k	INFO	Train Epoch: 1263 [86%]
2023-12-31 12:03:00,154	44k	INFO	Losses: [2.333582639694214, 2.4517507553100586, 7.794464111328125, 17.614700317382812, 0.4976727366447449], step: 44200, lr: 8.52028749952347e-05, reference_loss: 30.692171096801758
2023-12-31 12:03:03,784	44k	INFO	====> Epoch: 1263, cost 27.20 s
2023-12-31 12:03:30,214	44k	INFO	====> Epoch: 1264, cost 26.43 s
2023-12-31 12:03:57,111	44k	INFO	====> Epoch: 1265, cost 26.90 s
2023-12-31 12:04:23,720	44k	INFO	====> Epoch: 1266, cost 26.61 s
2023-12-31 12:04:50,320	44k	INFO	====> Epoch: 1267, cost 26.60 s
2023-12-31 12:05:16,802	44k	INFO	====> Epoch: 1268, cost 26.48 s
2023-12-31 12:05:32,884	44k	INFO	Train Epoch: 1269 [57%]
2023-12-31 12:05:32,885	44k	INFO	Losses: [2.4697513580322266, 2.2879815101623535, 6.659113883972168, 17.302352905273438, 0.8365485668182373], step: 44400, lr: 8.513899280508415e-05, reference_loss: 29.555747985839844
2023-12-31 12:05:43,893	44k	INFO	====> Epoch: 1269, cost 27.09 s
2023-12-31 12:06:10,520	44k	INFO	====> Epoch: 1270, cost 26.63 s
2023-12-31 12:06:37,018	44k	INFO	====> Epoch: 1271, cost 26.50 s
2023-12-31 12:07:03,830	44k	INFO	====> Epoch: 1272, cost 26.81 s
2023-12-31 12:07:30,423	44k	INFO	====> Epoch: 1273, cost 26.59 s
2023-12-31 12:07:57,085	44k	INFO	====> Epoch: 1274, cost 26.66 s
2023-12-31 12:08:05,570	44k	INFO	Train Epoch: 1275 [29%]
2023-12-31 12:08:05,570	44k	INFO	Losses: [2.5047836303710938, 2.2757396697998047, 6.9394755363464355, 17.890470504760742, 0.8232161402702332], step: 44600, lr: 8.507515851160631e-05, reference_loss: 30.433685302734375
2023-12-31 12:08:24,143	44k	INFO	====> Epoch: 1275, cost 27.06 s
2023-12-31 12:08:50,572	44k	INFO	====> Epoch: 1276, cost 26.43 s
2023-12-31 12:09:17,006	44k	INFO	====> Epoch: 1277, cost 26.43 s
2023-12-31 12:09:43,321	44k	INFO	====> Epoch: 1278, cost 26.31 s
2023-12-31 12:10:10,092	44k	INFO	====> Epoch: 1279, cost 26.77 s
2023-12-31 12:10:36,886	44k	INFO	====> Epoch: 1280, cost 26.79 s
2023-12-31 12:10:37,661	44k	INFO	Train Epoch: 1281 [0%]
2023-12-31 12:10:37,661	44k	INFO	Losses: [2.53975772857666, 2.5236294269561768, 6.203906059265137, 16.923202514648438, 0.6091499924659729], step: 44800, lr: 8.501137207888995e-05, reference_loss: 28.799644470214844
2023-12-31 12:10:45,020	44k	INFO	Saving model and optimizer state at iteration 1281 to ./logs/44k/G_44800.pth
2023-12-31 12:10:46,178	44k	INFO	Saving model and optimizer state at iteration 1281 to ./logs/44k/D_44800.pth
2023-12-31 12:10:46,952	44k	INFO	.. Free up space by deleting ckpt ./logs/44k/G_42400.pth
2023-12-31 12:10:47,008	44k	INFO	.. Free up space by deleting ckpt ./logs/44k/D_42400.pth
2023-12-31 12:11:13,000	44k	INFO	====> Epoch: 1281, cost 36.11 s
2023-12-31 12:11:39,729	44k	INFO	====> Epoch: 1282, cost 26.73 s
2023-12-31 12:12:06,299	44k	INFO	====> Epoch: 1283, cost 26.57 s
2023-12-31 12:12:33,158	44k	INFO	====> Epoch: 1284, cost 26.86 s
2023-12-31 12:12:59,757	44k	INFO	====> Epoch: 1285, cost 26.60 s
2023-12-31 12:13:19,497	44k	INFO	Train Epoch: 1286 [71%]
2023-12-31 12:13:19,498	44k	INFO	Losses: [2.5935676097869873, 2.126461982727051, 7.514280319213867, 17.933439254760742, 0.6056419610977173], step: 45000, lr: 8.495825325270724e-05, reference_loss: 30.773391723632812
2023-12-31 12:13:26,786	44k	INFO	====> Epoch: 1286, cost 27.03 s
2023-12-31 12:13:53,311	44k	INFO	====> Epoch: 1287, cost 26.52 s
2023-12-31 12:14:19,887	44k	INFO	====> Epoch: 1288, cost 26.58 s
2023-12-31 12:14:46,608	44k	INFO	====> Epoch: 1289, cost 26.72 s
2023-12-31 12:15:13,265	44k	INFO	====> Epoch: 1290, cost 26.66 s
2023-12-31 12:15:39,966	44k	INFO	====> Epoch: 1291, cost 26.70 s
2023-12-31 12:15:52,172	44k	INFO	Train Epoch: 1292 [43%]
2023-12-31 12:15:52,172	44k	INFO	Losses: [2.382570743560791, 2.4097540378570557, 6.004408836364746, 15.572489738464355, 0.7158614993095398], step: 45200, lr: 8.489455447153992e-05, reference_loss: 27.085086822509766
2023-12-31 12:16:07,443	44k	INFO	====> Epoch: 1292, cost 27.48 s
2023-12-31 12:16:33,962	44k	INFO	====> Epoch: 1293, cost 26.52 s
2023-12-31 12:17:00,441	44k	INFO	====> Epoch: 1294, cost 26.48 s
2023-12-31 12:17:26,911	44k	INFO	====> Epoch: 1295, cost 26.47 s
2023-12-31 12:17:53,345	44k	INFO	====> Epoch: 1296, cost 26.43 s
2023-12-31 12:18:19,916	44k	INFO	====> Epoch: 1297, cost 26.57 s
2023-12-31 12:18:24,546	44k	INFO	Train Epoch: 1298 [14%]
2023-12-31 12:18:24,547	44k	INFO	Losses: [2.5157439708709717, 2.3049676418304443, 6.5740580558776855, 19.120624542236328, 0.7595049142837524], step: 45400, lr: 8.483090344953156e-05, reference_loss: 31.274898529052734
2023-12-31 12:18:47,229	44k	INFO	====> Epoch: 1298, cost 27.31 s
2023-12-31 12:19:13,694	44k	INFO	====> Epoch: 1299, cost 26.47 s
2023-12-31 12:19:40,260	44k	INFO	====> Epoch: 1300, cost 26.57 s
2023-12-31 12:20:06,474	44k	INFO	====> Epoch: 1301, cost 26.21 s
2023-12-31 12:20:33,055	44k	INFO	====> Epoch: 1302, cost 26.58 s
2023-12-31 12:20:56,697	44k	INFO	Train Epoch: 1303 [86%]
2023-12-31 12:20:56,697	44k	INFO	Losses: [2.3938794136047363, 2.2990787029266357, 6.724761009216309, 16.80984115600586, 0.4477837383747101], step: 45600, lr: 8.477789738804749e-05, reference_loss: 28.675344467163086
2023-12-31 12:21:03,831	44k	INFO	Saving model and optimizer state at iteration 1303 to ./logs/44k/G_45600.pth
2023-12-31 12:21:05,005	44k	INFO	Saving model and optimizer state at iteration 1303 to ./logs/44k/D_45600.pth
2023-12-31 12:21:05,791	44k	INFO	.. Free up space by deleting ckpt ./logs/44k/G_43200.pth
2023-12-31 12:21:05,847	44k	INFO	.. Free up space by deleting ckpt ./logs/44k/D_43200.pth
2023-12-31 12:21:08,870	44k	INFO	====> Epoch: 1303, cost 35.81 s
2023-12-31 12:21:35,646	44k	INFO	====> Epoch: 1304, cost 26.78 s
2023-12-31 12:22:02,012	44k	INFO	====> Epoch: 1305, cost 26.37 s
2023-12-31 12:22:28,159	44k	INFO	====> Epoch: 1306, cost 26.15 s
2023-12-31 12:22:54,956	44k	INFO	====> Epoch: 1307, cost 26.80 s
2023-12-31 12:23:21,454	44k	INFO	====> Epoch: 1308, cost 26.50 s
2023-12-31 12:23:37,350	44k	INFO	Train Epoch: 1309 [57%]
2023-12-31 12:23:37,351	44k	INFO	Losses: [2.7098746299743652, 1.995194673538208, 5.158588409423828, 14.814680099487305, 0.7136995792388916], step: 45800, lr: 8.47143338315148e-05, reference_loss: 25.39203643798828
2023-12-31 12:23:48,556	44k	INFO	====> Epoch: 1309, cost 27.10 s
2023-12-31 12:24:15,054	44k	INFO	====> Epoch: 1310, cost 26.50 s
2023-12-31 12:24:41,784	44k	INFO	====> Epoch: 1311, cost 26.73 s
2023-12-31 12:25:08,418	44k	INFO	====> Epoch: 1312, cost 26.63 s
2023-12-31 12:25:35,263	44k	INFO	====> Epoch: 1313, cost 26.84 s
2023-12-31 12:26:01,758	44k	INFO	====> Epoch: 1314, cost 26.50 s
2023-12-31 12:26:10,253	44k	INFO	Train Epoch: 1315 [29%]
2023-12-31 12:26:10,254	44k	INFO	Losses: [2.4821557998657227, 2.205204486846924, 6.066188335418701, 17.116552352905273, 0.4874613285064697], step: 46000, lr: 8.465081793275431e-05, reference_loss: 28.357563018798828
2023-12-31 12:26:28,821	44k	INFO	====> Epoch: 1315, cost 27.06 s
2023-12-31 12:26:55,343	44k	INFO	====> Epoch: 1316, cost 26.52 s
2023-12-31 12:27:21,949	44k	INFO	====> Epoch: 1317, cost 26.61 s
2023-12-31 12:27:48,569	44k	INFO	====> Epoch: 1318, cost 26.62 s
2023-12-31 12:28:15,243	44k	INFO	====> Epoch: 1319, cost 26.67 s
2023-12-31 12:28:42,287	44k	INFO	====> Epoch: 1320, cost 27.04 s
2023-12-31 12:28:43,060	44k	INFO	Train Epoch: 1321 [0%]
2023-12-31 12:28:43,060	44k	INFO	Losses: [2.515613079071045, 2.28243088722229, 6.5378522872924805, 16.063594818115234, 0.23149272799491882], step: 46200, lr: 8.45873496560338e-05, reference_loss: 27.630983352661133
2023-12-31 12:29:09,441	44k	INFO	====> Epoch: 1321, cost 27.15 s
2023-12-31 12:29:35,969	44k	INFO	====> Epoch: 1322, cost 26.53 s
2023-12-31 12:30:02,623	44k	INFO	====> Epoch: 1323, cost 26.65 s
2023-12-31 12:30:29,185	44k	INFO	====> Epoch: 1324, cost 26.56 s
2023-12-31 12:30:55,685	44k	INFO	====> Epoch: 1325, cost 26.50 s
2023-12-31 12:31:15,596	44k	INFO	Train Epoch: 1326 [71%]
2023-12-31 12:31:15,596	44k	INFO	Losses: [2.4812300205230713, 2.152047634124756, 5.233489513397217, 14.9273099899292, 0.8578000068664551], step: 46400, lr: 8.453449577762016e-05, reference_loss: 25.651878356933594
2023-12-31 12:31:23,252	44k	INFO	Saving model and optimizer state at iteration 1326 to ./logs/44k/G_46400.pth
2023-12-31 12:31:24,430	44k	INFO	Saving model and optimizer state at iteration 1326 to ./logs/44k/D_46400.pth
2023-12-31 12:31:25,204	44k	INFO	.. Free up space by deleting ckpt ./logs/44k/G_44000.pth
2023-12-31 12:31:25,261	44k	INFO	.. Free up space by deleting ckpt ./logs/44k/D_44000.pth
2023-12-31 12:31:32,047	44k	INFO	====> Epoch: 1326, cost 36.36 s
2023-12-31 12:31:58,523	44k	INFO	====> Epoch: 1327, cost 26.48 s
2023-12-31 12:32:25,108	44k	INFO	====> Epoch: 1328, cost 26.59 s
2023-12-31 12:32:51,654	44k	INFO	====> Epoch: 1329, cost 26.55 s
2023-12-31 12:33:18,235	44k	INFO	====> Epoch: 1330, cost 26.58 s
2023-12-31 12:33:44,879	44k	INFO	====> Epoch: 1331, cost 26.64 s
2023-12-31 12:33:57,026	44k	INFO	Train Epoch: 1332 [43%]
2023-12-31 12:33:57,027	44k	INFO	Losses: [2.6434764862060547, 2.214444160461426, 5.37575101852417, 15.250868797302246, 0.8407381772994995], step: 46600, lr: 8.447111471525754e-05, reference_loss: 26.32527732849121
2023-12-31 12:34:12,270	44k	INFO	====> Epoch: 1332, cost 27.39 s
2023-12-31 12:34:38,803	44k	INFO	====> Epoch: 1333, cost 26.53 s
2023-12-31 12:35:05,426	44k	INFO	====> Epoch: 1334, cost 26.62 s
2023-12-31 12:35:31,990	44k	INFO	====> Epoch: 1335, cost 26.56 s
2023-12-31 12:35:58,646	44k	INFO	====> Epoch: 1336, cost 26.66 s
2023-12-31 12:36:25,272	44k	INFO	====> Epoch: 1337, cost 26.63 s
2023-12-31 12:36:29,885	44k	INFO	Train Epoch: 1338 [14%]
2023-12-31 12:36:29,886	44k	INFO	Losses: [2.488590717315674, 2.2803287506103516, 6.158401966094971, 18.025707244873047, 0.6433385610580444], step: 46800, lr: 8.440778117383924e-05, reference_loss: 29.59636878967285
2023-12-31 12:36:52,590	44k	INFO	====> Epoch: 1338, cost 27.32 s
2023-12-31 12:37:19,481	44k	INFO	====> Epoch: 1339, cost 26.89 s
2023-12-31 12:37:46,041	44k	INFO	====> Epoch: 1340, cost 26.56 s
2023-12-31 12:38:12,543	44k	INFO	====> Epoch: 1341, cost 26.50 s
2023-12-31 12:38:39,147	44k	INFO	====> Epoch: 1342, cost 26.60 s
2023-12-31 12:39:02,640	44k	INFO	Train Epoch: 1343 [86%]
2023-12-31 12:39:02,641	44k	INFO	Losses: [2.324557304382324, 2.620243787765503, 8.547409057617188, 16.95081329345703, 0.6276296973228455], step: 47000, lr: 8.43550394976729e-05, reference_loss: 31.070653915405273
2023-12-31 12:39:06,148	44k	INFO	====> Epoch: 1343, cost 27.00 s
2023-12-31 12:39:32,989	44k	INFO	====> Epoch: 1344, cost 26.84 s
2023-12-31 12:39:59,661	44k	INFO	====> Epoch: 1345, cost 26.67 s
2023-12-31 12:40:26,602	44k	INFO	====> Epoch: 1346, cost 26.94 s
2023-12-31 12:40:52,978	44k	INFO	====> Epoch: 1347, cost 26.38 s
2023-12-31 12:41:19,256	44k	INFO	====> Epoch: 1348, cost 26.28 s
2023-12-31 12:41:35,185	44k	INFO	Train Epoch: 1349 [57%]
2023-12-31 12:41:35,185	44k	INFO	Losses: [2.4541780948638916, 2.2909889221191406, 7.602505207061768, 17.73097038269043, 0.5499459505081177], step: 47200, lr: 8.429179298546718e-05, reference_loss: 30.62858772277832
2023-12-31 12:41:42,294	44k	INFO	Saving model and optimizer state at iteration 1349 to ./logs/44k/G_47200.pth
2023-12-31 12:41:43,470	44k	INFO	Saving model and optimizer state at iteration 1349 to ./logs/44k/D_47200.pth
2023-12-31 12:41:44,244	44k	INFO	.. Free up space by deleting ckpt ./logs/44k/G_44800.pth
2023-12-31 12:41:44,302	44k	INFO	.. Free up space by deleting ckpt ./logs/44k/D_44800.pth
2023-12-31 12:41:54,878	44k	INFO	====> Epoch: 1349, cost 35.62 s
2023-12-31 12:42:21,509	44k	INFO	====> Epoch: 1350, cost 26.63 s
2023-12-31 12:42:48,065	44k	INFO	====> Epoch: 1351, cost 26.56 s
2023-12-31 12:43:14,983	44k	INFO	====> Epoch: 1352, cost 26.92 s
2023-12-31 12:43:41,476	44k	INFO	====> Epoch: 1353, cost 26.49 s
2023-12-31 12:44:08,023	44k	INFO	====> Epoch: 1354, cost 26.55 s
2023-12-31 12:44:16,491	44k	INFO	Train Epoch: 1355 [29%]
2023-12-31 12:44:16,492	44k	INFO	Losses: [2.531740665435791, 2.13207745552063, 5.9840569496154785, 16.23419189453125, 0.6085293889045715], step: 47400, lr: 8.422859389332469e-05, reference_loss: 27.490596771240234
2023-12-31 12:44:35,185	44k	INFO	====> Epoch: 1355, cost 27.16 s
2023-12-31 12:45:01,618	44k	INFO	====> Epoch: 1356, cost 26.43 s
2023-12-31 12:45:28,209	44k	INFO	====> Epoch: 1357, cost 26.59 s
2023-12-31 12:45:54,630	44k	INFO	====> Epoch: 1358, cost 26.42 s
2023-12-31 12:46:21,226	44k	INFO	====> Epoch: 1359, cost 26.60 s
2023-12-31 12:46:48,023	44k	INFO	====> Epoch: 1360, cost 26.80 s
2023-12-31 12:46:48,805	44k	INFO	Train Epoch: 1361 [0%]
2023-12-31 12:46:48,806	44k	INFO	Losses: [2.533555746078491, 2.1133248805999756, 5.868374347686768, 15.36661434173584, 0.7228589653968811], step: 47600, lr: 8.41654421856915e-05, reference_loss: 26.60472869873047
2023-12-31 12:47:15,159	44k	INFO	====> Epoch: 1361, cost 27.14 s
2023-12-31 12:47:41,779	44k	INFO	====> Epoch: 1362, cost 26.62 s
2023-12-31 12:48:08,370	44k	INFO	====> Epoch: 1363, cost 26.59 s
2023-12-31 12:48:34,975	44k	INFO	====> Epoch: 1364, cost 26.60 s
2023-12-31 12:49:01,480	44k	INFO	====> Epoch: 1365, cost 26.51 s
2023-12-31 12:49:21,311	44k	INFO	Train Epoch: 1366 [71%]
2023-12-31 12:49:21,311	44k	INFO	Losses: [2.3637073040008545, 2.4978690147399902, 8.396346092224121, 18.9813289642334, 0.7951284050941467], step: 47800, lr: 8.411285193353202e-05, reference_loss: 33.03437805175781
2023-12-31 12:49:28,924	44k	INFO	====> Epoch: 1366, cost 27.44 s
2023-12-31 12:49:55,539	44k	INFO	====> Epoch: 1367, cost 26.61 s
2023-12-31 12:50:21,863	44k	INFO	====> Epoch: 1368, cost 26.32 s
2023-12-31 12:50:48,495	44k	INFO	====> Epoch: 1369, cost 26.63 s
2023-12-31 12:51:15,234	44k	INFO	====> Epoch: 1370, cost 26.74 s
2023-12-31 12:51:41,602	44k	INFO	====> Epoch: 1371, cost 26.37 s
2023-12-31 12:51:53,795	44k	INFO	Train Epoch: 1372 [43%]
2023-12-31 12:51:53,795	44k	INFO	Losses: [2.7515125274658203, 1.9784741401672363, 6.124128341674805, 16.60154151916504, 0.5073990821838379], step: 48000, lr: 8.404978700524619e-05, reference_loss: 27.963054656982422
2023-12-31 12:52:01,077	44k	INFO	Saving model and optimizer state at iteration 1372 to ./logs/44k/G_48000.pth
2023-12-31 12:52:02,558	44k	INFO	Saving model and optimizer state at iteration 1372 to ./logs/44k/D_48000.pth
2023-12-31 12:52:03,333	44k	INFO	.. Free up space by deleting ckpt ./logs/44k/G_45600.pth
2023-12-31 12:52:03,390	44k	INFO	.. Free up space by deleting ckpt ./logs/44k/D_45600.pth
2023-12-31 12:52:17,705	44k	INFO	====> Epoch: 1372, cost 36.10 s
2023-12-31 12:52:44,366	44k	INFO	====> Epoch: 1373, cost 26.66 s
2023-12-31 12:53:10,968	44k	INFO	====> Epoch: 1374, cost 26.60 s
2023-12-31 12:53:37,501	44k	INFO	====> Epoch: 1375, cost 26.53 s
2023-12-31 12:54:04,160	44k	INFO	====> Epoch: 1376, cost 26.66 s
2023-12-31 12:54:30,726	44k	INFO	====> Epoch: 1377, cost 26.57 s
2023-12-31 12:54:35,327	44k	INFO	Train Epoch: 1378 [14%]
2023-12-31 12:54:35,327	44k	INFO	Losses: [2.4527699947357178, 2.275148868560791, 6.53073263168335, 18.563016891479492, 0.6106645464897156], step: 48200, lr: 8.398676936087818e-05, reference_loss: 30.43233299255371
2023-12-31 12:54:57,837	44k	INFO	====> Epoch: 1378, cost 27.11 s
2023-12-31 12:55:24,727	44k	INFO	====> Epoch: 1379, cost 26.89 s
2023-12-31 12:55:51,204	44k	INFO	====> Epoch: 1380, cost 26.48 s
2023-12-31 12:56:17,699	44k	INFO	====> Epoch: 1381, cost 26.49 s
2023-12-31 12:56:44,287	44k	INFO	====> Epoch: 1382, cost 26.59 s
2023-12-31 12:57:07,686	44k	INFO	Train Epoch: 1383 [86%]
2023-12-31 12:57:07,687	44k	INFO	Losses: [2.3551645278930664, 2.4559290409088135, 9.265890121459961, 19.03461456298828, 0.8425020575523376], step: 48400, lr: 8.393429075132006e-05, reference_loss: 33.9541015625
2023-12-31 12:57:11,151	44k	INFO	====> Epoch: 1383, cost 26.86 s
2023-12-31 12:57:37,718	44k	INFO	====> Epoch: 1384, cost 26.57 s
2023-12-31 12:58:04,281	44k	INFO	====> Epoch: 1385, cost 26.56 s
2023-12-31 12:58:31,045	44k	INFO	====> Epoch: 1386, cost 26.76 s
2023-12-31 12:58:57,477	44k	INFO	====> Epoch: 1387, cost 26.43 s
2023-12-31 12:59:24,064	44k	INFO	====> Epoch: 1388, cost 26.59 s
2023-12-31 12:59:40,026	44k	INFO	Train Epoch: 1389 [57%]
2023-12-31 12:59:40,027	44k	INFO	Losses: [2.560936212539673, 2.0900447368621826, 4.878104209899902, 14.201488494873047, 0.5955636501312256], step: 48600, lr: 8.387135970207758e-05, reference_loss: 24.32613754272461
2023-12-31 12:59:51,085	44k	INFO	====> Epoch: 1389, cost 27.02 s
2023-12-31 13:00:17,478	44k	INFO	====> Epoch: 1390, cost 26.39 s
2023-12-31 13:00:44,022	44k	INFO	====> Epoch: 1391, cost 26.54 s
2023-12-31 13:01:10,621	44k	INFO	====> Epoch: 1392, cost 26.60 s
2023-12-31 13:01:37,536	44k	INFO	====> Epoch: 1393, cost 26.92 s
2023-12-31 13:02:04,117	44k	INFO	====> Epoch: 1394, cost 26.58 s
2023-12-31 13:02:12,503	44k	INFO	Train Epoch: 1395 [29%]
2023-12-31 13:02:12,504	44k	INFO	Losses: [2.3848323822021484, 2.3887085914611816, 7.637463092803955, 18.626115798950195, 0.86113440990448], step: 48800, lr: 8.380847583637502e-05, reference_loss: 31.89825439453125
2023-12-31 13:02:19,858	44k	INFO	Saving model and optimizer state at iteration 1395 to ./logs/44k/G_48800.pth
2023-12-31 13:02:21,034	44k	INFO	Saving model and optimizer state at iteration 1395 to ./logs/44k/D_48800.pth
2023-12-31 13:02:21,810	44k	INFO	.. Free up space by deleting ckpt ./logs/44k/G_46400.pth
2023-12-31 13:02:21,867	44k	INFO	.. Free up space by deleting ckpt ./logs/44k/D_46400.pth
2023-12-31 13:02:39,966	44k	INFO	====> Epoch: 1395, cost 35.85 s
2023-12-31 13:03:06,505	44k	INFO	====> Epoch: 1396, cost 26.54 s
2023-12-31 13:03:33,012	44k	INFO	====> Epoch: 1397, cost 26.51 s
2023-12-31 13:03:59,737	44k	INFO	====> Epoch: 1398, cost 26.73 s
2023-12-31 13:04:26,590	44k	INFO	====> Epoch: 1399, cost 26.85 s
2023-12-31 13:04:53,219	44k	INFO	====> Epoch: 1400, cost 26.63 s
2023-12-31 13:04:53,997	44k	INFO	Train Epoch: 1401 [0%]
2023-12-31 13:04:53,998	44k	INFO	Losses: [2.4468541145324707, 2.437558889389038, 7.696992874145508, 17.362714767456055, 0.7236554622650146], step: 49000, lr: 8.374563911883578e-05, reference_loss: 30.667776107788086
2023-12-31 13:05:20,209	44k	INFO	====> Epoch: 1401, cost 26.99 s
2023-12-31 13:05:46,750	44k	INFO	====> Epoch: 1402, cost 26.54 s
2023-12-31 13:06:13,336	44k	INFO	====> Epoch: 1403, cost 26.59 s
2023-12-31 13:06:40,120	44k	INFO	====> Epoch: 1404, cost 26.78 s
2023-12-31 13:07:06,796	44k	INFO	====> Epoch: 1405, cost 26.68 s
2023-12-31 13:07:26,771	44k	INFO	Train Epoch: 1406 [71%]
2023-12-31 13:07:26,771	44k	INFO	Losses: [2.5103495121002197, 2.263180732727051, 5.8449225425720215, 15.875825881958008, 0.653788149356842], step: 49200, lr: 8.369331117800705e-05, reference_loss: 27.1480655670166
2023-12-31 13:07:34,338	44k	INFO	====> Epoch: 1406, cost 27.54 s
2023-12-31 13:08:00,964	44k	INFO	====> Epoch: 1407, cost 26.63 s
2023-12-31 13:08:27,561	44k	INFO	====> Epoch: 1408, cost 26.60 s
2023-12-31 13:08:54,252	44k	INFO	====> Epoch: 1409, cost 26.69 s
2023-12-31 13:09:20,911	44k	INFO	====> Epoch: 1410, cost 26.66 s
2023-12-31 13:09:47,418	44k	INFO	====> Epoch: 1411, cost 26.51 s
2023-12-31 13:09:59,638	44k	INFO	Train Epoch: 1412 [43%]
2023-12-31 13:09:59,638	44k	INFO	Losses: [2.597438335418701, 2.2838239669799805, 7.0696539878845215, 16.337385177612305, 0.5640859007835388], step: 49400, lr: 8.363056080697438e-05, reference_loss: 28.852386474609375
2023-12-31 13:10:14,577	44k	INFO	====> Epoch: 1412, cost 27.16 s
2023-12-31 13:10:41,505	44k	INFO	====> Epoch: 1413, cost 26.93 s
2023-12-31 13:11:08,209	44k	INFO	====> Epoch: 1414, cost 26.70 s
2023-12-31 13:11:34,951	44k	INFO	====> Epoch: 1415, cost 26.74 s
2023-12-31 13:12:01,481	44k	INFO	====> Epoch: 1416, cost 26.53 s
2023-12-31 13:12:28,115	44k	INFO	====> Epoch: 1417, cost 26.63 s
2023-12-31 13:12:32,717	44k	INFO	Train Epoch: 1418 [14%]
2023-12-31 13:12:32,717	44k	INFO	Losses: [2.507384777069092, 2.1139657497406006, 6.472679138183594, 18.491430282592773, 0.5388664603233337], step: 49600, lr: 8.35678574840153e-05, reference_loss: 30.124326705932617
2023-12-31 13:12:39,963	44k	INFO	Saving model and optimizer state at iteration 1418 to ./logs/44k/G_49600.pth
2023-12-31 13:12:41,134	44k	INFO	Saving model and optimizer state at iteration 1418 to ./logs/44k/D_49600.pth
2023-12-31 13:12:41,905	44k	INFO	.. Free up space by deleting ckpt ./logs/44k/G_47200.pth
2023-12-31 13:12:41,961	44k	INFO	.. Free up space by deleting ckpt ./logs/44k/D_47200.pth
2023-12-31 13:13:04,143	44k	INFO	====> Epoch: 1418, cost 36.03 s
2023-12-31 13:13:30,805	44k	INFO	====> Epoch: 1419, cost 26.66 s
2023-12-31 13:13:57,236	44k	INFO	====> Epoch: 1420, cost 26.43 s
2023-12-31 13:14:23,899	44k	INFO	====> Epoch: 1421, cost 26.66 s
2023-12-31 13:14:50,524	44k	INFO	====> Epoch: 1422, cost 26.62 s
2023-12-31 13:15:14,139	44k	INFO	Train Epoch: 1423 [86%]
2023-12-31 13:15:14,139	44k	INFO	Losses: [2.4527034759521484, 2.349738597869873, 8.112668991088867, 17.343936920166016, 0.48036161065101624], step: 49800, lr: 8.351564062893342e-05, reference_loss: 30.739410400390625
2023-12-31 13:15:17,651	44k	INFO	====> Epoch: 1423, cost 27.13 s
2023-12-31 13:15:44,194	44k	INFO	====> Epoch: 1424, cost 26.54 s
2023-12-31 13:16:11,055	44k	INFO	====> Epoch: 1425, cost 26.86 s
2023-12-31 13:16:37,729	44k	INFO	====> Epoch: 1426, cost 26.67 s
2023-12-31 13:17:04,567	44k	INFO	====> Epoch: 1427, cost 26.84 s
2023-12-31 13:17:31,198	44k	INFO	====> Epoch: 1428, cost 26.63 s
2023-12-31 13:17:47,222	44k	INFO	Train Epoch: 1429 [57%]
2023-12-31 13:17:47,223	44k	INFO	Losses: [2.471837282180786, 2.2162749767303467, 5.930180072784424, 16.339210510253906, 0.7001039981842041], step: 50000, lr: 8.345302346917795e-05, reference_loss: 27.65760612487793
2023-12-31 13:17:58,387	44k	INFO	====> Epoch: 1429, cost 27.19 s
2023-12-31 13:18:25,209	44k	INFO	====> Epoch: 1430, cost 26.82 s
2023-12-31 13:18:51,829	44k	INFO	====> Epoch: 1431, cost 26.62 s
2023-12-31 13:19:18,482	44k	INFO	====> Epoch: 1432, cost 26.65 s
2023-12-31 13:19:45,191	44k	INFO	====> Epoch: 1433, cost 26.71 s
2023-12-31 13:20:12,034	44k	INFO	====> Epoch: 1434, cost 26.84 s
2023-12-31 13:20:20,494	44k	INFO	Train Epoch: 1435 [29%]
2023-12-31 13:20:20,494	44k	INFO	Losses: [2.5843217372894287, 2.198976516723633, 5.99623966217041, 17.27450180053711, 0.847313642501831], step: 50200, lr: 8.339045325761886e-05, reference_loss: 28.901351928710938
2023-12-31 13:20:39,182	44k	INFO	====> Epoch: 1435, cost 27.15 s
2023-12-31 13:21:05,904	44k	INFO	====> Epoch: 1436, cost 26.72 s
2023-12-31 13:21:32,490	44k	INFO	====> Epoch: 1437, cost 26.59 s
2023-12-31 13:21:59,101	44k	INFO	====> Epoch: 1438, cost 26.61 s
2023-12-31 13:22:25,780	44k	INFO	====> Epoch: 1439, cost 26.68 s
2023-12-31 13:22:52,557	44k	INFO	====> Epoch: 1440, cost 26.78 s
2023-12-31 13:22:53,327	44k	INFO	Train Epoch: 1441 [0%]
2023-12-31 13:22:53,328	44k	INFO	Losses: [2.4626636505126953, 2.2633235454559326, 6.662419319152832, 17.9344482421875, 0.49639150500297546], step: 50400, lr: 8.332792995905598e-05, reference_loss: 29.819246292114258
2023-12-31 13:23:01,195	44k	INFO	Saving model and optimizer state at iteration 1441 to ./logs/44k/G_50400.pth
2023-12-31 13:23:02,391	44k	INFO	Saving model and optimizer state at iteration 1441 to ./logs/44k/D_50400.pth
2023-12-31 13:23:03,171	44k	INFO	.. Free up space by deleting ckpt ./logs/44k/G_48000.pth
2023-12-31 13:23:03,227	44k	INFO	.. Free up space by deleting ckpt ./logs/44k/D_48000.pth
2023-12-31 13:23:29,159	44k	INFO	====> Epoch: 1441, cost 36.60 s
2023-12-31 13:23:55,851	44k	INFO	====> Epoch: 1442, cost 26.69 s
2023-12-31 13:24:22,758	44k	INFO	====> Epoch: 1443, cost 26.91 s
2023-12-31 13:24:49,375	44k	INFO	====> Epoch: 1444, cost 26.62 s
2023-12-31 13:25:16,059	44k	INFO	====> Epoch: 1445, cost 26.68 s
2023-12-31 13:25:35,969	44k	INFO	Train Epoch: 1446 [71%]
2023-12-31 13:25:35,969	44k	INFO	Losses: [2.2949535846710205, 2.38214111328125, 7.893056869506836, 17.074527740478516, 0.5720505714416504], step: 50600, lr: 8.327586302119321e-05, reference_loss: 30.21673011779785
2023-12-31 13:25:43,573	44k	INFO	====> Epoch: 1446, cost 27.51 s
2023-12-31 13:26:10,365	44k	INFO	====> Epoch: 1447, cost 26.79 s
2023-12-31 13:26:36,922	44k	INFO	====> Epoch: 1448, cost 26.56 s
2023-12-31 13:27:03,486	44k	INFO	====> Epoch: 1449, cost 26.56 s
2023-12-31 13:27:30,347	44k	INFO	====> Epoch: 1450, cost 26.86 s
2023-12-31 13:27:57,050	44k	INFO	====> Epoch: 1451, cost 26.70 s
2023-12-31 13:28:09,329	44k	INFO	Train Epoch: 1452 [43%]
2023-12-31 13:28:09,329	44k	INFO	Losses: [2.3492722511291504, 2.402967929840088, 5.881663799285889, 15.31518840789795, 0.5811171531677246], step: 50800, lr: 8.321342563845501e-05, reference_loss: 26.530210494995117
2023-12-31 13:28:24,279	44k	INFO	====> Epoch: 1452, cost 27.23 s
2023-12-31 13:28:51,164	44k	INFO	====> Epoch: 1453, cost 26.89 s
2023-12-31 13:29:17,789	44k	INFO	====> Epoch: 1454, cost 26.63 s
2023-12-31 13:29:44,453	44k	INFO	====> Epoch: 1455, cost 26.66 s
2023-12-31 13:30:10,977	44k	INFO	====> Epoch: 1456, cost 26.52 s
2023-12-31 13:30:37,715	44k	INFO	====> Epoch: 1457, cost 26.74 s
2023-12-31 13:30:42,317	44k	INFO	Train Epoch: 1458 [14%]
2023-12-31 13:30:42,318	44k	INFO	Losses: [2.629427194595337, 1.9916613101959229, 6.236307621002197, 16.87808609008789, 0.7776552438735962], step: 51000, lr: 8.315103506912256e-05, reference_loss: 28.51313591003418
2023-12-31 13:31:04,891	44k	INFO	====> Epoch: 1458, cost 27.18 s
2023-12-31 13:31:31,457	44k	INFO	====> Epoch: 1459, cost 26.57 s
2023-12-31 13:31:58,514	44k	INFO	====> Epoch: 1460, cost 27.06 s
2023-12-31 13:32:24,927	44k	INFO	====> Epoch: 1461, cost 26.41 s
2023-12-31 13:32:51,574	44k	INFO	====> Epoch: 1462, cost 26.65 s
2023-12-31 13:33:15,250	44k	INFO	Train Epoch: 1463 [86%]
2023-12-31 13:33:15,251	44k	INFO	Losses: [2.326218605041504, 2.439601182937622, 6.677862644195557, 16.54450798034668, 0.4560014307498932], step: 51200, lr: 8.309907866292964e-05, reference_loss: 28.444190979003906
2023-12-31 13:33:22,498	44k	INFO	Saving model and optimizer state at iteration 1463 to ./logs/44k/G_51200.pth
2023-12-31 13:33:23,666	44k	INFO	Saving model and optimizer state at iteration 1463 to ./logs/44k/D_51200.pth
2023-12-31 13:33:24,448	44k	INFO	.. Free up space by deleting ckpt ./logs/44k/G_48800.pth
2023-12-31 13:33:24,506	44k	INFO	.. Free up space by deleting ckpt ./logs/44k/D_48800.pth
2023-12-31 13:33:27,533	44k	INFO	====> Epoch: 1463, cost 35.96 s
2023-12-31 13:33:54,234	44k	INFO	====> Epoch: 1464, cost 26.70 s
2023-12-31 13:34:20,934	44k	INFO	====> Epoch: 1465, cost 26.70 s
2023-12-31 13:34:47,983	44k	INFO	====> Epoch: 1466, cost 27.05 s
2023-12-31 13:35:14,721	44k	INFO	====> Epoch: 1467, cost 26.74 s
2023-12-31 13:35:41,101	44k	INFO	====> Epoch: 1468, cost 26.38 s
2023-12-31 13:35:57,076	44k	INFO	Train Epoch: 1469 [57%]
2023-12-31 13:35:57,077	44k	INFO	Losses: [2.7330574989318848, 2.0377120971679688, 4.597408294677734, 14.477208137512207, 0.7316317558288574], step: 51400, lr: 8.303677382703322e-05, reference_loss: 24.57701873779297
2023-12-31 13:36:08,210	44k	INFO	====> Epoch: 1469, cost 27.11 s
2023-12-31 13:36:34,728	44k	INFO	====> Epoch: 1470, cost 26.52 s
2023-12-31 13:37:01,514	44k	INFO	====> Epoch: 1471, cost 26.79 s
2023-12-31 13:37:28,249	44k	INFO	====> Epoch: 1472, cost 26.73 s
2023-12-31 13:37:54,925	44k	INFO	====> Epoch: 1473, cost 26.68 s
2023-12-31 13:38:21,804	44k	INFO	====> Epoch: 1474, cost 26.88 s
2023-12-31 13:38:30,194	44k	INFO	Train Epoch: 1475 [29%]
2023-12-31 13:38:30,195	44k	INFO	Losses: [2.4506750106811523, 2.199030637741089, 6.18349552154541, 16.33249282836914, 0.39452096819877625], step: 51600, lr: 8.297451570516346e-05, reference_loss: 27.560216903686523
2023-12-31 13:38:49,102	44k	INFO	====> Epoch: 1475, cost 27.30 s
2023-12-31 13:39:15,911	44k	INFO	====> Epoch: 1476, cost 26.81 s
2023-12-31 13:39:42,596	44k	INFO	====> Epoch: 1477, cost 26.68 s
2023-12-31 13:40:09,180	44k	INFO	====> Epoch: 1478, cost 26.58 s
2023-12-31 13:40:35,761	44k	INFO	====> Epoch: 1479, cost 26.58 s
2023-12-31 13:41:02,506	44k	INFO	====> Epoch: 1480, cost 26.74 s
2023-12-31 13:41:03,282	44k	INFO	Train Epoch: 1481 [0%]
2023-12-31 13:41:03,283	44k	INFO	Losses: [2.4846670627593994, 2.0817503929138184, 6.321100234985352, 14.46044635772705, 0.228996142745018], step: 51800, lr: 8.29123042622958e-05, reference_loss: 25.576961517333984
2023-12-31 13:41:30,034	44k	INFO	====> Epoch: 1481, cost 27.53 s
2023-12-31 13:41:56,493	44k	INFO	====> Epoch: 1482, cost 26.46 s
2023-12-31 13:42:22,947	44k	INFO	====> Epoch: 1483, cost 26.45 s
2023-12-31 13:42:49,568	44k	INFO	====> Epoch: 1484, cost 26.62 s
2023-12-31 13:43:15,847	44k	INFO	====> Epoch: 1485, cost 26.28 s
2023-12-31 13:43:35,671	44k	INFO	Train Epoch: 1486 [71%]
2023-12-31 13:43:35,672	44k	INFO	Losses: [2.651136875152588, 2.0396230220794678, 4.43123197555542, 13.784953117370605, 0.8158326745033264], step: 52000, lr: 8.28604970255601e-05, reference_loss: 23.722776412963867
2023-12-31 13:43:42,882	44k	INFO	Saving model and optimizer state at iteration 1486 to ./logs/44k/G_52000.pth
2023-12-31 13:43:44,410	44k	INFO	Saving model and optimizer state at iteration 1486 to ./logs/44k/D_52000.pth
2023-12-31 13:43:45,189	44k	INFO	.. Free up space by deleting ckpt ./logs/44k/G_49600.pth
2023-12-31 13:43:45,245	44k	INFO	.. Free up space by deleting ckpt ./logs/44k/D_49600.pth
2023-12-31 13:43:51,940	44k	INFO	====> Epoch: 1486, cost 36.09 s
2023-12-31 13:44:18,665	44k	INFO	====> Epoch: 1487, cost 26.73 s
2023-12-31 13:44:45,427	44k	INFO	====> Epoch: 1488, cost 26.76 s
2023-12-31 13:45:12,199	44k	INFO	====> Epoch: 1489, cost 26.77 s
2023-12-31 13:45:38,873	44k	INFO	====> Epoch: 1490, cost 26.67 s
2023-12-31 13:46:05,533	44k	INFO	====> Epoch: 1491, cost 26.66 s
2023-12-31 13:46:17,696	44k	INFO	Train Epoch: 1492 [43%]
2023-12-31 13:46:17,697	44k	INFO	Losses: [2.625119209289551, 2.287252426147461, 6.1323018074035645, 15.441088676452637, 0.8912165760993958], step: 52200, lr: 8.279837106998346e-05, reference_loss: 27.37697982788086
2023-12-31 13:46:32,684	44k	INFO	====> Epoch: 1492, cost 27.15 s
2023-12-31 13:46:59,628	44k	INFO	====> Epoch: 1493, cost 26.94 s
2023-12-31 13:47:26,248	44k	INFO	====> Epoch: 1494, cost 26.62 s
2023-12-31 13:47:52,616	44k	INFO	====> Epoch: 1495, cost 26.37 s
2023-12-31 13:48:18,999	44k	INFO	====> Epoch: 1496, cost 26.38 s
2023-12-31 13:48:45,372	44k	INFO	====> Epoch: 1497, cost 26.37 s
2023-12-31 13:48:49,944	44k	INFO	Train Epoch: 1498 [14%]
2023-12-31 13:48:49,945	44k	INFO	Losses: [2.5564048290252686, 2.031564950942993, 6.950829982757568, 18.58092498779297, 0.6129838824272156], step: 52400, lr: 8.273629169431515e-05, reference_loss: 30.732707977294922
2023-12-31 13:49:12,406	44k	INFO	====> Epoch: 1498, cost 27.03 s
2023-12-31 13:49:38,866	44k	INFO	====> Epoch: 1499, cost 26.46 s
2023-12-31 13:50:05,591	44k	INFO	====> Epoch: 1500, cost 26.72 s
2023-12-31 13:50:32,064	44k	INFO	====> Epoch: 1501, cost 26.47 s
2023-12-31 13:50:58,443	44k	INFO	====> Epoch: 1502, cost 26.38 s
2023-12-31 13:51:21,971	44k	INFO	Train Epoch: 1503 [86%]
2023-12-31 13:51:21,971	44k	INFO	Losses: [2.35860538482666, 2.52553653717041, 9.100953102111816, 16.571338653564453, 0.6962774395942688], step: 52600, lr: 8.268459443793592e-05, reference_loss: 31.25271224975586
2023-12-31 13:51:25,498	44k	INFO	====> Epoch: 1503, cost 27.06 s
2023-12-31 13:51:52,074	44k	INFO	====> Epoch: 1504, cost 26.58 s
2023-12-31 13:52:18,556	44k	INFO	====> Epoch: 1505, cost 26.48 s
2023-12-31 13:52:45,184	44k	INFO	====> Epoch: 1506, cost 26.63 s
2023-12-31 13:53:11,711	44k	INFO	====> Epoch: 1507, cost 26.53 s
2023-12-31 13:53:38,355	44k	INFO	====> Epoch: 1508, cost 26.64 s
2023-12-31 13:53:54,339	44k	INFO	Train Epoch: 1509 [57%]
2023-12-31 13:53:54,339	44k	INFO	Losses: [2.3678083419799805, 2.4956350326538086, 8.900468826293945, 19.01201820373535, 0.5692237019538879], step: 52800, lr: 8.26226003680797e-05, reference_loss: 33.345157623291016
2023-12-31 13:54:01,589	44k	INFO	Saving model and optimizer state at iteration 1509 to ./logs/44k/G_52800.pth
2023-12-31 13:54:02,761	44k	INFO	Saving model and optimizer state at iteration 1509 to ./logs/44k/D_52800.pth
2023-12-31 13:54:03,535	44k	INFO	.. Free up space by deleting ckpt ./logs/44k/G_50400.pth
2023-12-31 13:54:03,594	44k	INFO	.. Free up space by deleting ckpt ./logs/44k/D_50400.pth
2023-12-31 13:54:14,266	44k	INFO	====> Epoch: 1509, cost 35.91 s
2023-12-31 13:54:41,012	44k	INFO	====> Epoch: 1510, cost 26.75 s
2023-12-31 13:55:07,729	44k	INFO	====> Epoch: 1511, cost 26.72 s
2023-12-31 13:55:34,417	44k	INFO	====> Epoch: 1512, cost 26.69 s
2023-12-31 13:56:01,432	44k	INFO	====> Epoch: 1513, cost 27.02 s
2023-12-31 13:56:28,127	44k	INFO	====> Epoch: 1514, cost 26.69 s
2023-12-31 13:56:36,658	44k	INFO	Train Epoch: 1515 [29%]
2023-12-31 13:56:36,658	44k	INFO	Losses: [2.5292205810546875, 2.0671727657318115, 6.1950249671936035, 16.227508544921875, 0.5563470721244812], step: 53000, lr: 8.256065277924843e-05, reference_loss: 27.575273513793945
2023-12-31 13:56:55,388	44k	INFO	====> Epoch: 1515, cost 27.26 s
2023-12-31 13:57:22,138	44k	INFO	====> Epoch: 1516, cost 26.75 s
2023-12-31 13:57:48,670	44k	INFO	====> Epoch: 1517, cost 26.53 s
2023-12-31 13:58:15,364	44k	INFO	====> Epoch: 1518, cost 26.69 s
2023-12-31 13:58:41,899	44k	INFO	====> Epoch: 1519, cost 26.53 s
2023-12-31 13:59:08,454	44k	INFO	====> Epoch: 1520, cost 26.55 s
2023-12-31 13:59:09,220	44k	INFO	Train Epoch: 1521 [0%]
2023-12-31 13:59:09,220	44k	INFO	Losses: [2.441009998321533, 2.3409698009490967, 6.396549224853516, 15.94528865814209, 0.6380800604820251], step: 53200, lr: 8.249875163659225e-05, reference_loss: 27.761898040771484
2023-12-31 13:59:35,977	44k	INFO	====> Epoch: 1521, cost 27.52 s
2023-12-31 14:00:02,745	44k	INFO	====> Epoch: 1522, cost 26.77 s
2023-12-31 14:00:29,475	44k	INFO	====> Epoch: 1523, cost 26.73 s
2023-12-31 14:00:55,865	44k	INFO	====> Epoch: 1524, cost 26.39 s
2023-12-31 14:01:22,326	44k	INFO	====> Epoch: 1525, cost 26.46 s
2023-12-31 14:01:42,067	44k	INFO	Train Epoch: 1526 [71%]
2023-12-31 14:01:42,068	44k	INFO	Losses: [2.3703207969665527, 2.4693682193756104, 8.379161834716797, 18.14154624938965, 0.7836856245994568], step: 53400, lr: 8.244720280563809e-05, reference_loss: 32.144081115722656
2023-12-31 14:01:49,302	44k	INFO	====> Epoch: 1526, cost 26.98 s
2023-12-31 14:02:16,242	44k	INFO	====> Epoch: 1527, cost 26.94 s
2023-12-31 14:02:42,895	44k	INFO	====> Epoch: 1528, cost 26.65 s
2023-12-31 14:03:09,343	44k	INFO	====> Epoch: 1529, cost 26.45 s
2023-12-31 14:03:36,060	44k	INFO	====> Epoch: 1530, cost 26.72 s
2023-12-31 14:04:02,770	44k	INFO	====> Epoch: 1531, cost 26.71 s
2023-12-31 14:04:14,932	44k	INFO	Train Epoch: 1532 [43%]
2023-12-31 14:04:14,933	44k	INFO	Losses: [2.5395305156707764, 2.149977922439575, 6.811153888702393, 15.876611709594727, 0.5225509405136108], step: 53600, lr: 8.23853867238767e-05, reference_loss: 27.899824142456055
2023-12-31 14:04:22,355	44k	INFO	Saving model and optimizer state at iteration 1532 to ./logs/44k/G_53600.pth
2023-12-31 14:04:23,524	44k	INFO	Saving model and optimizer state at iteration 1532 to ./logs/44k/D_53600.pth
2023-12-31 14:04:24,307	44k	INFO	.. Free up space by deleting ckpt ./logs/44k/G_51200.pth
2023-12-31 14:04:24,365	44k	INFO	.. Free up space by deleting ckpt ./logs/44k/D_51200.pth
2023-12-31 14:04:39,409	44k	INFO	====> Epoch: 1532, cost 36.64 s
2023-12-31 14:05:06,114	44k	INFO	====> Epoch: 1533, cost 26.71 s
2023-12-31 14:05:32,880	44k	INFO	====> Epoch: 1534, cost 26.77 s
2023-12-31 14:05:59,702	44k	INFO	====> Epoch: 1535, cost 26.82 s
2023-12-31 14:06:26,507	44k	INFO	====> Epoch: 1536, cost 26.80 s
2023-12-31 14:06:53,441	44k	INFO	====> Epoch: 1537, cost 26.93 s
2023-12-31 14:06:58,028	44k	INFO	Train Epoch: 1538 [14%]
2023-12-31 14:06:58,028	44k	INFO	Losses: [2.379042387008667, 2.1749415397644043, 6.353246212005615, 17.63387680053711, 0.6246733069419861], step: 53800, lr: 8.232361698969088e-05, reference_loss: 29.165781021118164
2023-12-31 14:07:20,829	44k	INFO	====> Epoch: 1538, cost 27.39 s
2023-12-31 14:07:47,839	44k	INFO	====> Epoch: 1539, cost 27.01 s
2023-12-31 14:08:14,768	44k	INFO	====> Epoch: 1540, cost 26.93 s
2023-12-31 14:08:41,868	44k	INFO	====> Epoch: 1541, cost 27.10 s
2023-12-31 14:09:08,683	44k	INFO	====> Epoch: 1542, cost 26.81 s
2023-12-31 14:09:32,530	44k	INFO	Train Epoch: 1543 [86%]
2023-12-31 14:09:32,531	44k	INFO	Losses: [2.0999038219451904, 2.6651618480682373, 9.924968719482422, 19.09383773803711, 0.8554260730743408], step: 54000, lr: 8.227217759052969e-05, reference_loss: 34.63930130004883
2023-12-31 14:09:36,104	44k	INFO	====> Epoch: 1543, cost 27.42 s
2023-12-31 14:10:03,009	44k	INFO	====> Epoch: 1544, cost 26.90 s
2023-12-31 14:10:29,925	44k	INFO	====> Epoch: 1545, cost 26.92 s
2023-12-31 14:10:56,863	44k	INFO	====> Epoch: 1546, cost 26.94 s
2023-12-31 14:11:23,904	44k	INFO	====> Epoch: 1547, cost 27.04 s
2023-12-31 14:11:50,829	44k	INFO	====> Epoch: 1548, cost 26.93 s
2023-12-31 14:12:07,385	44k	INFO	Train Epoch: 1549 [57%]
2023-12-31 14:12:07,386	44k	INFO	Losses: [2.5407769680023193, 2.1845927238464355, 5.269547462463379, 13.911125183105469, 0.5414302945137024], step: 54200, lr: 8.221049273666493e-05, reference_loss: 24.447471618652344
2023-12-31 14:12:18,551	44k	INFO	====> Epoch: 1549, cost 27.72 s
2023-12-31 14:12:45,296	44k	INFO	====> Epoch: 1550, cost 26.74 s
2023-12-31 14:13:12,398	44k	INFO	====> Epoch: 1551, cost 27.10 s
2023-12-31 14:13:39,264	44k	INFO	====> Epoch: 1552, cost 26.87 s
2023-12-31 14:14:06,032	44k	INFO	====> Epoch: 1553, cost 26.77 s
2023-12-31 14:14:32,957	44k	INFO	====> Epoch: 1554, cost 26.92 s
2023-12-31 14:14:41,573	44k	INFO	Train Epoch: 1555 [29%]
2023-12-31 14:14:41,574	44k	INFO	Losses: [2.395789623260498, 2.3971197605133057, 6.7566938400268555, 16.81673240661621, 0.8876152634620667], step: 54400, lr: 8.21488541319856e-05, reference_loss: 29.253952026367188
2023-12-31 14:14:49,752	44k	INFO	Saving model and optimizer state at iteration 1555 to ./logs/44k/G_54400.pth
2023-12-31 14:14:50,958	44k	INFO	Saving model and optimizer state at iteration 1555 to ./logs/44k/D_54400.pth
2023-12-31 14:14:51,756	44k	INFO	.. Free up space by deleting ckpt ./logs/44k/G_52000.pth
2023-12-31 14:14:51,813	44k	INFO	.. Free up space by deleting ckpt ./logs/44k/D_52000.pth
2023-12-31 14:15:10,151	44k	INFO	====> Epoch: 1555, cost 37.19 s
2023-12-31 14:15:37,070	44k	INFO	====> Epoch: 1556, cost 26.92 s
2023-12-31 14:16:04,010	44k	INFO	====> Epoch: 1557, cost 26.94 s
2023-12-31 14:16:30,688	44k	INFO	====> Epoch: 1558, cost 26.68 s
2023-12-31 14:16:57,507	44k	INFO	====> Epoch: 1559, cost 26.82 s
2023-12-31 14:17:24,370	44k	INFO	====> Epoch: 1560, cost 26.86 s
2023-12-31 14:17:25,144	44k	INFO	Train Epoch: 1561 [0%]
2023-12-31 14:17:25,144	44k	INFO	Losses: [2.3031868934631348, 2.4953155517578125, 7.545124053955078, 16.833404541015625, 0.7097319960594177], step: 54600, lr: 8.208726174181563e-05, reference_loss: 29.886762619018555
2023-12-31 14:17:52,125	44k	INFO	====> Epoch: 1561, cost 27.75 s
2023-12-31 14:18:18,896	44k	INFO	====> Epoch: 1562, cost 26.77 s
2023-12-31 14:18:45,557	44k	INFO	====> Epoch: 1563, cost 26.66 s
2023-12-31 14:19:12,352	44k	INFO	====> Epoch: 1564, cost 26.80 s
2023-12-31 14:19:39,125	44k	INFO	====> Epoch: 1565, cost 26.77 s
2023-12-31 14:19:59,178	44k	INFO	Train Epoch: 1566 [71%]
2023-12-31 14:19:59,178	44k	INFO	Losses: [2.6477441787719727, 2.2010910511016846, 5.924842834472656, 15.503274917602539, 0.5303868055343628], step: 54800, lr: 8.203597002775846e-05, reference_loss: 26.80733871459961
2023-12-31 14:20:06,580	44k	INFO	====> Epoch: 1566, cost 27.46 s
2023-12-31 14:20:33,605	44k	INFO	====> Epoch: 1567, cost 27.02 s
2023-12-31 14:21:00,964	44k	INFO	====> Epoch: 1568, cost 27.36 s
2023-12-31 14:21:27,851	44k	INFO	====> Epoch: 1569, cost 26.89 s
2023-12-31 14:21:54,859	44k	INFO	====> Epoch: 1570, cost 27.01 s
2023-12-31 14:22:21,850	44k	INFO	====> Epoch: 1571, cost 26.99 s
2023-12-31 14:22:34,184	44k	INFO	Train Epoch: 1572 [43%]
2023-12-31 14:22:34,185	44k	INFO	Losses: [2.4548022747039795, 2.2167563438415527, 7.71370267868042, 16.076663970947266, 0.5686144232749939], step: 55000, lr: 8.197446227421386e-05, reference_loss: 29.030540466308594
2023-12-31 14:22:49,342	44k	INFO	====> Epoch: 1572, cost 27.49 s
2023-12-31 14:23:16,183	44k	INFO	====> Epoch: 1573, cost 26.84 s
2023-12-31 14:23:43,054	44k	INFO	====> Epoch: 1574, cost 26.87 s
2023-12-31 14:24:10,188	44k	INFO	====> Epoch: 1575, cost 27.13 s
2023-12-31 14:24:36,898	44k	INFO	====> Epoch: 1576, cost 26.71 s
2023-12-31 14:25:03,897	44k	INFO	====> Epoch: 1577, cost 27.00 s
2023-12-31 14:25:08,517	44k	INFO	Train Epoch: 1578 [14%]
2023-12-31 14:25:08,518	44k	INFO	Losses: [2.408781051635742, 2.286651611328125, 6.921039581298828, 17.65133285522461, 0.5411492586135864], step: 55200, lr: 8.191300063707096e-05, reference_loss: 29.8089542388916
2023-12-31 14:25:16,677	44k	INFO	Saving model and optimizer state at iteration 1578 to ./logs/44k/G_55200.pth
2023-12-31 14:25:17,870	44k	INFO	Saving model and optimizer state at iteration 1578 to ./logs/44k/D_55200.pth
2023-12-31 14:25:18,666	44k	INFO	.. Free up space by deleting ckpt ./logs/44k/G_52800.pth
2023-12-31 14:25:18,722	44k	INFO	.. Free up space by deleting ckpt ./logs/44k/D_52800.pth
2023-12-31 14:25:40,967	44k	INFO	====> Epoch: 1578, cost 37.07 s
2023-12-31 14:26:07,919	44k	INFO	====> Epoch: 1579, cost 26.95 s
2023-12-31 14:26:35,051	44k	INFO	====> Epoch: 1580, cost 27.13 s
2023-12-31 14:27:01,943	44k	INFO	====> Epoch: 1581, cost 26.89 s
2023-12-31 14:27:29,003	44k	INFO	====> Epoch: 1582, cost 27.06 s
2023-12-31 14:27:52,895	44k	INFO	Train Epoch: 1583 [86%]
2023-12-31 14:27:52,896	44k	INFO	Losses: [2.291229009628296, 2.505948066711426, 8.549818992614746, 16.79075050354004, 0.43313488364219666], step: 55400, lr: 8.186181780897936e-05, reference_loss: 30.570880889892578
2023-12-31 14:27:56,610	44k	INFO	====> Epoch: 1583, cost 27.61 s
2023-12-31 14:28:23,600	44k	INFO	====> Epoch: 1584, cost 26.99 s
2023-12-31 14:28:50,622	44k	INFO	====> Epoch: 1585, cost 27.02 s
2023-12-31 14:29:17,482	44k	INFO	====> Epoch: 1586, cost 26.86 s
2023-12-31 14:29:44,501	44k	INFO	====> Epoch: 1587, cost 27.02 s
2023-12-31 14:30:11,403	44k	INFO	====> Epoch: 1588, cost 26.90 s
2023-12-31 14:30:27,929	44k	INFO	Train Epoch: 1589 [57%]
2023-12-31 14:30:27,930	44k	INFO	Losses: [2.6426801681518555, 2.1908724308013916, 6.389236927032471, 16.392885208129883, 0.5562463402748108], step: 55600, lr: 8.180044062878873e-05, reference_loss: 28.171920776367188
2023-12-31 14:30:39,237	44k	INFO	====> Epoch: 1589, cost 27.83 s
2023-12-31 14:31:06,125	44k	INFO	====> Epoch: 1590, cost 26.89 s
2023-12-31 14:31:32,962	44k	INFO	====> Epoch: 1591, cost 26.84 s
2023-12-31 14:31:59,775	44k	INFO	====> Epoch: 1592, cost 26.81 s
2023-12-31 14:32:26,626	44k	INFO	====> Epoch: 1593, cost 26.85 s
2023-12-31 14:32:53,446	44k	INFO	====> Epoch: 1594, cost 26.82 s
2023-12-31 14:33:02,020	44k	INFO	Train Epoch: 1595 [29%]
2023-12-31 14:33:02,021	44k	INFO	Losses: [2.4251809120178223, 2.279385566711426, 6.318336486816406, 17.28361701965332, 0.8123679757118225], step: 55800, lr: 8.173910946710035e-05, reference_loss: 29.11888885498047
2023-12-31 14:33:21,308	44k	INFO	====> Epoch: 1595, cost 27.86 s
2023-12-31 14:33:48,356	44k	INFO	====> Epoch: 1596, cost 27.05 s
2023-12-31 14:34:15,232	44k	INFO	====> Epoch: 1597, cost 26.88 s
2023-12-31 14:34:42,171	44k	INFO	====> Epoch: 1598, cost 26.94 s
2023-12-31 14:35:09,010	44k	INFO	====> Epoch: 1599, cost 26.84 s
2023-12-31 14:35:36,080	44k	INFO	====> Epoch: 1600, cost 27.07 s
2023-12-31 14:35:36,880	44k	INFO	Train Epoch: 1601 [0%]
2023-12-31 14:35:36,881	44k	INFO	Losses: [2.3670735359191895, 2.271993637084961, 6.521387577056885, 16.196937561035156, 0.4628591239452362], step: 56000, lr: 8.167782428941117e-05, reference_loss: 27.820249557495117
2023-12-31 14:35:45,120	44k	INFO	Saving model and optimizer state at iteration 1601 to ./logs/44k/G_56000.pth
2023-12-31 14:35:46,645	44k	INFO	Saving model and optimizer state at iteration 1601 to ./logs/44k/D_56000.pth
2023-12-31 14:35:47,446	44k	INFO	.. Free up space by deleting ckpt ./logs/44k/G_53600.pth
2023-12-31 14:35:47,504	44k	INFO	.. Free up space by deleting ckpt ./logs/44k/D_53600.pth
2023-12-31 14:36:13,436	44k	INFO	====> Epoch: 1601, cost 37.36 s
2023-12-31 14:36:40,303	44k	INFO	====> Epoch: 1602, cost 26.87 s
2023-12-31 14:37:07,377	44k	INFO	====> Epoch: 1603, cost 27.07 s
2023-12-31 14:37:34,295	44k	INFO	====> Epoch: 1604, cost 26.92 s
2023-12-31 14:38:01,386	44k	INFO	====> Epoch: 1605, cost 27.09 s
2023-12-31 14:38:21,456	44k	INFO	Train Epoch: 1606 [71%]
2023-12-31 14:38:21,456	44k	INFO	Losses: [2.3883979320526123, 2.3314049243927, 8.133320808410645, 17.185089111328125, 0.5812371373176575], step: 56200, lr: 8.162678840979515e-05, reference_loss: 30.619449615478516
2023-12-31 14:38:28,816	44k	INFO	====> Epoch: 1606, cost 27.43 s
2023-12-31 14:38:55,788	44k	INFO	====> Epoch: 1607, cost 26.97 s
2023-12-31 14:39:22,983	44k	INFO	====> Epoch: 1608, cost 27.19 s
2023-12-31 14:39:50,024	44k	INFO	====> Epoch: 1609, cost 27.04 s
2023-12-31 14:40:16,918	44k	INFO	====> Epoch: 1610, cost 26.89 s
2023-12-31 14:40:44,022	44k	INFO	====> Epoch: 1611, cost 27.10 s
2023-12-31 14:40:56,416	44k	INFO	Train Epoch: 1612 [43%]
2023-12-31 14:40:56,416	44k	INFO	Losses: [2.358797073364258, 2.3546881675720215, 6.788139343261719, 15.776558876037598, 0.563231348991394], step: 56400, lr: 8.156558744657806e-05, reference_loss: 27.841413497924805
2023-12-31 14:41:11,543	44k	INFO	====> Epoch: 1612, cost 27.52 s
2023-12-31 14:41:38,592	44k	INFO	====> Epoch: 1613, cost 27.05 s
2023-12-31 14:42:05,567	44k	INFO	====> Epoch: 1614, cost 26.97 s
2023-12-31 14:42:32,954	44k	INFO	====> Epoch: 1615, cost 27.39 s
2023-12-31 14:43:00,110	44k	INFO	====> Epoch: 1616, cost 27.16 s
2023-12-31 14:43:27,051	44k	INFO	====> Epoch: 1617, cost 26.94 s
2023-12-31 14:43:31,762	44k	INFO	Train Epoch: 1618 [14%]
2023-12-31 14:43:31,763	44k	INFO	Losses: [2.5740838050842285, 2.167912721633911, 6.543634414672852, 16.884654998779297, 0.7852725386619568], step: 56600, lr: 8.15044323697418e-05, reference_loss: 28.95555877685547
2023-12-31 14:43:54,695	44k	INFO	====> Epoch: 1618, cost 27.64 s
2023-12-31 14:44:21,586	44k	INFO	====> Epoch: 1619, cost 26.89 s
2023-12-31 14:44:48,707	44k	INFO	====> Epoch: 1620, cost 27.12 s
2023-12-31 14:45:15,947	44k	INFO	====> Epoch: 1621, cost 27.24 s
2023-12-31 14:45:43,377	44k	INFO	====> Epoch: 1622, cost 27.43 s
2023-12-31 14:46:07,241	44k	INFO	Train Epoch: 1623 [86%]
2023-12-31 14:46:07,241	44k	INFO	Losses: [2.315260410308838, 2.479651689529419, 6.897364139556885, 16.502796173095703, 0.40228739380836487], step: 56800, lr: 8.145350483298648e-05, reference_loss: 28.59735870361328
2023-12-31 14:46:15,309	44k	INFO	Saving model and optimizer state at iteration 1623 to ./logs/44k/G_56800.pth
2023-12-31 14:46:16,507	44k	INFO	Saving model and optimizer state at iteration 1623 to ./logs/44k/D_56800.pth
2023-12-31 14:46:17,370	44k	INFO	.. Free up space by deleting ckpt ./logs/44k/G_54400.pth
2023-12-31 14:46:17,430	44k	INFO	.. Free up space by deleting ckpt ./logs/44k/D_54400.pth
2023-12-31 14:46:20,494	44k	INFO	====> Epoch: 1623, cost 37.12 s
2023-12-31 14:46:47,347	44k	INFO	====> Epoch: 1624, cost 26.85 s
2023-12-31 14:47:14,305	44k	INFO	====> Epoch: 1625, cost 26.96 s
2023-12-31 14:47:41,368	44k	INFO	====> Epoch: 1626, cost 27.06 s
2023-12-31 14:48:08,355	44k	INFO	====> Epoch: 1627, cost 26.99 s
2023-12-31 14:48:35,665	44k	INFO	====> Epoch: 1628, cost 27.31 s
2023-12-31 14:48:51,809	44k	INFO	Train Epoch: 1629 [57%]
2023-12-31 14:48:51,810	44k	INFO	Losses: [2.645315647125244, 2.1395373344421387, 5.023806095123291, 13.516523361206055, 0.6942556500434875], step: 57000, lr: 8.139243379184544e-05, reference_loss: 24.019437789916992
2023-12-31 14:49:03,115	44k	INFO	====> Epoch: 1629, cost 27.45 s
2023-12-31 14:49:30,151	44k	INFO	====> Epoch: 1630, cost 27.04 s
2023-12-31 14:49:57,267	44k	INFO	====> Epoch: 1631, cost 27.12 s
2023-12-31 14:50:24,395	44k	INFO	====> Epoch: 1632, cost 27.13 s
2023-12-31 14:50:51,431	44k	INFO	====> Epoch: 1633, cost 27.04 s
2023-12-31 14:51:18,249	44k	INFO	====> Epoch: 1634, cost 26.82 s
2023-12-31 14:51:26,838	44k	INFO	Train Epoch: 1635 [29%]
2023-12-31 14:51:26,839	44k	INFO	Losses: [2.5766022205352783, 2.2352585792541504, 6.202106952667236, 16.484174728393555, 0.37397220730781555], step: 57200, lr: 8.133140853967411e-05, reference_loss: 27.872114181518555
2023-12-31 14:51:46,001	44k	INFO	====> Epoch: 1635, cost 27.75 s
2023-12-31 14:52:12,947	44k	INFO	====> Epoch: 1636, cost 26.95 s
2023-12-31 14:52:39,757	44k	INFO	====> Epoch: 1637, cost 26.81 s
2023-12-31 14:53:06,827	44k	INFO	====> Epoch: 1638, cost 27.07 s
2023-12-31 14:53:33,763	44k	INFO	====> Epoch: 1639, cost 26.94 s
2023-12-31 14:54:00,839	44k	INFO	====> Epoch: 1640, cost 27.08 s
2023-12-31 14:54:01,625	44k	INFO	Train Epoch: 1641 [0%]
2023-12-31 14:54:01,626	44k	INFO	Losses: [2.3986053466796875, 2.1839725971221924, 7.154581546783447, 15.61508846282959, 0.2601889669895172], step: 57400, lr: 8.127042904214151e-05, reference_loss: 27.612438201904297
2023-12-31 14:54:28,674	44k	INFO	====> Epoch: 1641, cost 27.83 s
2023-12-31 14:54:55,544	44k	INFO	====> Epoch: 1642, cost 26.87 s
2023-12-31 14:55:22,324	44k	INFO	====> Epoch: 1643, cost 26.78 s
2023-12-31 14:55:49,153	44k	INFO	====> Epoch: 1644, cost 26.83 s
2023-12-31 14:56:16,031	44k	INFO	====> Epoch: 1645, cost 26.88 s
2023-12-31 14:56:36,146	44k	INFO	Train Epoch: 1646 [71%]
2023-12-31 14:56:36,147	44k	INFO	Losses: [2.603022575378418, 2.165806770324707, 5.045209884643555, 14.534584999084473, 0.8368164896965027], step: 57600, lr: 8.121964772090749e-05, reference_loss: 25.185441970825195
2023-12-31 14:56:44,242	44k	INFO	Saving model and optimizer state at iteration 1646 to ./logs/44k/G_57600.pth
2023-12-31 14:56:45,456	44k	INFO	Saving model and optimizer state at iteration 1646 to ./logs/44k/D_57600.pth
2023-12-31 14:56:46,263	44k	INFO	.. Free up space by deleting ckpt ./logs/44k/G_55200.pth
2023-12-31 14:56:46,319	44k	INFO	.. Free up space by deleting ckpt ./logs/44k/D_55200.pth
2023-12-31 14:56:53,162	44k	INFO	====> Epoch: 1646, cost 37.13 s
2023-12-31 14:57:20,318	44k	INFO	====> Epoch: 1647, cost 27.16 s
2023-12-31 14:57:47,257	44k	INFO	====> Epoch: 1648, cost 26.94 s
2023-12-31 14:58:14,209	44k	INFO	====> Epoch: 1649, cost 26.95 s
2023-12-31 14:58:41,156	44k	INFO	====> Epoch: 1650, cost 26.95 s
2023-12-31 14:59:07,982	44k	INFO	====> Epoch: 1651, cost 26.83 s
2023-12-31 14:59:20,326	44k	INFO	Train Epoch: 1652 [43%]
2023-12-31 14:59:20,327	44k	INFO	Losses: [2.5517826080322266, 2.127345085144043, 5.888580799102783, 14.588094711303711, 0.7832539677619934], step: 57800, lr: 8.115875201779938e-05, reference_loss: 25.939058303833008
2023-12-31 14:59:35,409	44k	INFO	====> Epoch: 1652, cost 27.43 s
2023-12-31 15:00:02,382	44k	INFO	====> Epoch: 1653, cost 26.97 s
2023-12-31 15:00:29,257	44k	INFO	====> Epoch: 1654, cost 26.88 s
2023-12-31 15:00:56,165	44k	INFO	====> Epoch: 1655, cost 26.91 s
2023-12-31 15:01:23,535	44k	INFO	====> Epoch: 1656, cost 27.37 s
2023-12-31 15:01:50,660	44k	INFO	====> Epoch: 1657, cost 27.13 s
2023-12-31 15:01:55,329	44k	INFO	Train Epoch: 1658 [14%]
2023-12-31 15:01:55,330	44k	INFO	Losses: [2.4631505012512207, 2.146042585372925, 6.69158411026001, 17.852237701416016, 0.550722599029541], step: 58000, lr: 8.109790197219855e-05, reference_loss: 29.703737258911133
2023-12-31 15:02:18,137	44k	INFO	====> Epoch: 1658, cost 27.48 s
2023-12-31 15:02:45,097	44k	INFO	====> Epoch: 1659, cost 26.96 s
2023-12-31 15:03:12,004	44k	INFO	====> Epoch: 1660, cost 26.91 s
2023-12-31 15:03:38,937	44k	INFO	====> Epoch: 1661, cost 26.93 s
2023-12-31 15:04:05,843	44k	INFO	====> Epoch: 1662, cost 26.91 s
2023-12-31 15:04:30,140	44k	INFO	Train Epoch: 1663 [86%]
2023-12-31 15:04:30,141	44k	INFO	Losses: [2.3832457065582275, 2.543649196624756, 9.539125442504883, 16.148035049438477, 0.5793787240982056], step: 58200, lr: 8.104722845342925e-05, reference_loss: 31.19343376159668
2023-12-31 15:04:33,897	44k	INFO	====> Epoch: 1663, cost 28.05 s
2023-12-31 15:05:00,839	44k	INFO	====> Epoch: 1664, cost 26.94 s
2023-12-31 15:05:27,926	44k	INFO	====> Epoch: 1665, cost 27.09 s
2023-12-31 15:05:55,030	44k	INFO	====> Epoch: 1666, cost 27.10 s
2023-12-31 15:06:22,086	44k	INFO	====> Epoch: 1667, cost 27.06 s
2023-12-31 15:06:49,208	44k	INFO	====> Epoch: 1668, cost 27.12 s
2023-12-31 15:07:05,455	44k	INFO	Train Epoch: 1669 [57%]
2023-12-31 15:07:05,455	44k	INFO	Losses: [2.397348165512085, 2.398712396621704, 8.450931549072266, 17.26560401916504, 0.5162420272827148], step: 58400, lr: 8.098646202436773e-05, reference_loss: 31.028839111328125
2023-12-31 15:07:14,057	44k	INFO	Saving model and optimizer state at iteration 1669 to ./logs/44k/G_58400.pth
2023-12-31 15:07:15,275	44k	INFO	Saving model and optimizer state at iteration 1669 to ./logs/44k/D_58400.pth
2023-12-31 15:07:16,094	44k	INFO	.. Free up space by deleting ckpt ./logs/44k/G_56000.pth
2023-12-31 15:07:16,152	44k	INFO	.. Free up space by deleting ckpt ./logs/44k/D_56000.pth
2023-12-31 15:07:26,947	44k	INFO	====> Epoch: 1669, cost 37.74 s
2023-12-31 15:07:54,035	44k	INFO	====> Epoch: 1670, cost 27.09 s
2023-12-31 15:08:21,037	44k	INFO	====> Epoch: 1671, cost 27.00 s
2023-12-31 15:08:47,835	44k	INFO	====> Epoch: 1672, cost 26.80 s
2023-12-31 15:09:14,738	44k	INFO	====> Epoch: 1673, cost 26.90 s
2023-12-31 15:09:41,521	44k	INFO	====> Epoch: 1674, cost 26.78 s
2023-12-31 15:09:49,973	44k	INFO	Train Epoch: 1675 [29%]
2023-12-31 15:09:49,974	44k	INFO	Losses: [2.5166943073272705, 2.3249058723449707, 7.066347122192383, 16.463449478149414, 0.5700104832649231], step: 58600, lr: 8.092574115588822e-05, reference_loss: 28.941408157348633
2023-12-31 15:10:09,405	44k	INFO	====> Epoch: 1675, cost 27.88 s
2023-12-31 15:10:36,450	44k	INFO	====> Epoch: 1676, cost 27.05 s
2023-12-31 15:11:03,503	44k	INFO	====> Epoch: 1677, cost 27.05 s
2023-12-31 15:11:30,511	44k	INFO	====> Epoch: 1678, cost 27.01 s
2023-12-31 15:11:57,606	44k	INFO	====> Epoch: 1679, cost 27.09 s
2023-12-31 15:12:24,518	44k	INFO	====> Epoch: 1680, cost 26.91 s
2023-12-31 15:12:25,317	44k	INFO	Train Epoch: 1681 [0%]
2023-12-31 15:12:25,318	44k	INFO	Losses: [2.49540638923645, 2.219139814376831, 6.891810417175293, 15.981424331665039, 0.6304011940956116], step: 58800, lr: 8.086506581383101e-05, reference_loss: 28.218181610107422
2023-12-31 15:12:52,238	44k	INFO	====> Epoch: 1681, cost 27.72 s
2023-12-31 15:13:19,511	44k	INFO	====> Epoch: 1682, cost 27.27 s
2023-12-31 15:13:46,563	44k	INFO	====> Epoch: 1683, cost 27.05 s
2023-12-31 15:14:13,521	44k	INFO	====> Epoch: 1684, cost 26.96 s
2023-12-31 15:14:40,616	44k	INFO	====> Epoch: 1685, cost 27.10 s
2023-12-31 15:15:00,700	44k	INFO	Train Epoch: 1686 [71%]
2023-12-31 15:15:00,701	44k	INFO	Losses: [2.329364538192749, 2.4447295665740967, 8.797919273376465, 18.414670944213867, 0.7399705052375793], step: 59000, lr: 8.081453778128458e-05, reference_loss: 32.726654052734375
2023-12-31 15:15:08,127	44k	INFO	====> Epoch: 1686, cost 27.51 s
2023-12-31 15:15:35,146	44k	INFO	====> Epoch: 1687, cost 27.02 s
2023-12-31 15:16:02,148	44k	INFO	====> Epoch: 1688, cost 27.00 s
2023-12-31 15:16:29,646	44k	INFO	====> Epoch: 1689, cost 27.50 s
2023-12-31 15:16:56,836	44k	INFO	====> Epoch: 1690, cost 27.19 s
2023-12-31 15:17:23,903	44k	INFO	====> Epoch: 1691, cost 27.07 s
2023-12-31 15:17:36,358	44k	INFO	Train Epoch: 1692 [43%]
2023-12-31 15:17:36,359	44k	INFO	Losses: [2.637810707092285, 2.089797258377075, 7.156919956207275, 15.812908172607422, 0.4032593369483948], step: 59200, lr: 8.075394581569939e-05, reference_loss: 28.10069465637207
2023-12-31 15:17:44,671	44k	INFO	Saving model and optimizer state at iteration 1692 to ./logs/44k/G_59200.pth
2023-12-31 15:17:45,872	44k	INFO	Saving model and optimizer state at iteration 1692 to ./logs/44k/D_59200.pth
2023-12-31 15:17:46,681	44k	INFO	.. Free up space by deleting ckpt ./logs/44k/G_56800.pth
2023-12-31 15:17:46,738	44k	INFO	.. Free up space by deleting ckpt ./logs/44k/D_56800.pth
2023-12-31 15:18:01,480	44k	INFO	====> Epoch: 1692, cost 37.58 s
2023-12-31 15:18:28,643	44k	INFO	====> Epoch: 1693, cost 27.16 s
2023-12-31 15:18:55,732	44k	INFO	====> Epoch: 1694, cost 27.09 s
2023-12-31 15:19:23,229	44k	INFO	====> Epoch: 1695, cost 27.50 s
2023-12-31 15:19:50,165	44k	INFO	====> Epoch: 1696, cost 26.94 s
2023-12-31 15:20:17,050	44k	INFO	====> Epoch: 1697, cost 26.88 s
2023-12-31 15:20:21,765	44k	INFO	Train Epoch: 1698 [14%]
2023-12-31 15:20:21,766	44k	INFO	Losses: [2.4839982986450195, 2.096851110458374, 5.990529537200928, 17.548542022705078, 0.6030272841453552], step: 59400, lr: 8.06933992798895e-05, reference_loss: 28.72294807434082
2023-12-31 15:20:44,674	44k	INFO	====> Epoch: 1698, cost 27.62 s
2023-12-31 15:21:11,648	44k	INFO	====> Epoch: 1699, cost 26.97 s
2023-12-31 15:21:38,864	44k	INFO	====> Epoch: 1700, cost 27.22 s
2023-12-31 15:22:05,703	44k	INFO	====> Epoch: 1701, cost 26.84 s
2023-12-31 15:22:32,863	44k	INFO	====> Epoch: 1702, cost 27.16 s
2023-12-31 15:22:57,187	44k	INFO	Train Epoch: 1703 [86%]
2023-12-31 15:22:57,188	44k	INFO	Losses: [2.1136248111724854, 2.700575351715088, 10.187690734863281, 18.732324600219727, 0.8888774514198303], step: 59600, lr: 8.064297851210724e-05, reference_loss: 34.62309265136719
2023-12-31 15:23:00,823	44k	INFO	====> Epoch: 1703, cost 27.96 s
2023-12-31 15:23:27,934	44k	INFO	====> Epoch: 1704, cost 27.11 s
2023-12-31 15:23:55,193	44k	INFO	====> Epoch: 1705, cost 27.26 s
2023-12-31 15:24:22,345	44k	INFO	====> Epoch: 1706, cost 27.15 s
2023-12-31 15:24:49,363	44k	INFO	====> Epoch: 1707, cost 27.02 s
2023-12-31 15:25:16,368	44k	INFO	====> Epoch: 1708, cost 27.01 s
2023-12-31 15:25:32,619	44k	INFO	Train Epoch: 1709 [57%]
2023-12-31 15:25:32,620	44k	INFO	Losses: [2.6138579845428467, 2.1616950035095215, 5.838167667388916, 14.787611961364746, 0.5027626156806946], step: 59800, lr: 8.058251517577144e-05, reference_loss: 25.904094696044922
2023-12-31 15:25:44,299	44k	INFO	====> Epoch: 1709, cost 27.93 s
2023-12-31 15:26:11,204	44k	INFO	====> Epoch: 1710, cost 26.91 s
2023-12-31 15:26:38,235	44k	INFO	====> Epoch: 1711, cost 27.03 s
2023-12-31 15:27:05,369	44k	INFO	====> Epoch: 1712, cost 27.13 s
2023-12-31 15:27:32,511	44k	INFO	====> Epoch: 1713, cost 27.14 s
2023-12-31 15:27:59,365	44k	INFO	====> Epoch: 1714, cost 26.85 s
2023-12-31 15:28:07,942	44k	INFO	Train Epoch: 1715 [29%]
2023-12-31 15:28:07,943	44k	INFO	Losses: [2.3767168521881104, 2.373964309692383, 7.796932697296143, 17.659141540527344, 0.8160154223442078], step: 60000, lr: 8.052209717276913e-05, reference_loss: 31.022769927978516
2023-12-31 15:28:16,334	44k	INFO	Saving model and optimizer state at iteration 1715 to ./logs/44k/G_60000.pth
2023-12-31 15:28:17,900	44k	INFO	Saving model and optimizer state at iteration 1715 to ./logs/44k/D_60000.pth
2023-12-31 15:28:18,709	44k	INFO	.. Free up space by deleting ckpt ./logs/44k/G_57600.pth
2023-12-31 15:28:18,766	44k	INFO	.. Free up space by deleting ckpt ./logs/44k/D_57600.pth
2023-12-31 15:28:37,005	44k	INFO	====> Epoch: 1715, cost 37.64 s
2023-12-31 15:29:04,034	44k	INFO	====> Epoch: 1716, cost 27.03 s
2023-12-31 15:29:30,992	44k	INFO	====> Epoch: 1717, cost 26.96 s
2023-12-31 15:29:58,111	44k	INFO	====> Epoch: 1718, cost 27.12 s
2023-12-31 15:30:24,918	44k	INFO	====> Epoch: 1719, cost 26.81 s
2023-12-31 15:30:52,083	44k	INFO	====> Epoch: 1720, cost 27.17 s
2023-12-31 15:30:52,876	44k	INFO	Train Epoch: 1721 [0%]
2023-12-31 15:30:52,877	44k	INFO	Losses: [2.287276268005371, 2.4687256813049316, 8.322789192199707, 17.038768768310547, 0.6636207103729248], step: 60200, lr: 8.046172446911096e-05, reference_loss: 30.781179428100586
2023-12-31 15:31:19,813	44k	INFO	====> Epoch: 1721, cost 27.73 s
2023-12-31 15:31:47,020	44k	INFO	====> Epoch: 1722, cost 27.21 s
2023-12-31 15:32:14,116	44k	INFO	====> Epoch: 1723, cost 27.10 s
2023-12-31 15:32:41,196	44k	INFO	====> Epoch: 1724, cost 27.08 s
2023-12-31 15:33:08,230	44k	INFO	====> Epoch: 1725, cost 27.03 s
2023-12-31 15:33:28,279	44k	INFO	Train Epoch: 1726 [71%]
2023-12-31 15:33:28,280	44k	INFO	Losses: [2.6199698448181152, 2.214841365814209, 6.328884601593018, 15.962300300598145, 0.6251024007797241], step: 60400, lr: 8.041144846189078e-05, reference_loss: 27.7510986328125
2023-12-31 15:33:35,697	44k	INFO	====> Epoch: 1726, cost 27.47 s
2023-12-31 15:34:02,611	44k	INFO	====> Epoch: 1727, cost 26.91 s
2023-12-31 15:34:29,711	44k	INFO	====> Epoch: 1728, cost 27.10 s
2023-12-31 15:34:57,141	44k	INFO	====> Epoch: 1729, cost 27.43 s
2023-12-31 15:35:24,210	44k	INFO	====> Epoch: 1730, cost 27.07 s
2023-12-31 15:35:51,412	44k	INFO	====> Epoch: 1731, cost 27.20 s
2023-12-31 15:36:03,784	44k	INFO	Train Epoch: 1732 [43%]
2023-12-31 15:36:03,785	44k	INFO	Losses: [2.484565258026123, 2.277261257171631, 7.612672328948975, 16.306821823120117, 0.6663329601287842], step: 60600, lr: 8.03511587188368e-05, reference_loss: 29.347652435302734
2023-12-31 15:36:18,943	44k	INFO	====> Epoch: 1732, cost 27.53 s
2023-12-31 15:36:45,937	44k	INFO	====> Epoch: 1733, cost 26.99 s
2023-12-31 15:37:12,976	44k	INFO	====> Epoch: 1734, cost 27.04 s
2023-12-31 15:37:40,064	44k	INFO	====> Epoch: 1735, cost 27.09 s
2023-12-31 15:38:07,139	44k	INFO	====> Epoch: 1736, cost 27.07 s
2023-12-31 15:38:34,434	44k	INFO	====> Epoch: 1737, cost 27.30 s
2023-12-31 15:38:39,078	44k	INFO	Train Epoch: 1738 [14%]
2023-12-31 15:38:39,079	44k	INFO	Losses: [2.3905558586120605, 2.3773703575134277, 8.089696884155273, 18.819971084594727, 0.5150259137153625], step: 60800, lr: 8.029091417896206e-05, reference_loss: 32.19261932373047
2023-12-31 15:38:47,465	44k	INFO	Saving model and optimizer state at iteration 1738 to ./logs/44k/G_60800.pth
2023-12-31 15:38:48,687	44k	INFO	Saving model and optimizer state at iteration 1738 to ./logs/44k/D_60800.pth
2023-12-31 15:38:49,500	44k	INFO	.. Free up space by deleting ckpt ./logs/44k/G_58400.pth
2023-12-31 15:38:49,559	44k	INFO	.. Free up space by deleting ckpt ./logs/44k/D_58400.pth
2023-12-31 15:39:11,622	44k	INFO	====> Epoch: 1738, cost 37.19 s
2023-12-31 15:39:38,783	44k	INFO	====> Epoch: 1739, cost 27.16 s
2023-12-31 15:40:05,916	44k	INFO	====> Epoch: 1740, cost 27.13 s
2023-12-31 15:40:33,026	44k	INFO	====> Epoch: 1741, cost 27.11 s
2023-12-31 15:41:00,474	44k	INFO	====> Epoch: 1742, cost 27.45 s
2023-12-31 15:41:24,460	44k	INFO	Train Epoch: 1743 [86%]
2023-12-31 15:41:24,461	44k	INFO	Losses: [2.211945056915283, 2.754077196121216, 8.929303169250488, 16.629169464111328, 0.49035894870758057], step: 61000, lr: 8.024074490148745e-05, reference_loss: 31.01485252380371
2023-12-31 15:41:28,050	44k	INFO	====> Epoch: 1743, cost 27.58 s
2023-12-31 15:41:55,150	44k	INFO	====> Epoch: 1744, cost 27.10 s
2023-12-31 15:42:22,283	44k	INFO	====> Epoch: 1745, cost 27.13 s
2023-12-31 15:42:49,417	44k	INFO	====> Epoch: 1746, cost 27.13 s
2023-12-31 15:43:16,575	44k	INFO	====> Epoch: 1747, cost 27.16 s
2023-12-31 15:43:43,727	44k	INFO	====> Epoch: 1748, cost 27.15 s
2023-12-31 15:44:00,059	44k	INFO	Train Epoch: 1749 [57%]
2023-12-31 15:44:00,060	44k	INFO	Losses: [2.5776166915893555, 2.275937080383301, 6.737374782562256, 16.110069274902344, 0.5814889669418335], step: 61200, lr: 8.01805831461018e-05, reference_loss: 28.282487869262695
2023-12-31 15:44:11,764	44k	INFO	====> Epoch: 1749, cost 28.04 s
2023-12-31 15:44:38,711	44k	INFO	====> Epoch: 1750, cost 26.95 s
2023-12-31 15:45:05,707	44k	INFO	====> Epoch: 1751, cost 27.00 s
2023-12-31 15:45:32,813	44k	INFO	====> Epoch: 1752, cost 27.11 s
2023-12-31 15:45:59,793	44k	INFO	====> Epoch: 1753, cost 26.98 s
2023-12-31 15:46:26,521	44k	INFO	====> Epoch: 1754, cost 26.73 s
2023-12-31 15:46:35,071	44k	INFO	Train Epoch: 1755 [29%]
2023-12-31 15:46:35,072	44k	INFO	Losses: [2.539646863937378, 2.15621280670166, 6.951637268066406, 17.835397720336914, 0.7765624523162842], step: 61400, lr: 8.012046649793461e-05, reference_loss: 30.259456634521484
2023-12-31 15:46:53,807	44k	INFO	====> Epoch: 1755, cost 27.29 s
2023-12-31 15:47:21,132	44k	INFO	====> Epoch: 1756, cost 27.32 s
2023-12-31 15:47:48,231	44k	INFO	====> Epoch: 1757, cost 27.10 s
2023-12-31 15:48:15,271	44k	INFO	====> Epoch: 1758, cost 27.04 s
2023-12-31 15:48:42,127	44k	INFO	====> Epoch: 1759, cost 26.86 s
2023-12-31 15:49:09,244	44k	INFO	====> Epoch: 1760, cost 27.12 s
2023-12-31 15:49:10,035	44k	INFO	Train Epoch: 1761 [0%]
2023-12-31 15:49:10,036	44k	INFO	Losses: [2.3620400428771973, 2.3561415672302246, 6.994775772094727, 17.228641510009766, 0.5171669125556946], step: 61600, lr: 8.006039492316607e-05, reference_loss: 29.458765029907227
2023-12-31 15:49:18,537	44k	INFO	Saving model and optimizer state at iteration 1761 to ./logs/44k/G_61600.pth
2023-12-31 15:49:19,733	44k	INFO	Saving model and optimizer state at iteration 1761 to ./logs/44k/D_61600.pth
2023-12-31 15:49:20,558	44k	INFO	.. Free up space by deleting ckpt ./logs/44k/G_59200.pth
2023-12-31 15:49:20,615	44k	INFO	.. Free up space by deleting ckpt ./logs/44k/D_59200.pth
2023-12-31 15:49:47,164	44k	INFO	====> Epoch: 1761, cost 37.92 s
2023-12-31 15:50:14,046	44k	INFO	====> Epoch: 1762, cost 26.88 s
2023-12-31 15:50:41,017	44k	INFO	====> Epoch: 1763, cost 26.97 s
2023-12-31 15:51:07,884	44k	INFO	====> Epoch: 1764, cost 26.87 s
2023-12-31 15:51:34,753	44k	INFO	====> Epoch: 1765, cost 26.87 s
2023-12-31 15:51:54,783	44k	INFO	Train Epoch: 1766 [71%]
2023-12-31 15:51:54,784	44k	INFO	Losses: [2.4038078784942627, 2.49226975440979, 8.863960266113281, 17.48595428466797, 0.5365117192268372], step: 61800, lr: 8.00103696842122e-05, reference_loss: 31.78250503540039
2023-12-31 15:52:02,226	44k	INFO	====> Epoch: 1766, cost 27.47 s
2023-12-31 15:52:29,375	44k	INFO	====> Epoch: 1767, cost 27.15 s
2023-12-31 15:52:56,438	44k	INFO	====> Epoch: 1768, cost 27.06 s
2023-12-31 15:53:23,290	44k	INFO	====> Epoch: 1769, cost 26.85 s
2023-12-31 15:53:50,517	44k	INFO	====> Epoch: 1770, cost 27.23 s
2023-12-31 15:54:17,427	44k	INFO	====> Epoch: 1771, cost 26.91 s
2023-12-31 15:54:29,790	44k	INFO	Train Epoch: 1772 [43%]
2023-12-31 15:54:29,791	44k	INFO	Losses: [2.410494089126587, 2.343571662902832, 6.882706642150879, 14.988767623901367, 0.554450273513794], step: 62000, lr: 7.995038065625431e-05, reference_loss: 27.179990768432617
2023-12-31 15:54:45,053	44k	INFO	====> Epoch: 1772, cost 27.63 s
2023-12-31 15:55:11,954	44k	INFO	====> Epoch: 1773, cost 26.90 s
2023-12-31 15:55:38,680	44k	INFO	====> Epoch: 1774, cost 26.73 s
2023-12-31 15:56:05,566	44k	INFO	====> Epoch: 1775, cost 26.89 s
2023-12-31 15:56:32,650	44k	INFO	====> Epoch: 1776, cost 27.08 s
2023-12-31 15:56:59,899	44k	INFO	====> Epoch: 1777, cost 27.25 s
2023-12-31 15:57:04,548	44k	INFO	Train Epoch: 1778 [14%]
2023-12-31 15:57:04,549	44k	INFO	Losses: [2.5412161350250244, 2.265794038772583, 6.907576084136963, 16.977582931518555, 0.747677206993103], step: 62200, lr: 7.98904366060098e-05, reference_loss: 29.43984603881836
2023-12-31 15:57:27,388	44k	INFO	====> Epoch: 1778, cost 27.49 s
2023-12-31 15:57:54,455	44k	INFO	====> Epoch: 1779, cost 27.07 s
2023-12-31 15:58:21,513	44k	INFO	====> Epoch: 1780, cost 27.06 s
2023-12-31 15:58:48,684	44k	INFO	====> Epoch: 1781, cost 27.17 s
2023-12-31 15:59:15,778	44k	INFO	====> Epoch: 1782, cost 27.09 s
2023-12-31 15:59:39,750	44k	INFO	Train Epoch: 1783 [86%]
2023-12-31 15:59:39,751	44k	INFO	Losses: [2.3326985836029053, 2.4436941146850586, 7.4316020011901855, 16.28290557861328, 0.4066747725009918], step: 62400, lr: 7.984051756445148e-05, reference_loss: 28.89757537841797
2023-12-31 15:59:48,509	44k	INFO	Saving model and optimizer state at iteration 1783 to ./logs/44k/G_62400.pth
2023-12-31 15:59:49,734	44k	INFO	Saving model and optimizer state at iteration 1783 to ./logs/44k/D_62400.pth
2023-12-31 15:59:50,554	44k	INFO	.. Free up space by deleting ckpt ./logs/44k/G_60000.pth
2023-12-31 15:59:50,610	44k	INFO	.. Free up space by deleting ckpt ./logs/44k/D_60000.pth
2023-12-31 15:59:53,698	44k	INFO	====> Epoch: 1783, cost 37.92 s
2023-12-31 16:00:20,762	44k	INFO	====> Epoch: 1784, cost 27.06 s
2023-12-31 16:00:47,880	44k	INFO	====> Epoch: 1785, cost 27.12 s
2023-12-31 16:01:14,950	44k	INFO	====> Epoch: 1786, cost 27.07 s
2023-12-31 16:01:41,921	44k	INFO	====> Epoch: 1787, cost 26.97 s
2023-12-31 16:02:08,825	44k	INFO	====> Epoch: 1788, cost 26.90 s
2023-12-31 16:02:25,121	44k	INFO	Train Epoch: 1789 [57%]
2023-12-31 16:02:25,122	44k	INFO	Losses: [2.708735227584839, 1.9896223545074463, 4.548862934112549, 12.627767562866211, 0.664836049079895], step: 62600, lr: 7.978065588578095e-05, reference_loss: 22.539825439453125
2023-12-31 16:02:36,751	44k	INFO	====> Epoch: 1789, cost 27.93 s
2023-12-31 16:03:03,648	44k	INFO	====> Epoch: 1790, cost 26.90 s
2023-12-31 16:03:30,817	44k	INFO	====> Epoch: 1791, cost 27.17 s
2023-12-31 16:03:57,919	44k	INFO	====> Epoch: 1792, cost 27.10 s
2023-12-31 16:04:24,806	44k	INFO	====> Epoch: 1793, cost 26.89 s
2023-12-31 16:04:51,961	44k	INFO	====> Epoch: 1794, cost 27.15 s
2023-12-31 16:05:00,486	44k	INFO	Train Epoch: 1795 [29%]
2023-12-31 16:05:00,487	44k	INFO	Losses: [2.50260853767395, 2.3888702392578125, 6.485600471496582, 15.479879379272461, 0.3499191105365753], step: 62800, lr: 7.972083908934167e-05, reference_loss: 27.206876754760742
2023-12-31 16:05:19,454	44k	INFO	====> Epoch: 1795, cost 27.49 s
2023-12-31 16:05:46,808	44k	INFO	====> Epoch: 1796, cost 27.35 s
2023-12-31 16:06:13,616	44k	INFO	====> Epoch: 1797, cost 26.81 s
2023-12-31 16:06:40,653	44k	INFO	====> Epoch: 1798, cost 27.04 s
2023-12-31 16:07:07,511	44k	INFO	====> Epoch: 1799, cost 26.86 s
2023-12-31 16:07:34,533	44k	INFO	====> Epoch: 1800, cost 27.02 s
2023-12-31 16:07:35,332	44k	INFO	Train Epoch: 1801 [0%]
2023-12-31 16:07:35,332	44k	INFO	Losses: [2.3739511966705322, 2.3890128135681152, 7.336015701293945, 14.369301795959473, 0.2662293314933777], step: 63000, lr: 7.96610671414825e-05, reference_loss: 26.73451042175293
2023-12-31 16:08:02,232	44k	INFO	====> Epoch: 1801, cost 27.70 s
2023-12-31 16:08:29,211	44k	INFO	====> Epoch: 1802, cost 26.98 s
2023-12-31 16:08:56,448	44k	INFO	====> Epoch: 1803, cost 27.24 s
2023-12-31 16:09:23,706	44k	INFO	====> Epoch: 1804, cost 27.26 s
2023-12-31 16:09:50,779	44k	INFO	====> Epoch: 1805, cost 27.07 s
2023-12-31 16:10:10,891	44k	INFO	Train Epoch: 1806 [71%]
2023-12-31 16:10:10,892	44k	INFO	Losses: [2.6300277709960938, 2.1650424003601074, 4.952639102935791, 13.900959968566895, 0.8155522346496582], step: 63200, lr: 7.9611291420005e-05, reference_loss: 24.464221954345703
2023-12-31 16:10:19,390	44k	INFO	Saving model and optimizer state at iteration 1806 to ./logs/44k/G_63200.pth
2023-12-31 16:10:20,613	44k	INFO	Saving model and optimizer state at iteration 1806 to ./logs/44k/D_63200.pth
2023-12-31 16:10:21,428	44k	INFO	.. Free up space by deleting ckpt ./logs/44k/G_60800.pth
2023-12-31 16:10:21,485	44k	INFO	.. Free up space by deleting ckpt ./logs/44k/D_60800.pth
2023-12-31 16:10:28,357	44k	INFO	====> Epoch: 1806, cost 37.58 s
2023-12-31 16:10:55,309	44k	INFO	====> Epoch: 1807, cost 26.95 s
2023-12-31 16:11:22,267	44k	INFO	====> Epoch: 1808, cost 26.96 s
2023-12-31 16:11:49,625	44k	INFO	====> Epoch: 1809, cost 27.36 s
2023-12-31 16:12:16,542	44k	INFO	====> Epoch: 1810, cost 26.92 s
2023-12-31 16:12:43,471	44k	INFO	====> Epoch: 1811, cost 26.93 s
2023-12-31 16:12:55,780	44k	INFO	Train Epoch: 1812 [43%]
2023-12-31 16:12:55,781	44k	INFO	Losses: [2.5647900104522705, 2.1633243560791016, 5.972996234893799, 14.782397270202637, 0.7694672346115112], step: 63400, lr: 7.955160160722687e-05, reference_loss: 26.252975463867188
2023-12-31 16:13:10,750	44k	INFO	====> Epoch: 1812, cost 27.28 s
2023-12-31 16:13:37,815	44k	INFO	====> Epoch: 1813, cost 27.06 s
2023-12-31 16:14:04,824	44k	INFO	====> Epoch: 1814, cost 27.01 s
2023-12-31 16:14:31,938	44k	INFO	====> Epoch: 1815, cost 27.11 s
2023-12-31 16:14:59,022	44k	INFO	====> Epoch: 1816, cost 27.08 s
2023-12-31 16:15:26,440	44k	INFO	====> Epoch: 1817, cost 27.42 s
2023-12-31 16:15:31,127	44k	INFO	Train Epoch: 1818 [14%]
2023-12-31 16:15:31,128	44k	INFO	Losses: [2.421581983566284, 2.4200923442840576, 8.19888687133789, 18.76303482055664, 0.541892409324646], step: 63600, lr: 7.949195654782087e-05, reference_loss: 32.345489501953125
2023-12-31 16:15:54,134	44k	INFO	====> Epoch: 1818, cost 27.69 s
2023-12-31 16:16:21,162	44k	INFO	====> Epoch: 1819, cost 27.03 s
2023-12-31 16:16:48,226	44k	INFO	====> Epoch: 1820, cost 27.06 s
2023-12-31 16:17:15,437	44k	INFO	====> Epoch: 1821, cost 27.21 s
2023-12-31 16:17:42,568	44k	INFO	====> Epoch: 1822, cost 27.13 s
2023-12-31 16:18:06,662	44k	INFO	Train Epoch: 1823 [86%]
2023-12-31 16:18:06,662	44k	INFO	Losses: [2.2880196571350098, 2.607523202896118, 9.767231941223145, 16.262210845947266, 0.5623328685760498], step: 63800, lr: 7.94422864940442e-05, reference_loss: 31.48731803894043
2023-12-31 16:18:10,624	44k	INFO	====> Epoch: 1823, cost 28.06 s
2023-12-31 16:18:37,562	44k	INFO	====> Epoch: 1824, cost 26.94 s
2023-12-31 16:19:04,573	44k	INFO	====> Epoch: 1825, cost 27.01 s
2023-12-31 16:19:31,616	44k	INFO	====> Epoch: 1826, cost 27.04 s
2023-12-31 16:19:58,633	44k	INFO	====> Epoch: 1827, cost 27.02 s
2023-12-31 16:20:25,712	44k	INFO	====> Epoch: 1828, cost 27.08 s
2023-12-31 16:20:41,915	44k	INFO	Train Epoch: 1829 [57%]
2023-12-31 16:20:41,916	44k	INFO	Losses: [2.222745418548584, 2.4475412368774414, 8.6878662109375, 17.62944984436035, 0.5370470881462097], step: 64000, lr: 7.938272339535662e-05, reference_loss: 31.524648666381836
2023-12-31 16:20:50,197	44k	INFO	Saving model and optimizer state at iteration 1829 to ./logs/44k/G_64000.pth
2023-12-31 16:20:51,793	44k	INFO	Saving model and optimizer state at iteration 1829 to ./logs/44k/D_64000.pth
2023-12-31 16:20:52,610	44k	INFO	.. Free up space by deleting ckpt ./logs/44k/G_61600.pth
2023-12-31 16:20:52,667	44k	INFO	.. Free up space by deleting ckpt ./logs/44k/D_61600.pth
2023-12-31 16:21:03,430	44k	INFO	====> Epoch: 1829, cost 37.72 s
2023-12-31 16:21:30,686	44k	INFO	====> Epoch: 1830, cost 27.26 s
2023-12-31 16:21:57,650	44k	INFO	====> Epoch: 1831, cost 26.96 s
2023-12-31 16:22:24,563	44k	INFO	====> Epoch: 1832, cost 26.91 s
2023-12-31 16:22:51,539	44k	INFO	====> Epoch: 1833, cost 26.98 s
2023-12-31 16:23:18,401	44k	INFO	====> Epoch: 1834, cost 26.86 s
2023-12-31 16:23:26,930	44k	INFO	Train Epoch: 1835 [29%]
2023-12-31 16:23:26,931	44k	INFO	Losses: [2.528648853302002, 2.1649301052093506, 6.779654026031494, 16.173837661743164, 0.5501657724380493], step: 64200, lr: 7.932320495503528e-05, reference_loss: 28.197235107421875
2023-12-31 16:23:46,087	44k	INFO	====> Epoch: 1835, cost 27.69 s
2023-12-31 16:24:13,459	44k	INFO	====> Epoch: 1836, cost 27.37 s
2023-12-31 16:24:40,543	44k	INFO	====> Epoch: 1837, cost 27.08 s
2023-12-31 16:25:07,535	44k	INFO	====> Epoch: 1838, cost 26.99 s
2023-12-31 16:25:34,478	44k	INFO	====> Epoch: 1839, cost 26.94 s
2023-12-31 16:26:01,429	44k	INFO	====> Epoch: 1840, cost 26.95 s
2023-12-31 16:26:02,221	44k	INFO	Train Epoch: 1841 [0%]
2023-12-31 16:26:02,222	44k	INFO	Losses: [2.345747232437134, 2.48687481880188, 7.030104160308838, 15.958971977233887, 0.5994659066200256], step: 64400, lr: 7.92637311395969e-05, reference_loss: 28.42116355895996
2023-12-31 16:26:29,069	44k	INFO	====> Epoch: 1841, cost 27.64 s
2023-12-31 16:26:56,194	44k	INFO	====> Epoch: 1842, cost 27.12 s
2023-12-31 16:27:23,280	44k	INFO	====> Epoch: 1843, cost 27.09 s
2023-12-31 16:27:50,610	44k	INFO	====> Epoch: 1844, cost 27.33 s
2023-12-31 16:28:17,631	44k	INFO	====> Epoch: 1845, cost 27.02 s
2023-12-31 16:28:37,802	44k	INFO	Train Epoch: 1846 [71%]
2023-12-31 16:28:37,803	44k	INFO	Losses: [2.430722236633301, 2.295271873474121, 8.590018272399902, 17.89552879333496, 0.7068485021591187], step: 64600, lr: 7.92142036910446e-05, reference_loss: 31.91838836669922
2023-12-31 16:28:45,353	44k	INFO	====> Epoch: 1846, cost 27.72 s
2023-12-31 16:29:12,482	44k	INFO	====> Epoch: 1847, cost 27.13 s
2023-12-31 16:29:39,519	44k	INFO	====> Epoch: 1848, cost 27.04 s
2023-12-31 16:30:06,613	44k	INFO	====> Epoch: 1849, cost 27.09 s
2023-12-31 16:30:33,789	44k	INFO	====> Epoch: 1850, cost 27.18 s
2023-12-31 16:31:01,095	44k	INFO	====> Epoch: 1851, cost 27.31 s
2023-12-31 16:31:13,431	44k	INFO	Train Epoch: 1852 [43%]
2023-12-31 16:31:13,432	44k	INFO	Losses: [2.5074961185455322, 2.2161850929260254, 7.359525680541992, 15.973823547363281, 0.40033504366874695], step: 64800, lr: 7.915481160101127e-05, reference_loss: 28.457365036010742
2023-12-31 16:31:21,887	44k	INFO	Saving model and optimizer state at iteration 1852 to ./logs/44k/G_64800.pth
2023-12-31 16:31:23,091	44k	INFO	Saving model and optimizer state at iteration 1852 to ./logs/44k/D_64800.pth
2023-12-31 16:31:23,903	44k	INFO	.. Free up space by deleting ckpt ./logs/44k/G_62400.pth
2023-12-31 16:31:23,961	44k	INFO	.. Free up space by deleting ckpt ./logs/44k/D_62400.pth
2023-12-31 16:31:38,645	44k	INFO	====> Epoch: 1852, cost 37.55 s
2023-12-31 16:32:05,722	44k	INFO	====> Epoch: 1853, cost 27.08 s
2023-12-31 16:32:32,666	44k	INFO	====> Epoch: 1854, cost 26.94 s
2023-12-31 16:32:59,756	44k	INFO	====> Epoch: 1855, cost 27.09 s
2023-12-31 16:33:27,127	44k	INFO	====> Epoch: 1856, cost 27.37 s
2023-12-31 16:33:54,000	44k	INFO	====> Epoch: 1857, cost 26.87 s
2023-12-31 16:33:58,661	44k	INFO	Train Epoch: 1858 [14%]
2023-12-31 16:33:58,661	44k	INFO	Losses: [2.3732972145080566, 2.1994340419769287, 7.1096272468566895, 17.637939453125, 0.6083182096481323], step: 65000, lr: 7.909546404112776e-05, reference_loss: 29.92861557006836
2023-12-31 16:34:21,365	44k	INFO	====> Epoch: 1858, cost 27.36 s
2023-12-31 16:34:48,292	44k	INFO	====> Epoch: 1859, cost 26.93 s
2023-12-31 16:35:15,180	44k	INFO	====> Epoch: 1860, cost 26.89 s
2023-12-31 16:35:42,167	44k	INFO	====> Epoch: 1861, cost 26.99 s
2023-12-31 16:36:09,316	44k	INFO	====> Epoch: 1862, cost 27.15 s
2023-12-31 16:36:33,051	44k	INFO	Train Epoch: 1863 [86%]
2023-12-31 16:36:33,052	44k	INFO	Losses: [2.098630905151367, 2.555657148361206, 10.101827621459961, 18.677453994750977, 0.7970212697982788], step: 65200, lr: 7.904604173322357e-05, reference_loss: 34.2305908203125
2023-12-31 16:36:36,966	44k	INFO	====> Epoch: 1863, cost 27.65 s
2023-12-31 16:37:03,964	44k	INFO	====> Epoch: 1864, cost 27.00 s
2023-12-31 16:37:30,941	44k	INFO	====> Epoch: 1865, cost 26.98 s
2023-12-31 16:37:57,901	44k	INFO	====> Epoch: 1866, cost 26.96 s
2023-12-31 16:38:24,883	44k	INFO	====> Epoch: 1867, cost 26.98 s
2023-12-31 16:38:52,006	44k	INFO	====> Epoch: 1868, cost 27.12 s
2023-12-31 16:39:08,357	44k	INFO	Train Epoch: 1869 [57%]
2023-12-31 16:39:08,358	44k	INFO	Losses: [2.725020170211792, 2.0376110076904297, 4.925921440124512, 13.478827476501465, 0.5145958065986633], step: 65400, lr: 7.89867757252522e-05, reference_loss: 23.681976318359375
2023-12-31 16:39:19,737	44k	INFO	====> Epoch: 1869, cost 27.73 s
2023-12-31 16:39:46,999	44k	INFO	====> Epoch: 1870, cost 27.26 s
2023-12-31 16:40:14,178	44k	INFO	====> Epoch: 1871, cost 27.18 s
2023-12-31 16:40:41,269	44k	INFO	====> Epoch: 1872, cost 27.09 s
2023-12-31 16:41:08,314	44k	INFO	====> Epoch: 1873, cost 27.04 s
2023-12-31 16:41:35,379	44k	INFO	====> Epoch: 1874, cost 27.06 s
2023-12-31 16:41:43,998	44k	INFO	Train Epoch: 1875 [29%]
2023-12-31 16:41:43,999	44k	INFO	Losses: [2.4273362159729004, 2.2998228073120117, 7.274530410766602, 16.85538101196289, 0.8260619044303894], step: 65600, lr: 7.892755415289867e-05, reference_loss: 29.68313217163086
2023-12-31 16:41:52,434	44k	INFO	Saving model and optimizer state at iteration 1875 to ./logs/44k/G_65600.pth
2023-12-31 16:41:53,632	44k	INFO	Saving model and optimizer state at iteration 1875 to ./logs/44k/D_65600.pth
2023-12-31 16:41:54,448	44k	INFO	.. Free up space by deleting ckpt ./logs/44k/G_63200.pth
2023-12-31 16:41:54,505	44k	INFO	.. Free up space by deleting ckpt ./logs/44k/D_63200.pth
2023-12-31 16:42:13,399	44k	INFO	====> Epoch: 1875, cost 38.02 s
2023-12-31 16:42:40,445	44k	INFO	====> Epoch: 1876, cost 27.05 s
2023-12-31 16:43:07,453	44k	INFO	====> Epoch: 1877, cost 27.01 s
2023-12-31 16:43:34,415	44k	INFO	====> Epoch: 1878, cost 26.96 s
2023-12-31 16:44:01,513	44k	INFO	====> Epoch: 1879, cost 27.10 s
2023-12-31 16:44:28,379	44k	INFO	====> Epoch: 1880, cost 26.87 s
2023-12-31 16:44:29,183	44k	INFO	Train Epoch: 1881 [0%]
2023-12-31 16:44:29,183	44k	INFO	Losses: [2.332202434539795, 2.5795979499816895, 7.940252304077148, 16.449539184570312, 0.6902133226394653], step: 65800, lr: 7.886837698284666e-05, reference_loss: 29.991806030273438
2023-12-31 16:44:55,916	44k	INFO	====> Epoch: 1881, cost 27.54 s
2023-12-31 16:45:22,895	44k	INFO	====> Epoch: 1882, cost 26.98 s
2023-12-31 16:45:50,010	44k	INFO	====> Epoch: 1883, cost 27.12 s
2023-12-31 16:46:17,348	44k	INFO	====> Epoch: 1884, cost 27.34 s
2023-12-31 16:46:44,377	44k	INFO	====> Epoch: 1885, cost 27.03 s
2023-12-31 16:47:04,630	44k	INFO	Train Epoch: 1886 [71%]
2023-12-31 16:47:04,631	44k	INFO	Losses: [2.6084625720977783, 2.1235251426696777, 6.5470805168151855, 15.102579116821289, 0.5139046311378479], step: 66000, lr: 7.881909656887598e-05, reference_loss: 26.895551681518555
2023-12-31 16:47:12,016	44k	INFO	====> Epoch: 1886, cost 27.64 s
2023-12-31 16:47:39,021	44k	INFO	====> Epoch: 1887, cost 27.01 s
2023-12-31 16:48:06,109	44k	INFO	====> Epoch: 1888, cost 27.09 s
2023-12-31 16:48:33,226	44k	INFO	====> Epoch: 1889, cost 27.12 s
2023-12-31 16:49:00,142	44k	INFO	====> Epoch: 1890, cost 26.92 s
2023-12-31 16:49:27,106	44k	INFO	====> Epoch: 1891, cost 26.96 s
2023-12-31 16:49:39,793	44k	INFO	Train Epoch: 1892 [43%]
2023-12-31 16:49:39,794	44k	INFO	Losses: [2.410820484161377, 2.4463279247283936, 8.270427703857422, 15.962207794189453, 0.624047577381134], step: 66200, lr: 7.876000071659647e-05, reference_loss: 29.713830947875977
2023-12-31 16:49:55,141	44k	INFO	====> Epoch: 1892, cost 28.04 s
2023-12-31 16:50:22,139	44k	INFO	====> Epoch: 1893, cost 27.00 s
2023-12-31 16:50:49,126	44k	INFO	====> Epoch: 1894, cost 26.99 s
2023-12-31 16:51:16,074	44k	INFO	====> Epoch: 1895, cost 26.95 s
2023-12-31 16:51:43,173	44k	INFO	====> Epoch: 1896, cost 27.10 s
2023-12-31 16:52:10,249	44k	INFO	====> Epoch: 1897, cost 27.08 s
2023-12-31 16:52:14,902	44k	INFO	Train Epoch: 1898 [14%]
2023-12-31 16:52:14,902	44k	INFO	Losses: [2.4801769256591797, 2.420395612716675, 7.616649150848389, 18.463321685791016, 0.6435941457748413], step: 66400, lr: 7.870094917235789e-05, reference_loss: 31.62413787841797
2023-12-31 16:52:23,491	44k	INFO	Saving model and optimizer state at iteration 1898 to ./logs/44k/G_66400.pth
2023-12-31 16:52:24,694	44k	INFO	Saving model and optimizer state at iteration 1898 to ./logs/44k/D_66400.pth
2023-12-31 16:52:25,507	44k	INFO	.. Free up space by deleting ckpt ./logs/44k/G_64000.pth
2023-12-31 16:52:25,564	44k	INFO	.. Free up space by deleting ckpt ./logs/44k/D_64000.pth
2023-12-31 16:52:47,862	44k	INFO	====> Epoch: 1898, cost 37.61 s
2023-12-31 16:53:14,792	44k	INFO	====> Epoch: 1899, cost 26.93 s
2023-12-31 16:53:41,773	44k	INFO	====> Epoch: 1900, cost 26.98 s
2023-12-31 16:54:08,874	44k	INFO	====> Epoch: 1901, cost 27.10 s
2023-12-31 16:54:35,886	44k	INFO	====> Epoch: 1902, cost 27.01 s
2023-12-31 16:54:59,711	44k	INFO	Train Epoch: 1903 [86%]
2023-12-31 16:54:59,712	44k	INFO	Losses: [2.340531826019287, 2.600743293762207, 9.107412338256836, 16.883440017700195, 0.43575745820999146], step: 66600, lr: 7.865177337461142e-05, reference_loss: 31.367883682250977
2023-12-31 16:55:03,633	44k	INFO	====> Epoch: 1903, cost 27.75 s
2023-12-31 16:55:30,557	44k	INFO	====> Epoch: 1904, cost 26.92 s
2023-12-31 16:55:57,535	44k	INFO	====> Epoch: 1905, cost 26.98 s
2023-12-31 16:56:24,493	44k	INFO	====> Epoch: 1906, cost 26.96 s
2023-12-31 16:56:51,424	44k	INFO	====> Epoch: 1907, cost 26.93 s
2023-12-31 16:57:18,582	44k	INFO	====> Epoch: 1908, cost 27.16 s
2023-12-31 16:57:34,835	44k	INFO	Train Epoch: 1909 [57%]
2023-12-31 16:57:34,835	44k	INFO	Losses: [2.497802257537842, 2.3989648818969727, 6.397219657897949, 15.464762687683105, 0.5662063360214233], step: 66800, lr: 7.859280297551778e-05, reference_loss: 27.3249568939209
2023-12-31 16:57:46,099	44k	INFO	====> Epoch: 1909, cost 27.52 s
2023-12-31 16:58:13,306	44k	INFO	====> Epoch: 1910, cost 27.21 s
2023-12-31 16:58:40,577	44k	INFO	====> Epoch: 1911, cost 27.27 s
2023-12-31 16:59:07,437	44k	INFO	====> Epoch: 1912, cost 26.86 s
2023-12-31 16:59:34,459	44k	INFO	====> Epoch: 1913, cost 27.02 s
2023-12-31 17:00:01,358	44k	INFO	====> Epoch: 1914, cost 26.90 s
2023-12-31 17:00:09,938	44k	INFO	Train Epoch: 1915 [29%]
2023-12-31 17:00:09,939	44k	INFO	Losses: [2.4369163513183594, 2.151747465133667, 6.52205753326416, 16.457109451293945, 0.7538910508155823], step: 67000, lr: 7.853387679040456e-05, reference_loss: 28.32172203063965
2023-12-31 17:00:28,829	44k	INFO	====> Epoch: 1915, cost 27.47 s
2023-12-31 17:00:55,900	44k	INFO	====> Epoch: 1916, cost 27.07 s
2023-12-31 17:01:22,983	44k	INFO	====> Epoch: 1917, cost 27.08 s
2023-12-31 17:01:50,083	44k	INFO	====> Epoch: 1918, cost 27.10 s
2023-12-31 17:02:16,883	44k	INFO	====> Epoch: 1919, cost 26.80 s
2023-12-31 17:02:43,867	44k	INFO	====> Epoch: 1920, cost 26.98 s
2023-12-31 17:02:44,641	44k	INFO	Train Epoch: 1921 [0%]
2023-12-31 17:02:44,642	44k	INFO	Losses: [2.3768043518066406, 2.3284623622894287, 7.231654167175293, 15.917823791503906, 0.43924275040626526], step: 67200, lr: 7.847499478612166e-05, reference_loss: 28.293987274169922
2023-12-31 17:02:53,041	44k	INFO	Saving model and optimizer state at iteration 1921 to ./logs/44k/G_67200.pth
2023-12-31 17:02:54,243	44k	INFO	Saving model and optimizer state at iteration 1921 to ./logs/44k/D_67200.pth
2023-12-31 17:02:55,052	44k	INFO	.. Free up space by deleting ckpt ./logs/44k/G_64800.pth
2023-12-31 17:02:55,109	44k	INFO	.. Free up space by deleting ckpt ./logs/44k/D_64800.pth
2023-12-31 17:03:21,429	44k	INFO	====> Epoch: 1921, cost 37.56 s
2023-12-31 17:03:48,421	44k	INFO	====> Epoch: 1922, cost 26.99 s
2023-12-31 17:04:15,716	44k	INFO	====> Epoch: 1923, cost 27.29 s
2023-12-31 17:04:42,600	44k	INFO	====> Epoch: 1924, cost 26.88 s
2023-12-31 17:05:09,579	44k	INFO	====> Epoch: 1925, cost 26.98 s
2023-12-31 17:05:29,719	44k	INFO	Train Epoch: 1926 [71%]
2023-12-31 17:05:29,720	44k	INFO	Losses: [2.321432590484619, 2.499191999435425, 8.484663963317871, 16.83180046081543, 0.5396342873573303], step: 67400, lr: 7.842596017456562e-05, reference_loss: 30.67672348022461
2023-12-31 17:05:37,055	44k	INFO	====> Epoch: 1926, cost 27.48 s
2023-12-31 17:06:04,130	44k	INFO	====> Epoch: 1927, cost 27.07 s
2023-12-31 17:06:30,989	44k	INFO	====> Epoch: 1928, cost 26.86 s
2023-12-31 17:06:57,877	44k	INFO	====> Epoch: 1929, cost 26.89 s
2023-12-31 17:07:24,809	44k	INFO	====> Epoch: 1930, cost 26.93 s
2023-12-31 17:07:51,924	44k	INFO	====> Epoch: 1931, cost 27.12 s
2023-12-31 17:08:04,681	44k	INFO	Train Epoch: 1932 [43%]
2023-12-31 17:08:04,682	44k	INFO	Losses: [2.400266170501709, 2.413473129272461, 7.275378227233887, 14.732144355773926, 0.6273593902587891], step: 67600, lr: 7.836715908245587e-05, reference_loss: 27.44862174987793
2023-12-31 17:08:19,901	44k	INFO	====> Epoch: 1932, cost 27.98 s
2023-12-31 17:08:46,954	44k	INFO	====> Epoch: 1933, cost 27.05 s
2023-12-31 17:09:13,955	44k	INFO	====> Epoch: 1934, cost 27.00 s
2023-12-31 17:09:40,912	44k	INFO	====> Epoch: 1935, cost 26.96 s
2023-12-31 17:10:08,101	44k	INFO	====> Epoch: 1936, cost 27.19 s
2023-12-31 17:10:34,994	44k	INFO	====> Epoch: 1937, cost 26.89 s
2023-12-31 17:10:39,662	44k	INFO	Train Epoch: 1938 [14%]
2023-12-31 17:10:39,662	44k	INFO	Losses: [2.4591143131256104, 2.386287212371826, 7.101146221160889, 17.207061767578125, 0.7029879093170166], step: 67800, lr: 7.830840207738599e-05, reference_loss: 29.856597900390625
2023-12-31 17:11:02,777	44k	INFO	====> Epoch: 1938, cost 27.78 s
2023-12-31 17:11:29,667	44k	INFO	====> Epoch: 1939, cost 26.89 s
2023-12-31 17:11:56,738	44k	INFO	====> Epoch: 1940, cost 27.07 s
2023-12-31 17:12:23,897	44k	INFO	====> Epoch: 1941, cost 27.16 s
2023-12-31 17:12:50,792	44k	INFO	====> Epoch: 1942, cost 26.90 s
2023-12-31 17:13:14,558	44k	INFO	Train Epoch: 1943 [86%]
2023-12-31 17:13:14,559	44k	INFO	Losses: [2.3487024307250977, 2.5519092082977295, 7.424498081207275, 16.053123474121094, 0.38137978315353394], step: 68000, lr: 7.825947156024605e-05, reference_loss: 28.759613037109375
2023-12-31 17:13:22,910	44k	INFO	Saving model and optimizer state at iteration 1943 to ./logs/44k/G_68000.pth
2023-12-31 17:13:24,476	44k	INFO	Saving model and optimizer state at iteration 1943 to ./logs/44k/D_68000.pth
2023-12-31 17:13:25,289	44k	INFO	.. Free up space by deleting ckpt ./logs/44k/G_65600.pth
2023-12-31 17:13:25,347	44k	INFO	.. Free up space by deleting ckpt ./logs/44k/D_65600.pth
2023-12-31 17:13:28,395	44k	INFO	====> Epoch: 1943, cost 37.60 s
2023-12-31 17:13:55,268	44k	INFO	====> Epoch: 1944, cost 26.87 s
2023-12-31 17:14:22,173	44k	INFO	====> Epoch: 1945, cost 26.90 s
2023-12-31 17:14:49,248	44k	INFO	====> Epoch: 1946, cost 27.08 s
2023-12-31 17:15:16,253	44k	INFO	====> Epoch: 1947, cost 27.00 s
2023-12-31 17:15:43,379	44k	INFO	====> Epoch: 1948, cost 27.13 s
2023-12-31 17:15:59,740	44k	INFO	Train Epoch: 1949 [57%]
2023-12-31 17:15:59,741	44k	INFO	Losses: [2.6388378143310547, 2.104985475540161, 5.255954742431641, 13.998044967651367, 0.5933741927146912], step: 68200, lr: 7.820079529558277e-05, reference_loss: 24.591196060180664
2023-12-31 17:16:11,209	44k	INFO	====> Epoch: 1949, cost 27.83 s
2023-12-31 17:16:38,093	44k	INFO	====> Epoch: 1950, cost 26.88 s
2023-12-31 17:17:05,354	44k	INFO	====> Epoch: 1951, cost 27.26 s
2023-12-31 17:17:32,381	44k	INFO	====> Epoch: 1952, cost 27.03 s
2023-12-31 17:17:59,417	44k	INFO	====> Epoch: 1953, cost 27.04 s
2023-12-31 17:18:26,148	44k	INFO	====> Epoch: 1954, cost 26.73 s
2023-12-31 17:18:34,761	44k	INFO	Train Epoch: 1955 [29%]
2023-12-31 17:18:34,761	44k	INFO	Losses: [2.461419105529785, 2.270001173019409, 6.453183174133301, 15.319608688354492, 0.3587598502635956], step: 68400, lr: 7.814216302436802e-05, reference_loss: 26.86297035217285
2023-12-31 17:18:53,715	44k	INFO	====> Epoch: 1955, cost 27.57 s
2023-12-31 17:19:20,670	44k	INFO	====> Epoch: 1956, cost 26.95 s
2023-12-31 17:19:47,668	44k	INFO	====> Epoch: 1957, cost 27.00 s
2023-12-31 17:20:14,883	44k	INFO	====> Epoch: 1958, cost 27.22 s
2023-12-31 17:20:41,954	44k	INFO	====> Epoch: 1959, cost 27.07 s
2023-12-31 17:21:09,010	44k	INFO	====> Epoch: 1960, cost 27.06 s
2023-12-31 17:21:09,804	44k	INFO	Train Epoch: 1961 [0%]
2023-12-31 17:21:09,804	44k	INFO	Losses: [2.5126850605010986, 2.1546897888183594, 7.3790602684021, 14.56411075592041, 0.22818174958229065], step: 68600, lr: 7.808357471361705e-05, reference_loss: 26.838727951049805
2023-12-31 17:21:36,465	44k	INFO	====> Epoch: 1961, cost 27.46 s
2023-12-31 17:22:03,496	44k	INFO	====> Epoch: 1962, cost 27.03 s
2023-12-31 17:22:30,476	44k	INFO	====> Epoch: 1963, cost 26.98 s
2023-12-31 17:22:57,464	44k	INFO	====> Epoch: 1964, cost 26.99 s
2023-12-31 17:23:24,768	44k	INFO	====> Epoch: 1965, cost 27.30 s
2023-12-31 17:23:44,832	44k	INFO	Train Epoch: 1966 [71%]
2023-12-31 17:23:44,832	44k	INFO	Losses: [2.747157335281372, 2.1407899856567383, 5.77517557144165, 14.344947814941406, 0.7636385560035706], step: 68800, lr: 7.80347846784546e-05, reference_loss: 25.771709442138672
2023-12-31 17:23:53,426	44k	INFO	Saving model and optimizer state at iteration 1966 to ./logs/44k/G_68800.pth
2023-12-31 17:23:54,623	44k	INFO	Saving model and optimizer state at iteration 1966 to ./logs/44k/D_68800.pth
2023-12-31 17:23:55,437	44k	INFO	.. Free up space by deleting ckpt ./logs/44k/G_66400.pth
2023-12-31 17:23:55,495	44k	INFO	.. Free up space by deleting ckpt ./logs/44k/D_66400.pth
2023-12-31 17:24:02,400	44k	INFO	====> Epoch: 1966, cost 37.63 s
2023-12-31 17:24:29,460	44k	INFO	====> Epoch: 1967, cost 27.06 s
2023-12-31 17:24:56,552	44k	INFO	====> Epoch: 1968, cost 27.09 s
2023-12-31 17:25:23,687	44k	INFO	====> Epoch: 1969, cost 27.14 s
2023-12-31 17:25:50,680	44k	INFO	====> Epoch: 1970, cost 26.99 s
2023-12-31 17:26:17,809	44k	INFO	====> Epoch: 1971, cost 27.13 s
2023-12-31 17:26:30,143	44k	INFO	Train Epoch: 1972 [43%]
2023-12-31 17:26:30,144	44k	INFO	Losses: [2.6295032501220703, 2.002946376800537, 5.762385368347168, 14.789055824279785, 0.7652334570884705], step: 69000, lr: 7.797627687630045e-05, reference_loss: 25.949125289916992
2023-12-31 17:26:45,447	44k	INFO	====> Epoch: 1972, cost 27.64 s
2023-12-31 17:27:12,501	44k	INFO	====> Epoch: 1973, cost 27.05 s
2023-12-31 17:27:39,377	44k	INFO	====> Epoch: 1974, cost 26.88 s
2023-12-31 17:28:06,369	44k	INFO	====> Epoch: 1975, cost 26.99 s
2023-12-31 17:28:33,444	44k	INFO	====> Epoch: 1976, cost 27.08 s
2023-12-31 17:29:00,470	44k	INFO	====> Epoch: 1977, cost 27.03 s
2023-12-31 17:29:05,156	44k	INFO	Train Epoch: 1978 [14%]
2023-12-31 17:29:05,157	44k	INFO	Losses: [2.3719329833984375, 2.3257620334625244, 8.402162551879883, 18.483234405517578, 0.6282352209091187], step: 69200, lr: 7.791781294128744e-05, reference_loss: 32.211326599121094
2023-12-31 17:29:28,247	44k	INFO	====> Epoch: 1978, cost 27.78 s
2023-12-31 17:29:55,191	44k	INFO	====> Epoch: 1979, cost 26.94 s
2023-12-31 17:30:22,215	44k	INFO	====> Epoch: 1980, cost 27.02 s
2023-12-31 17:30:49,213	44k	INFO	====> Epoch: 1981, cost 27.00 s
2023-12-31 17:31:16,298	44k	INFO	====> Epoch: 1982, cost 27.09 s
2023-12-31 17:31:40,070	44k	INFO	Train Epoch: 1983 [86%]
2023-12-31 17:31:40,071	44k	INFO	Losses: [2.1825926303863525, 2.685703754425049, 10.058311462402344, 16.166105270385742, 0.5562192797660828], step: 69400, lr: 7.786912648133565e-05, reference_loss: 31.6489315032959
2023-12-31 17:31:43,684	44k	INFO	====> Epoch: 1983, cost 27.39 s
2023-12-31 17:32:11,090	44k	INFO	====> Epoch: 1984, cost 27.41 s
2023-12-31 17:32:37,924	44k	INFO	====> Epoch: 1985, cost 26.83 s
2023-12-31 17:33:04,757	44k	INFO	====> Epoch: 1986, cost 26.83 s
2023-12-31 17:33:31,808	44k	INFO	====> Epoch: 1987, cost 27.05 s
2023-12-31 17:33:58,725	44k	INFO	====> Epoch: 1988, cost 26.92 s
2023-12-31 17:34:14,961	44k	INFO	Train Epoch: 1989 [57%]
2023-12-31 17:34:14,962	44k	INFO	Losses: [2.3161659240722656, 2.6215903759002686, 8.715604782104492, 16.451745986938477, 0.5036386251449585], step: 69600, lr: 7.781074288400968e-05, reference_loss: 30.608745574951172
2023-12-31 17:34:23,268	44k	INFO	Saving model and optimizer state at iteration 1989 to ./logs/44k/G_69600.pth
2023-12-31 17:34:24,470	44k	INFO	Saving model and optimizer state at iteration 1989 to ./logs/44k/D_69600.pth
2023-12-31 17:34:25,283	44k	INFO	.. Free up space by deleting ckpt ./logs/44k/G_67200.pth
2023-12-31 17:34:25,341	44k	INFO	.. Free up space by deleting ckpt ./logs/44k/D_67200.pth
2023-12-31 17:34:36,143	44k	INFO	====> Epoch: 1989, cost 37.42 s
2023-12-31 17:35:03,582	44k	INFO	====> Epoch: 1990, cost 27.44 s
2023-12-31 17:35:30,480	44k	INFO	====> Epoch: 1991, cost 26.90 s
2023-12-31 17:35:57,511	44k	INFO	====> Epoch: 1992, cost 27.03 s
2023-12-31 17:36:24,299	44k	INFO	====> Epoch: 1993, cost 26.79 s
2023-12-31 17:36:51,369	44k	INFO	====> Epoch: 1994, cost 27.07 s
2023-12-31 17:36:59,921	44k	INFO	Train Epoch: 1995 [29%]
2023-12-31 17:36:59,921	44k	INFO	Losses: [2.5463857650756836, 2.3117964267730713, 6.452993392944336, 15.605659484863281, 0.5268325209617615], step: 69800, lr: 7.77524030607003e-05, reference_loss: 27.443668365478516
2023-12-31 17:37:18,841	44k	INFO	====> Epoch: 1995, cost 27.47 s
2023-12-31 17:37:45,837	44k	INFO	====> Epoch: 1996, cost 27.00 s
2023-12-31 17:38:12,776	44k	INFO	====> Epoch: 1997, cost 26.94 s
2023-12-31 17:38:39,884	44k	INFO	====> Epoch: 1998, cost 27.11 s
2023-12-31 17:39:07,185	44k	INFO	====> Epoch: 1999, cost 27.30 s
2023-12-31 17:39:34,150	44k	INFO	====> Epoch: 2000, cost 26.97 s
